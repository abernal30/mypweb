a<- 4
rm(a)
ticker<-"APPL"
class(ticker)
ticker
# or
print(ticher)
ticker
# or
print(ticker)
num<-4
print(num)
print(class(num))
gc()
num2<-5.56
class(num2)
num2<-5.56
class(num2)
v1<-c(160,165,167,145,145)
class(v1)
getwd()
library(openxlsx)
library(dplyr)
library(caret)
df<-read.xlsx("data/credit_short.xlsx")
getwd()
co<-count(df,loan_status,sort =T )
co
# para saber cuantas cztegorias tiene term
co<-count(df,loan_status,sort =T )
barplot(co$n , names.arg=co$loan_status , las=2 ,
col=c("red","blue","green","purple","black"),
ylim=c(0,700))
df2<-df %>%
filter(loan_status == "Fully Paid" | loan_status== "Charged Off")
count(df2,loan_status)
Default<-ifelse(df2$loan_status=="Fully Paid",0,1)
#Default<-factor(Default ,levels =c(1,0))
# combining default and df2
df3<-cbind(Default,df2)
# delete loan status, is the second colum
df4<-df3[,-2]
head(df4)
# Don´t forget to eliminate the column loan_status, because it would be duplicated with Default
#library(devtools)
#doremotes::install_github("abernal30/dataclean")
#library(devtools)
#devtools::install_github("abernal30/dataclean")
library(dataclean)
df5<-asnum(df4)
head(df5)
df5<-read.xlsx("data/df5.xlsx")
set.seed (3)
dim<-dim(df5)
train_sample<-sample(dim[1],dim[1]*.8)
train <- df5[train_sample, ] # this is the 80% o the sample
test  <- df5[-train_sample, ] # This is 20%
head(train)
model<-glm(Default~. ,data=train,family=binomial())
model<-glm(Default~. ,data=train,family=binomial())
library(openxlsx)
library(dplyr)
library(caret)
df<-read.xlsx("data/credit_short.xlsx")
getwd()
co<-count(df,loan_status,sort =T )
co
# para saber cuantas cztegorias tiene term
co<-count(df,loan_status,sort =T )
barplot(co$n , names.arg=co$loan_status , las=2 ,
col=c("red","blue","green","purple","black"),
ylim=c(0,700))
df2<-df %>%
filter(loan_status == "Fully Paid" | loan_status== "Charged Off")
count(df2,loan_status)
Default<-ifelse(df2$loan_status=="Fully Paid",0,1)
#Default<-factor(Default ,levels =c(1,0))
# combining default and df2
df3<-cbind(Default,df2)
# delete loan status, is the second colum
df4<-df3[,-2]
head(df4)
# Don´t forget to eliminate the column loan_status, because it would be duplicated with Default
#library(devtools)
#doremotes::install_github("abernal30/dataclean")
#library(devtools)
#devtools::install_github("abernal30/dataclean")
library(dataclean)
df5<-asnum(df4)
head(df5)
df5<-read.xlsx("data/df5.xlsx")
set.seed (3)
dim<-dim(df5)
train_sample<-sample(dim[1],dim[1]*.8)
train <- df5[train_sample, ] # this is the 80% o the sample
test  <- df5[-train_sample, ] # This is 20%
head(train)
model<-glm(Default~. ,data=train,family=binomial())
View(train)
View(df)
View(df5)
df5<-read.xlsx("data/df5.xlsx")
set.seed (3)
dim<-dim(df5)
train_sample<-sample(dim[1],dim[1]*.8)
train <- df5[train_sample, ] # this is the 80% o the sample
test  <- df5[-train_sample, ] # This is 20%
head(train)
library(openxlsx)
library(dplyr)
library(caret)
df<-read.xlsx("data/credit_short.xlsx")
co<-count(df,loan_status,sort =T )
co
# para saber cuantas cztegorias tiene term
co<-count(df,loan_status,sort =T )
barplot(co$n , names.arg=co$loan_status , las=2 ,
col=c("red","blue","green","purple","black"),
ylim=c(0,700))
df2<-df %>%
filter(loan_status == "Fully Paid" | loan_status== "Charged Off")
count(df2,loan_status)
Default<-ifelse(df2$loan_status=="Fully Paid",0,1)
#Default<-factor(Default ,levels =c(1,0))
# combining default and df2
df3<-cbind(Default,df2)
# delete loan status, is the second colum
df4<-df3[,-2]
head(df4)
# Don´t forget to eliminate the column loan_status, because it would be duplicated with Default
#library(devtools)
#doremotes::install_github("abernal30/dataclean")
#library(devtools)
#devtools::install_github("abernal30/dataclean")
library(dataclean)
df5<-asnum(df4)
head(df5)
set.seed (3)
dim<-dim(df5)
train_sample<-sample(dim[1],dim[1]*.8)
train <- df5[train_sample, ] # this is the 80% o the sample
test  <- df5[-train_sample, ] # This is 20%
head(train)
model<-glm(Default~. ,data=train,family=binomial())
predict<-predict(model,newdata = test,type = "response")
#trasnform that probability into a 0,1
predictp<-ifelse(predict>.5,1,0)
predictp
# the real data is in test and we need to transoform in factor
real<-factor(test[,"Default"],levels = c(1,0))
predictf<-factor(predictp,levels = c(1,0))
confusionMatrix(predictf,real)
def_train_f<-factor(train$Default,levels=c(1,0))
trainf<-train
trainf[,"Default"]<-def_train_f
testf<-test
testf[,"Default"]<-real # antes una f
#set.seed(1)
trainf<-na.omit(trainf)# delete the rows with nas or missing values
gbmFit1 <- train(Default ~ ., data = trainf,
method = "glmStepAIC",
trControl = trainControl(method = "cv", number = 10),
trace=0,   metric="Accuracy")
gbmFit1
gbmFit1$finalModel$formula
test[1,]
model<-glm(Default~loan_amnt + term + int_rate + installment + grade +
sub_grade + home_ownership + open_acc + pub_rec + mort_acc +
pub_rec_bankruptcies ,data=train,family=binomial())
predict<-predict(model,newdata = test[1,],type = "response")
#trasnform that probability into a 0,1
predictp<-ifelse(predict>.5,1,0)
predictp
library(quantmod)
library(FinTS)
library(tseries)
library(rugarch)
ticker<-"GAPB.MX"
getSymbols(ticker)
all<-get(ticker)
dji<-all[,4]
dji<-Delt(all[,4])
la2<-stats::lag(dji,2)
la3<-stats::lag(dji,3)
dji<-cbind(dji,la2,la3)
dji<-na.omit(dji)
colnames(dji)<-c("SP500","SP500_lag2","SP500_lag3")
head(dji)
summary(lm(SP500~.,data =dji))
ticker<-"^GSPC"
getSymbols(ticker,from="2021-05-01",to="2022-05-01")
dji_long<-GSPC[,4]
ar<-ArchTest(dji_long,lags=1)
data.frame(ar$p.value)
library(quantmod)
#library(xml2) # this are for the  code
#library(rvest) # in 154 to 177 lines, just instaal it if you are running that code
library(openxlsx)
library(FinTS)
library(tseries)
library(rugarch)
df<-read.xlsx("data/df_dates.xlsx", detectDates = T)
date<-df[,1]
dim<-dim(df)
# important to takeout the data before transforming to xts, other wise does not transform into numeric.
data<-df[,2:dim[2]]
datax<- xts(data,
order.by = as.Date(date))
dfx<-na.omit(datax)
return<-Delt(dfx[,1])
# we are going to apply the Delt function to the 100 stocks
# function apply()
return_all<-apply(dfx, 2, Delt)
# es 1 for rows o 2for columns
dfr<-return_all
m<-26
#---de aqui es la creación del Loop for, esto rebasa el nivel de este curso
ar<-c()
n<-dim(dfr)[2]
for (i in 1:n){
ar1<-ArchTest(dfr[,i],lags=m)$p.value
ar<-c(ar,ar1)
}
#-----
ar<-data.frame(ar)
# it has the p value of the EMH test, if the p-value is lees than 10%, then we could make predictions
# add the name of the ticker
col_name<-colnames(return_all)
col_name
# add the ticker na,me
rownames(ar)<-col_name
ar
library(dplyr)
pred<-ifelse(ar[,1]<0.1,"Predict","No Predict")
# merge with the ar object
ar<-cbind(ar,pred)
ar
library(dplyr)
arf<- ar %>%
filter(pred == "Predict")
# this code takes the names of the filtered tickers
col_filterd<-rownames(arf)
dfx_2<-dfx[,col_filterd]
dfx_3<-data.frame(dfx_2)
date<-rownames(dfx_3)
dfx_4<-cbind(date,dfx_3)
write.xlsx(dfx_4,"dfx_2.xlsx")
library(openxlsx)
library(quantmod)
data<-read.xlsx("dfx_2.xlsx")
# The following codes are to transform the data frame date into xts (as we did last session)
date<-data[,1]
# liminating the fits column
data<-data[,-1]
#Applying the xts function
datax<- xts(data,
order.by = as.Date(date))
# Eliminating rows with missing values
datax<-na.omit(datax)
#Also we estimate the returns for each stock, as we did in last session
ret<-apply(datax,2,Delt)
# we lost the xts
retx<- xts(ret,
order.by = as.Date(date))
# Eliminating rows with missing values
retx<-na.omit(retx)
in_sd<- "2018-05-26"
in_ed<- "2021-05-26"
out_sd<- "2021-05-27"
out_ed<- "2022-05-27"
in_sd<- "2018-05-26"  # están en la 59
in_ed<- "2021-05-26"
# in_sample of the price of the stocks
in_datax<- subset(datax,
+index(datax)>= in_sd &
+index(datax)<= in_ed)
# in_sample of the returns
in_ret<- subset(retx,
+index(retx)>= in_sd &
+index(retx)<= in_ed)
out_datax<- subset(datax,
+index(datax)>= out_sd &
+index(datax)<= out_ed)
# in_sample of the returns
out_ret<- subset(retx,
+index(retx)>= out_sd &
+index(retx)<= out_ed)
macd<-MACD(in_datax[,3] , nFast =12 , nSlow =26  , nSig =9 ,maType="SMA")
bb<-BBands(in_datax[,3], n = 20, maType="SMA", sd = 2)
signal <- ifelse(in_datax[,1]> bb[,'up'] & macd[,'macd'] >macd[,'signal'],1,ifelse(in_datax[,1]< bb[,'dn'] &macd[,'macd'] <macd[,'signal'],-1,0))
plot(signal[,1])
strat_ret<-in_ret[,1]*(stats::lag(signal[,1]))
library(PerformanceAnalytics)
# The strategy return signal
ret_annual<-Return.annualized(strat_ret,geometric = T,scale= 252)
ret_annual
sd<-StdDev.annualized(strat_ret,scale=252)
# Sharpe ratio
ret_annual/sd
library(openxlsx)
library(quantmod)
data<-read.xlsx("data/dfx_2.xlsx")
# The following codes are to transform the data frame date into xts (as we did last session)
date<-data[,1]
# liminating the fits column
data<-data[,-1]
#Applying the xts function
datax<- xts(data,
order.by = as.Date(date))
# Eliminating rows with missing values
datax<-na.omit(datax)
#Also we estimate the returns for each stock, as we did in last session
ret<-apply(datax,2,Delt)
# we lost the xts
retx<- xts(ret,
order.by = as.Date(date))
# Eliminating rows with missing values
retx<-na.omit(retx)
in_sd<- "2018-05-26"
in_ed<- "2021-05-26"
out_sd<- "2021-05-27"
out_ed<- "2022-05-27"
in_sd<- "2018-05-26"  # están en la 59
in_ed<- "2021-05-26"
# in_sample of the price of the stocks
in_datax<- subset(datax,
+index(datax)>= in_sd &
+index(datax)<= in_ed)
# in_sample of the returns
in_ret<- subset(retx,
+index(retx)>= in_sd &
+index(retx)<= in_ed)
out_datax<- subset(datax,
+index(datax)>= out_sd &
+index(datax)<= out_ed)
# in_sample of the returns
out_ret<- subset(retx,
+index(retx)>= out_sd &
+index(retx)<= out_ed)
macd<-MACD(in_datax[,3] , nFast =12 , nSlow =26  , nSig =9 ,maType="SMA")
bb<-BBands(in_datax[,3], n = 20, maType="SMA", sd = 2)
signal <- ifelse(in_datax[,1]> bb[,'up'] & macd[,'macd'] >macd[,'signal'],1,ifelse(in_datax[,1]< bb[,'dn'] &macd[,'macd'] <macd[,'signal'],-1,0))
plot(signal[,1])
strat_ret<-in_ret[,1]*(stats::lag(signal[,1]))
library(PerformanceAnalytics)
# The strategy return signal
ret_annual<-Return.annualized(strat_ret,geometric = T,scale= 252)
ret_annual
sd<-StdDev.annualized(strat_ret,scale=252)
# Sharpe ratio
ret_annual/sd
library(openxlsx)
library(quantmod)
library(dplyr)
library(PerformanceAnalytics)
df_merge<-read.xlsx("df_merge.xlsx",rowNames=T)
library(openxlsx)
library(quantmod)
library(dplyr)
library(PerformanceAnalytics)
df_merge<-read.xlsx("data/df_merge.xlsx",rowNames=T)
treh<-0.2
df_merge2<- df_merge   %>%
filter(Sharpe_diff < treh & Sharpe_diff > -treh)
df_merge2
df_filtered<- df_merge2 %>% arrange(desc(in_Sharpe))
# to gt the names of those stocks
co<-rownames(df_filtered)
le<-length(co)
n<-round(le/3,0)
win<-co[1:n] # long positions
n
loss<-co[(le-n):le]
loss # short positions
co_all<-c(win,loss)
co_all
w<- 1.2 # long position weight
w_short<- 1-w
set.seed(42)
#runif
ru<-runif(n , 0, 1)
# weigths sum
su<-sum(ru)
# runif/sum and trasnsforming into data frame
we_win<-data.frame(ru*w/su)
#colnanmes weigth
colnames(we_win)<-"we"
# row names from win
rownames(we_win)<-win
ru<-runif(length(loss), 0, 1)
set.seed(42)
su<-sum(ru)
# runif/sum and trasnsforming into data frame
we_loss<-data.frame(ru*w_short/su)
#colnanmes weigth
colnames(we_loss)<-"we"
# row names from loss
rownames(we_loss)<-loss
sum(we_loss) # set.seed(42)
we_loss
we_all<-rbind(we_win,we_loss)
we_all
data<-read.xlsx("dfx_2.xlsx")
date<-data[,1]
data<-data[,-1]
datax<- xts(data,
order.by = as.Date(date))
datax<-na.omit(datax)
ret<-apply(datax,2,Delt)
retx<- xts(ret,
order.by = as.Date(date))
retx<-na.omit(retx)
head(retx)
retx_all<-retx[,co_all]
covar<-cov(retx_all,use="complete.obs")
portfolio_std =covar %*% we_all[,1]
portfolio_std
twe<-t(we_all[,1])
portfolio_std_1=(twe%*%portfolio_std*252)^.5
ret_a<-df_merge2[co_all,1]
ret_a_f<-twe %*%ret_a
ret_a_f
library(openxlsx)
library(quantmod)
library(dplyr)
library(PerformanceAnalytics)
df_merge<-read.xlsx("data/df_merge.xlsx",rowNames=T)
treh<-0.2
df_merge2<- df_merge   %>%
filter(Sharpe_diff < treh & Sharpe_diff > -treh)
df_merge2
df_filtered<- df_merge2 %>% arrange(desc(in_Sharpe))
# to gt the names of those stocks
co<-rownames(df_filtered)
le<-length(co)
n<-round(le/3,0)
win<-co[1:n] # long positions
n
loss<-co[(le-n):le]
loss # short positions
co_all<-c(win,loss)
co_all
w<- 1.2 # long position weight
w_short<- 1-w
set.seed(42)
#runif
ru<-runif(n , 0, 1)
# weigths sum
su<-sum(ru)
# runif/sum and trasnsforming into data frame
we_win<-data.frame(ru*w/su)
#colnanmes weigth
colnames(we_win)<-"we"
# row names from win
rownames(we_win)<-win
ru<-runif(length(loss), 0, 1)
set.seed(42)
su<-sum(ru)
# runif/sum and trasnsforming into data frame
we_loss<-data.frame(ru*w_short/su)
#colnanmes weigth
colnames(we_loss)<-"we"
# row names from loss
rownames(we_loss)<-loss
sum(we_loss) # set.seed(42)
we_loss
we_all<-rbind(we_win,we_loss)
we_all
data<-read.xlsx("data/dfx_2.xlsx")
date<-data[,1]
data<-data[,-1]
datax<- xts(data,
order.by = as.Date(date))
datax<-na.omit(datax)
ret<-apply(datax,2,Delt)
retx<- xts(ret,
order.by = as.Date(date))
retx<-na.omit(retx)
head(retx)
retx_all<-retx[,co_all]
covar<-cov(retx_all,use="complete.obs")
portfolio_std =covar %*% we_all[,1]
portfolio_std
twe<-t(we_all[,1])
portfolio_std_1=(twe%*%portfolio_std*252)^.5
ret_a<-df_merge2[co_all,1]
ret_a_f<-twe %*%ret_a
ret_a_f
data<-read.xlsx("data/dfx_2.xlsx")
date<-data[,1]
data<-data[,-1]
datax<- xts(data,
order.by = as.Date(date))
datax<-na.omit(datax)
ret<-apply(datax,2,Delt)
retx<- xts(ret,
order.by = as.Date(date))
retx<-na.omit(retx)
head(retx[,1:5])
