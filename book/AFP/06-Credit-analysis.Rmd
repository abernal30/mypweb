# Credit analysis


In this chapter we will use the following libraries:

```{r warning=FALSE, message=FALSE}
library(openxlsx)
library(caret)
library(MASS)
```

```{r echo=FALSE}
data<-read.csv("https://raw.githubusercontent.com/abernal30/BookAFP/main/data/credit.csv")
               
dim<-dim(data)
```

In this chapter, we will cover credit allocation analysis (loan origination). The database credit.xlsx has historical information
on Lendingclub, <https://www.lendingclub.com/> fintech marketplace bank
at scale. On the spreadsheets, you will find the variable description.
The original data set has at least 2 million observations and 150
variables. Inside the file "credit.xlsx," you will find only `r dim[1]`
observations (rows) and `r dim[2]` columns. Each row represents a
Lendingclub client. We previously made the data cleaning (missing
values, correlated variables, Zero- and Near Zero-Variance Predictors).
To know more about data cleaning, see the 2nd chapter of this book.

In the next output, we see the variables of Lendingclub's customers when
they granted the loan. For example, the variable term is the term, in
years, of the loan, "annual_inc," which is the customer's annual income
when she got the loan.

```{r}
data<-read.csv("https://raw.githubusercontent.com/abernal30/BookAFP/main/data/credit.csv")
str(data[,1:10])
```

The variable "Default", winch originally has the name "loan_status", it
has two labels:

```{r}
table(data[,"Default"])
```

"Charge off" means that the credit grantor wrote your account off of
their receivables as a loss and is closed to future charges. When an
account displays a status of "charge off," it is closed to future use,
although the customer still owns the debt. For this example, we will
consider Charged Off equivalent to Default and Fully Paid as no default.

In a previous output, we show that the "Default" variable class is
"character," and a function we will apply below does only accept numeric
or factor variables. We transform that variable into "factor."

```{r}
data[,"Default"]<-factor(data[,"Default"])
```

### Prediction with the Logit model

First, we split the data set into training and test, 80% of the training
and 20% of the test data set. For an explanation of this procedure, see
chapter [Machine learning with market direction prediction: Logit].

We use the function sample when the data set is not a time series. Which
randomly generates dim[1]\*n numbers of the full data set. Where dim[1]
is the number of rows of the full data set, and n is a %, in this case,
80%.

```{r}
set.seed (1)
dim<-dim(data)
train_sample<-sample(dim[1],dim[1]*0.8)

train <- data[train_sample, ]
test  <- data[-train_sample, ]

```

Because the function "sample" generates random numbers, we use
"set.seed" to specify seeds to control the results; in other words, we
will always get the same results, even when we generate random numbers.

We will run the following logit model:

$$Default=\alpha_{0}\ +\beta_{1}\ term_{1}+\beta_{2}\ grade_{2}+...+\beta_{n}\ variable_{n}+e$$
Where $e$ is the error term.

The next code is to run the logit model using the train set:

```{r}
model<-glm(Default ~ term+grade ,data= train ,family=binomial())
summary(model)
```

The next code is to run the logit model with all the variables, using
the train set is:

$$Default=\alpha_{0}\ +\beta\ X+e$$ Where $X$ represents all the
variables inside the data bases, except "Default".

```{r }
model_all<-glm(Default ~. ,data=train ,family=binomial())
```

The next step is to make the prediction on the test data set, based on
the the "model_all".

```{r}
predict<-predict(model_all, newdata = test,type = "response")
head(predict)
```

The "type = response" argument is to transform the results into
probability.

In the previous output, we see that our prediction is not of the type
"Fully Paid" or "Charged Off," then we transform our forecast in that
categories. We set as threshold 0.5; if our prediction is higher than
0.5, then we transform it into "Fully Paid," and otherwise "Charged
Off." It is a common practice if you wonder why our threshold is 0.5.
Still, more importantly, after estimating the prediction accuracy of our
model, we could change this threshold to improve the prediction
performance.

```{r}
predicf_char<-ifelse(predict>.5,"Fully Paid","Charged Off")
head(predicf_char)
```

## Measuring model performance

To measure the performance of our prediction, we will use the confusion
Matrix. Before that, we need to transform our prediction into a factor.

```{r}

predict_factor<-factor(predicf_char)
caret::confusionMatrix(predict_factor,test[,"Default"])$table
```

The confusion Matrix categorizes our predictions according to whether
they match the actual value. One of the table's dimensions indicates the
possible categories of predicted values, while the other shows the same
for real (reference) values.

There are other measures that the "confusionMatrix" functions show. For
this chapter, we are only concerned about Accuracy and Sensitivity.

```{r}
confusionMatrix(predict_factor,test[,"Default"])
```

```{r, echo=FALSE}
co<-confusionMatrix(predict_factor,test[,"Default"])
a<-co$overall[1]
s<-co$byClass[1]
```

The accuracy is, therefore, a proportion representing the number of true
positives and negatives divided by the total number of predictions. In
this case, our mode Accuracy is `r a`

The sensitivity of a model (also called the true positive rate) measures
the proportion of positive examples correctly classified. For this
example, at the end of the "confusionMatrix" output we see that the
'Positive' Class is "Charged Off". Our mode Sensitivity is `r s`.

## Prediction with Linear Discriminant Analysis (LDA)

Why do we need another method when we already have the logistic model?
There are several reasons [@statistical_lerarning]:

• When the classes are well-separated, the parameter estimates for the
logistic regression model are surprisingly unstable. The linear
Discriminant Analysis method does not suffer from this problem.

• If the number of observations is small and the distribution of the
independent variables is approximately normal in each class, the linear
discriminant model is again more stable than the logistic regression
model.

For this chapter, we will use the LDA model to compare its Accuracy
against the Logit model.

The next code estimates the LDA model and makes the prediction:

```{r}
model_lda<-MASS::lda(Default~.,data=train)
pred_lda<-predict(model_lda, newdata = test)
```

The object "pred_lda" is an R-list, which contains the prediction, but
also many other statistics, then to apply the "confusionMatrix" we need
to get the prediction results:

```{r}
confusionMatrix(pred_lda[["class"]],test[,"Default"])
```

```{r, echo=FALSE}
co1<-confusionMatrix(pred_lda[["class"]],test[,"Default"])
a1<-co1$overall[1]
s1<-co1$byClass[1]
```

The Accuracy of the LDA model is `r a1`, and its Sensitivity is `r s1`.

In this case, the LDA Accuracy is higher than the logit model; the
sensitivity is the opposite. Depending on what we are interested in, the
model better predicts performance. In "credit allocation," we are
usually more concerned with the 'Positive' cases, in this case, "Charged
Off," because of the default risk.

### 3 Cross validation.

Cross-validation is a "resampling" method. It involves repeatedly
drawing samples from a training set and refitting a model of interest on each sample to obtain additional information about the model. Such an approach may allow us to get information that would not be available from fitting the model only once using the original training sample.

Instead of dividing the sample only once, this approach involves
randomly k-fold CV splits the set of observations into k groups, or
folds, of approximately equal size.

In other words, this procedure would validate it our Accuracy/sensitivity will be stable when we split the sample in training and test it several times. The Caret function "train" can optimize for Accuracy.

We start by applying it to the logit model:

```{r warning=FALSE}
# similar to the previous models
gbmFit1 <- train(Default ~ ., data = train,
                  
                 method = "glm",
                 
# in here we have the tuning parameters, in this case we use the "cv" method for cross, which is the number of times the model split the sample.                   
                            trControl = trainControl(method = "cv", number = 10),
                        trace=0,   metric="Accuracy")
gbmFit1
```

And for the LDA:

```{r}
# similar to the previous models
gbmFit1 <- train(Default ~ ., data = train,
                  
                 method = "lda",
                 
# in here we have the tuning parameters, in this case we use the "cv" method for cross, which is the number of times the model split the sample.                   
                            trControl = trainControl(method = "cv", number = 10),
                        trace=0,   metric="Accuracy")
gbmFit1
```

Our results are consistent with the previous result; the Accuracy of the
LDA is higher than the Logit one.

To improve the accuracy, we could also apply variable selection methods,
such as glmStepAIC.

***Warning*****:** The following code takes 20 minutes to run, depending
on the processor.

```{r eval=FALSE}
gbmFit1 <- train(Default ~ ., data = train, method = "glmStepAIC",
              
                 trControl = trainControl(method = "cv", number = 10),
                        trace=0,   metric="Accuracy")
    gbmFit1

```

    ## Generalized Linear Model with Stepwise Feature Selection
    ##
    ## 698 samples 
    ## 70 predictor 
    ## 2 classes: 'Charged Off', 'Fully Paid'
    ##
    ## No pre-processing Resampling: Cross-Validated (10 fold)
    ## Summary of sample sizes: 628, 629, 628, 628, 628, 628, ... 
    ## Resampling results:
    ##
    ## Accuracy   Kappa
    ## 0.9599149 0.8585923


The accuracy of 0.9599 is higher than the Logit model, which includes all variables 0.9470. 

To get the final model or the final variables, we apply the following:

```{r eval=FALSE}
step_var<-rownames(data.frame(gbmFit1$finalModel$coefficients))[-1] 
step_var
```

```{r echo=FALSE}
step_var<-c("term","purpose","revol_bal",           "total_rec_int","recoveries","last_pymnt_amnt","last_fico_range_high","open_act_il","total_cu_tl","num_accts_ever_120_pd","num_sats","num_tl_op_past_12m","total_bc_limit")
step_var
```

If we want to use those variables to further improve the model:

```{r}
train_step<-cbind(train[,"Default"],train[,step_var])
colnames(train_step)[1]<-"Default"
test_step<-cbind(test[,"Default"],test[,step_var])
colnames(test_step)[1]<-"Default"

```

For example, if we run the LDA model with those variables again:
```{r}
gbmFit1 <- train(Default ~ ., data = train_step,
                  
                 method = "lda",
                 
# in here we have the tuning parameters, in this case we use the "cv" method for cross, which is the number of times the model split the sample.                   
                            trControl = trainControl(method = "cv", number = 10),
                        trace=0,   metric="Accuracy")
gbmFit1
```

We got a higher accuracy.
