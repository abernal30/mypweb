[{"path":"index.html","id":"algorithms-and-financial-programming-in-r","chapter":"Algorithms and Financial Programming in R","heading":"Algorithms and Financial Programming in R","text":"book Algorithms Financial Programming R!work Aturo Bernal\nVisit GitHub repository site.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"started writing book guidance undergraduate courses related algorithms financial programming. extend content become book wish start learning program R want expand knowledge programming applications finance. codes book R written RStudio Markdowns worldwide known programming language.book aims get introductory programming competencies, well topics data cleaning, data analysis, machine learning, apply concepts finance.R files stored GitHub repository site.","code":""},{"path":"preface.html","id":"outline","chapter":"Preface","heading":"Outline","text":"","code":""},{"path":"r-basics.html","id":"r-basics","chapter":"1 R Basics","heading":"1 R Basics","text":"section covers topics required following chapters. suggest covering section someone yet gain previous knowledge R programming.","code":""},{"path":"r-basics.html","id":"r-markdown","chapter":"1 R Basics","heading":"1.1 1 R Markdown","text":"book, work R Markdowns, document format embed code chunks (R languages) documents. importantly, allows printing (knitr) authoring languages, including LaTeX, HTML, Text.See markdown content (Yihui Xie Grolemund 2019).","code":""},{"path":"r-basics.html","id":"coding-basics","chapter":"1 R Basics","heading":"1.2 2 Coding basics","text":"entities can create manipulate R called objects. may include variables, arrays numbers, character strings, functions, general structures. create objects applying assignment operator (‘<-’). consists two characters ‘<’ (“less ”) ‘-’ (“minus”) occurring strictly side--side, ‘points’ object receiving value expression (Team 2022).also apply operator ‘=’; however, experience, functions use “=” operator inside, programming language can interpret “=” operator variable creation.example, create object “”; winch assigned value 4.delete object environment. can also use function rm(). However, suggest using R-studio. introduction RStudio, suggest reviewing chapter 1 book (Ismay Kim 2019)","code":"\na<- 4 \nrm(a)"},{"path":"r-basics.html","id":"atomic-structures","chapter":"1 R Basics","heading":"1.3 Atomic structures","text":"objects frequently used finance numeric, character, vectors logical. known “atomic” structures since components identical. rest objects, like matrix Data frames, built atomic objects.type character (strings) objects using either matching double (“) single (’) quotes. example:use function “print” print object write object name.review object class, use function “class”:following example numeric objects.","code":"\nticker<-\"APPL\" \nticker\n#> [1] \"APPL\"\n# or \nprint(ticker)\n#> [1] \"APPL\"\nclass(ticker)\n#> [1] \"character\"\nnum<-4\nprint(num)\n#> [1] 4\n\n# To print the class of the object\nprint(class(num))\n#> [1] \"numeric\""},{"path":"r-basics.html","id":"vectors","chapter":"1 R Basics","heading":"1.4 Vectors","text":"R, vectors consist ordered collection numbers characters. -n programming languages, list. R, list another kind object.finance applications, use vectors store ticker names (character vectors) store stock price (numeric vector). built vectors applying function concatenate “c()”. example:can see, object class numeric vector taking class atomic objects, case, numeric. example character vector:Selecting element vector.select element, use brackets: “[]”. example, select first element vector “v2”:Also, select sub-sample:former example, just select sub-sample, object “v2” hasn’t changed (see environment, didn´t create object). want change object, need create new one.example, want delete element, use minus sign “-”. example 2nd element “v2”:case, object “v2” changed. Also, object “v2” now environment. Vectors mutable; winch means change element vector, example, changing element “Amazon” “Meta”:like add new element, example “Amazon_new”, need apply “c” function:","code":"\nv1<-c(160,165,167,145,145)\n\nprint(v1)\n#> [1] 160 165 167 145 145\nclass(v1)\n#> [1] \"numeric\"\nv2<-c(\"Apple\",\"Meta\",\"Amazon\")\nprint(v2)\n#> [1] \"Apple\"  \"Meta\"   \"Amazon\"\nclass(v2)\n#> [1] \"character\"\nv2[1]\n#> [1] \"Apple\"\nv2[1:2]\n#> [1] \"Apple\" \"Meta\"\nv2<-v2[-2]\nv2\n#> [1] \"Apple\"  \"Amazon\"\nv2[2]<-\"Meta\"\nv2\n#> [1] \"Apple\" \"Meta\"\nv2<-c(v2,\"Amazon_new\")\nv2\n#> [1] \"Apple\"      \"Meta\"       \"Amazon_new\""},{"path":"r-basics.html","id":"data-frames","chapter":"1 R Basics","heading":"1.5 Data frames","text":"Finance common use Data Frames, tabular-form data objects column can different form, , numeric character.example use data frame created library Wooldridge manipulations.Get data frame k401k library WooldridgeRemember library set functions someone created. Wooldridge library many data sets econometrics book author (Wooldridge 2020).import library, apply function library()import databases library, library must imported, just calling data set name, case “k401k”.can see, object class Data Frame.function “colnames” shows names column data frame. case, character vector:Sometimes convenient change column row names data frame; example, change name first column “prate_1”. case, use function “colnames” select, brackets, column number want change. changing column names vector, need establish assignment operator “<-”.show change row name, use “rownames” function. convenience, select first five rows.apply procedure made “colnames” function modify row data frame.Selecting rows columnsThere many ways select column row data frame.Selecting rows columns position, example, selecting first row, column 5. data frame two dimensions, rows columns, selecting also use brackets, separating rows columns aSelecting columns $ symbolMerging two data frames columns.Suppose following Data Frame:Print dimension data frame, applying function paste, print dim:Applying function cbind merge two data frames call object df3Note method duplicates column age.takeoff one columns, select number position adding minus symbolCreate new variable, tot_part_age (totpart/age) variable row names index, call index, data frame. Insert object df3.Eliminate 2nd row object df3 call df4.Apply function cbind merge df3 df4It show debug “Error data.frame(…, check.names = FALSE) :\narguments imply differing number rows: 6, 5”, means number rows .Careful: number rows data frame multiple another, coincidence, “cbind” function merge. However, R going fill missing values repeating values data frame.Try now function merge(x,y,.x=,.y=,=T F, .x=T F, .y=T F)merge function needs pivot reference variable make merge. case, column index identification id (share variable). id must unique value row must present data frames. Also, need specify want keep data data frame x y.","code":"\nlibrary(wooldridge) \nk4<-k401k\nclass(k4)\n#> [1] \"data.frame\"\ncolnames(k4)\n#> [1] \"prate\"   \"mrate\"   \"totpart\" \"totelg\"  \"age\"     \"totemp\"  \"sole\"   \n#> [8] \"ltotemp\"\ncolnames(k4)[1]<-\"prate_1\" \ncolnames(k4)\n#> [1] \"prate_1\" \"mrate\"   \"totpart\" \"totelg\"  \"age\"     \"totemp\"  \"sole\"   \n#> [8] \"ltotemp\"\nrownames(k4)[1:5]\n#> [1] \"1\" \"2\" \"3\" \"4\" \"5\"\nk4[1,5]\n#> [1] 8\nk4$age[1:10]\n#>  [1]  8  6 10  7 28  7 31 13 21 10\ndf1<-k4[1:6,c(\"prate_1\",\"totpart\",\"age\")]\ndf2<-k4[1:6,c(\"age\",\"totemp\")]\n\ndf3<-cbind(df1,df2)\nhead(df3,10) \n#>   prate_1 totpart age age totemp\n#> 1    26.1    1653   8   8   8709\n#> 2   100.0     262   6   6    315\n#> 3    97.6     166  10  10    275\n#> 4   100.0     257   7   7    500\n#> 5    82.5     591  28  28    933\n#> 6   100.0      92   7   7    143\ndim<-dim(df3) \ndim\n#> [1] 6 5\ndf1<-k4[1:6,c(\"prate_1\",\"totpart\",\"age\")]\ndf2<-k4[1:6,c(\"age\",\"totemp\")]\ndf3<-cbind(df1,df2)\ndf3\n#>   prate_1 totpart age age totemp\n#> 1    26.1    1653   8   8   8709\n#> 2   100.0     262   6   6    315\n#> 3    97.6     166  10  10    275\n#> 4   100.0     257   7   7    500\n#> 5    82.5     591  28  28    933\n#> 6   100.0      92   7   7    143\ndf3<-df3[,-3]\ndf3\n#>   prate_1 totpart age totemp\n#> 1    26.1    1653   8   8709\n#> 2   100.0     262   6    315\n#> 3    97.6     166  10    275\n#> 4   100.0     257   7    500\n#> 5    82.5     591  28    933\n#> 6   100.0      92   7    143\ndf3[\" tot_part_age\"]<-(df3[,\"totpart\"]/df3[,\"age\"])\ndf3[\"index\"]<-rownames(df3)\ndf4<-df3[-2,]\ndf5<-cbind(df3,df4)\ndf5<-merge(df3,df4,by.x=\"age\",by.y=\"age\")\ndf5\n#>   age prate_1.x totpart.x totemp.x  tot_part_age.x index.x prate_1.y totpart.y\n#> 1   7     100.0       257      500        36.71429       4     100.0       257\n#> 2   7     100.0       257      500        36.71429       4     100.0        92\n#> 3   7     100.0        92      143        13.14286       6     100.0       257\n#> 4   7     100.0        92      143        13.14286       6     100.0        92\n#> 5   8      26.1      1653     8709       206.62500       1      26.1      1653\n#> 6  10      97.6       166      275        16.60000       3      97.6       166\n#> 7  28      82.5       591      933        21.10714       5      82.5       591\n#>   totemp.y  tot_part_age.y index.y\n#> 1      500        36.71429       4\n#> 2      143        13.14286       6\n#> 3      500        36.71429       4\n#> 4      143        13.14286       6\n#> 5     8709       206.62500       1\n#> 6      275        16.60000       3\n#> 7      933        21.10714       5"},{"path":"r-basics.html","id":"xts-objects","chapter":"1 R Basics","heading":"1.6 xts objects","text":"xts class object provides uniform handling R’s different time-based data classes. Also, APIs, “quantmod”, download data xts format. example, library “xts” write xlsx file data set “sample_matrix.”next section covered read “xlsx” file.default, object class data frame. feature “xts” objects row names date objects. first, replace numerical row names dates inside object.useful functions can use “xts” objects, example, transforming weekly, monthly, quarterly, yearly, etc.Making sub sample:Note two examples, use “apply.monthly” function object like data_df_2, work rownames dates, can´t apply subset function object; generate empty object.","code":"\n#data(sample_matrix)\n#sm<-get(\"sample_matrix\")\n#data_df<-data.frame(sample_matrix)\n#date<-rownames(data_df)\n#data_df<-cbind(date,data_df)\n#write.xlsx(data_df,\"data/data_df.xlsx\")\nlibrary(openxlsx)\ndata_df<-read.xlsx(\"data/data_df.xlsx\")\ndate<-data_df[,1]\nrownames(data_df)<-date\n# Also I eliminate the dates in row one. \ndata_df_2<-data_df[,-1]\nlibrary(xts)\ndata_xts<- as.xts(data_df_2)\ndata_xt_m<-apply.monthly(data_xts,mean)\nsub_set<-subset(data_xts,\n  +index(data_xts)>=\"2007-05-01\" &\n  +index(data_xts)<=\"2007-06-30\")"},{"path":"r-basics.html","id":"reading-and-writing-csv-and-xlsx","chapter":"1 R Basics","heading":"1.7 Reading and writing CSV and xlsx","text":"libraries write open xlsx CSV file. suggest using “openxlsx”.open file use, File must directory need specify directory location; otherwise, error:","code":"\nwrite.xlsx(df5,\"data/df5.xlsx\")\nwrite.csv(df5,\"data/df5.csv\")\nlibrary(openxlsx)\nfdf5_x<-read.xlsx(\"data/df5.xlsx\")\nfdf5_c<-read.csv(\"data/df5.csv\")"},{"path":"clean.html","id":"clean","chapter":"2 Big data and data cleaning with datapro","heading":"2 Big data and data cleaning with datapro","text":"chapter, use library write data processing, “datapro” install run following code chunk:\ninstall run:library(devtools):remotes::install_github(“datanalyticss/datapro”)ordevtools::install_github(“datanalyticss/datapro”)chapter, use file credit_semioriginal.xlsx, historical information lendingclub, https://www.lendingclub.com/ fintech marketplace bank scale. original data set least 2 million observations 150 variables. find credit_semioriginal.xlsx first 1,000 observations 150 variables. Using 2 million rows sample make processor low, challenge try original data set see big data .dataset source:\nhttps://www.kaggle.com/wordsforthewise/lending-clubReview data structure credit data set descriptive statistics, first 10 columns.see numerical columns categorical. categorical mean elements characters.","code":"\nlibrary(datapro)\ndata<-read.csv(\"https://raw.githubusercontent.com/abernal30/BookAFP/main/data/credit_semioriginal.csv\")\nstr(data[,1:10])\n#> 'data.frame':    1000 obs. of  10 variables:\n#>  $ loan_amnt      : int  3600 24700 20000 35000 10400 11950 20000 20000 10000 8000 ...\n#>  $ funded_amnt    : int  3600 24700 20000 35000 10400 11950 20000 20000 10000 8000 ...\n#>  $ funded_amnt_inv: int  3600 24700 20000 35000 10400 11950 20000 20000 10000 8000 ...\n#>  $ term           : chr  \"36 months\" \"36 months\" \"60 months\" \"60 months\" ...\n#>  $ int_rate       : num  14 12 10.8 14.8 22.4 ...\n#>  $ installment    : num  123 820 433 830 290 ...\n#>  $ grade          : chr  \"C\" \"C\" \"B\" \"C\" ...\n#>  $ sub_grade      : chr  \"C4\" \"C1\" \"B4\" \"C5\" ...\n#>  $ emp_title      : chr  \"leadman\" \"Engineer\" \"truck driver\" \"Information Systems Officer\" ...\n#>  $ emp_length     : chr  \"10+ years\" \"10+ years\" \"10+ years\" \"10+ years\" ..."},{"path":"clean.html","id":"categorical-into-numerical-filtering-and-coditionals","chapter":"2 Big data and data cleaning with datapro","heading":"2.1 Categorical into numerical: filtering and coditionals","text":"several reasons transform numerical column variable categorical. detailed explanation suggest review chapter “Handling Text Categorical Attributes” book “Machine learning introductory guide R”. moment functions use chapter work variables categorical.see “loan_status” variable categorical. First review many categories column loan_status :data[,col][!duplicated(data[,“col”])]data name dataframe col column nameAnother possibility applying function categ library dataproThere 5 categories, going transform column verification_status numeric:Create filter, way loan_status contains Fully Paid Charged .data %>%\nfilter(col== “categ1” |col== “categ2”)result, now 873 rows.Besides “loan_status” three several categorical columns, example term, winch 2 categories:method use transform simple, example “36 months” take value one “60 months” value 2. column 3 categories, 3rd categories take value 3 .former example easy 3 categories, however, otherWe use charname function see many categorical variables . print first rows using head function.33 categorical columns. function “tonum” transform categorical column numeric, example transforming column “grade”, following categories:need specify data source column name.Finally, sure want transform data set numerical, function “asnum” reviews detect categorical columns transform numeric, result get data frame. apply function review now winch categorical columns, get ., reason, get error, run following code:","code":"\ncol <- \"loan_status\"\ndata[,col][!duplicated(data[,col])]\n#> [1] \"Fully Paid\"         \"Current\"            \"Charged Off\"       \n#> [4] \"In Grace Period\"    \"Late (31-120 days)\"\ncateg(data,col)\n#> [1] \"Fully Paid\"         \"Current\"            \"Charged Off\"       \n#> [4] \"In Grace Period\"    \"Late (31-120 days)\"#> [1] 5\nlibrary(dplyr)\n#col <- \"loan_status\"\ndata1 <- data %>%\n  filter(data[,\"loan_status\"] == \"Fully Paid\" | data[,\"loan_status\"] == \"Charged Off\")#> [1] 873\ncol <- \"term\"\ncat <- categ(data,col)\ncat\n#> [1] \"36 months\" \"60 months\"\nncat <- c(1:length(cat))\nncat\n#> [1] 1 2\ncat[1]\n#> [1] \"36 months\"\ncol_cat <- ifelse(data1[, col] == cat[1],ncat[1],data1[, col])\nhead(col_cat)\n#> [1] \"1\"         \"1\"         \"60 months\" \"60 months\" \"1\"         \"1\"\ncol_cat <- ifelse(data1[, col] == cat[1],ncat[1],ncat[2])\nhead(col_cat)\n#> [1] 1 1 2 2 1 1\ntail(col_cat)\n#> [1] 1 1 1 1 1 1\ndata1[1,\"mths_since_recent_bc\"]*2\n#> [1] 8\nhead(charname(data1))\n#> [1] \"term\"           \"grade\"          \"sub_grade\"      \"emp_title\"     \n#> [5] \"emp_length\"     \"home_ownership\"\ntail(charname(data1))\n#> [1] \"hardship_loan_status\"      \"disbursement_method\"      \n#> [3] \"debt_settlement_flag\"      \"debt_settlement_flag_date\"\n#> [5] \"settlement_status\"         \"settlement_date\"#> [1] 33\ncol <- \"grade\"\n\ncat <- categ(data,col)\n#cat <- data[,col][!duplicated(data[,col])]\ncat\n#> [1] \"C\" \"B\" \"F\" \"A\" \"E\" \"D\" \"G\"\ncol_cat2 <- tonum(data1,col)\nhead(col_cat2[,1:5])\n#>   loan_amnt funded_amnt funded_amnt_inv      term int_rate\n#> 1      3600        3600            3600 36 months    13.99\n#> 2     24700       24700           24700 36 months    11.99\n#> 3     20000       20000           20000 60 months    10.78\n#> 4     10400       10400           10400 60 months    22.45\n#> 5     11950       11950           11950 36 months    13.44\n#> 6     20000       20000           20000 36 months     9.17\n# Warning: this code may take a few minutes to finish, depending on the processor.\ndata2 <- datapro::asnum(data1)\nhead(charname(data2))#> [1] \"There are no character columns\"\ndata2<-read.csv(\"https://raw.githubusercontent.com/abernal30/BookAFP/main/data/credit_semioriginal_num.csv\")"},{"path":"clean.html","id":"missing-values","chapter":"2 Big data and data cleaning with datapro","heading":"2.2 Missing values","text":"treat missing values, suggest taking one following alternatives combination : ) eliminating columns significant amount missing values; ii) eliminating row missing(s) value(s) () located; iii) replace missing values Na´s statistic.firs alternative, lets first apply function “summaryna” detect columns 50 percent missing values:case 29 columns 50 percent missing values. like eliminate columns apply following:confirm, apply function summarynafor second alternative, eliminating rows missing(s) value(s) () located; applying na.omit function. However, careful, case row data frame least one missing value, cace delete rows data frame, like case:third alternative replacing missing values metric. us function “repnas”, object data3 wich already drop columns 50 percent missing values:","code":"\n\nna_perc <- datapro::summaryna(data2,.5)\nhead(na_perc)\n#>                                Percentage of NAs Column number\n#> mths_since_last_record                 0.8064147            27\n#> mths_since_last_major_derog            0.7090493            50\n#> annual_inc_joint                       0.9919817            53\n#> dti_joint                              0.9919817            54\n#> mths_since_recent_bc_dlq               0.7502864            86\n#> mths_since_recent_revol_delinq         0.6575029            88\ndata3 <- data2[,-na_perc[,2]]\nsummaryna(data3,.5)\n#> [1] \"There are no columns with missing values\"\ndata3_1 <- na.omit(data2)\n\ndata3 <- datapro::repnas(data3,\"median\")"},{"path":"clean.html","id":"zero--and-near-zero-variance-predictors","chapter":"2 Big data and data cleaning with datapro","heading":"2.3 Zero- and Near Zero-Variance Predictors","text":"wil use library (Kuhn 2019) section. Zero- Near Zero-Variance Predictors variables columns single unique value, winch refereed “zero-variance predictor”. Also, variables might unique values occur low frequencies. cases may cause troubles estimating econometric machine learning model.function nearZeroVar shows columns number Zero- Near Zero-Variance Predictors.understand better “nearZeroVar” function , lets estimate metrics settlement_date columns, first apply function “table”, gives frequency per category:851 rows label 1, 1 rows label 2 .“frequency ratio” frequency prevalent value second frequent value. near one well-behaved predictors large highly-unbalanced, “grade” column :estimate “frequency ratio” e apply “.max” function gives position frequency prevalent value:get frequent value:second frequent value beThen, “frequency ratio” :default, threshold 19 (95/5), terms object “nzv” show column “frequency ratio” higher 19.Also, nearZeroVar function shows “percent unique values,” number unique values divided total number rows data frame (times 100). approaches zero granularity data increases.percent unique number categories, case 851 column estimated applying first function “length”:number rows data frame, obtain applying fucntio “dim”:“percent unique values” :object “nzv” shows “frequency ratio” “percent unique values”; however, apply filter get columns “frequency ratio” “percent unique values” higher respective threshold apply “nearZeroVar” time without argument “saveMetrics= TRUE”:object nzv_2 shows position colums tresholds higher, create object excluding columns.","code":"\nlibrary(caret)\nnzv <- nearZeroVar(data3,saveMetrics= TRUE)\nhead(nzv)\n#>                 freqRatio percentUnique zeroVar   nzv\n#> loan_amnt        1.173077    24.7422680   FALSE FALSE\n#> funded_amnt      1.173077    24.7422680   FALSE FALSE\n#> funded_amnt_inv  1.173077    24.7422680   FALSE FALSE\n#> term             3.546875     0.2290951   FALSE FALSE\n#> int_rate         1.129032     3.7800687   FALSE FALSE\n#> installment      1.125000    70.4467354   FALSE FALSE\ntail(nzv)\n#>                           freqRatio percentUnique zeroVar  nzv\n#> hardship_loan_status      289.00000     0.5727377   FALSE TRUE\n#> disbursement_method         0.00000     0.1145475    TRUE TRUE\n#> debt_settlement_flag       38.68182     0.2290951   FALSE TRUE\n#> debt_settlement_flag_date 170.20000     1.4891180   FALSE TRUE\n#> settlement_status          77.36364     0.4581901   FALSE TRUE\n#> settlement_date           283.66667     1.8327606   FALSE TRUE\nt<-table(data3[,col])\nt\n#> \n#>   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16 \n#> 851   1   1   1   2   1   1   2   2   1   1   1   1   3   2   2\nw <- which.max(t)\nw\n#> 1 \n#> 1\nt[w]\n#>   1 \n#> 851\nmax(t[-w])\n#> [1] 3\nt[w]/max(t[-w])\n#>        1 \n#> 283.6667\nlength(table(data3[,col])) \n#> [1] 16\ndim(data3)[1]\n#> [1] 873\n(length(table(data3[,col]))/dim(data3)[1])*100\n#> [1] 1.832761\nnzv_2 <- nearZeroVar(data3)\nnzv_2 \n#>  [1]  14  26  32  33  34  39  44  48  49  50  51  52  74  75  93  94  95 100 105\n#> [20] 106 107 108 109 110 111 112 113 114 115 116 117\ndata4<-data3[,-nzv_2]"},{"path":"clean.html","id":"collinearity","chapter":"2 Big data and data cleaning with datapro","heading":"2.4 Collinearity","text":"Collinearity two variables closely related one another. presence collinearity can pose problems model estimation, regression, difficult separate individual effects collinear variables response (James et al. 2017).function “cor” estimate correlation matrix, function “findCorrelation” shows correlated variables “n” (cutoff argument). case, apply cut-80%.argument “names = T” get column’s names correlated variables. Still, wish cut variables data frame, add argument, getting column numbers:cut variables data4 following way:","code":"#>  [1] \"open_acc\"                   \"num_sats\"                  \n#>  [3] \"total_rev_hi_lim\"           \"total_bc_limit\"            \n#>  [5] \"total_rec_prncp\"            \"acc_open_past_24mths\"      \n#>  [7] \"total_pymnt_inv\"            \"total_pymnt\"               \n#>  [9] \"loan_amnt\"                  \"funded_amnt\"               \n#> [11] \"funded_amnt_inv\"            \"num_tl_op_past_12m\"        \n#> [13] \"sub_grade\"                  \"int_rate\"                  \n#> [15] \"num_rev_accts\"              \"num_bc_sats\"               \n#> [17] \"tot_hi_cred_lim\"            \"total_bal_ex_mort\"         \n#> [19] \"num_actv_rev_tl\"            \"fico_range_low\"            \n#> [21] \"last_fico_range_high\"       \"tot_cur_bal\"               \n#> [23] \"revol_util\"                 \"total_il_high_credit_limit\"\n#> [25] \"bc_util\"                    \"collection_recovery_fee\"\nhc<-findCorrelation(descrCor, cutoff = .8)\nhc\n#>  [1] 25 78 54 85 32 58 31 30  1  2  3 79  8  5 76 72 83 84 71 22 39 42 28 86 61\n#> [26] 35\ndata_corr <- data4[ , -hc]\nhead(data_corr[,1:5])\n#>   term installment grade emp_title emp_length\n#> 1    1      123.03     3       300          4\n#> 2    1      820.28     3       210          4\n#> 3    2      432.66     2       624          4\n#> 4    2      289.91     6       127          6\n#> 5    1      405.18     3       634          7\n#> 6    1      637.58     2       637          4"},{"path":"graphs.html","id":"graphs","chapter":"3 APIS and R graphs","heading":"3 APIS and R graphs","text":"","code":""},{"path":"graphs.html","id":"apis-application-programming-interface","chapter":"3 APIS and R graphs","heading":"3.1 API´s (Application Programming Interface)","text":"","code":""},{"path":"graphs.html","id":"quantmod-api","chapter":"3 APIS and R graphs","heading":"3.1.1 Quantmod API","text":"Quantitative Financial Modelling FrameworkThe (Wickham Francois 2016) package R designed assist quantitative trader developing, testing, deploying statistically based trading models.function getSymbols wrapper load data various sources, local remote. One popular default yahoo fiance,\ngetSymbols(“Symbol”):can see, object class xts.eliminate warnings, add argument warnings = FGetting data specific date:getSymbols(“symbol”, =“YY/m/d”,=“YY/m/d”): YY= 4 digit year, m= 2 digit month, d= 2 digit day.previous way download data store information object environment, default assign name object, previous example “AMZN”.Another way store different name, form example name “data”, following:object class list. get information apply following code.previous method useful tickers like Bitcoin:apparently name object environment “BTC-USD”, want modify name, show debug like: “object ‘BTC’ found”even looks “BTC-USD” name kind brackets:Another alternative, helpful apply loops get information environment :like download intra-day data, get data Alphavantage, applying function getSymbols.get dividends, xts object ticker name, needs R environment, case “AAPL”:","code":"\nlibrary(quantmod)\ngetSymbols(\"AAPL\")\n#> [1] \"AAPL\"\nclass(AAPL)\n#> [1] \"xts\" \"zoo\"\ngetSymbols(\"AMZN\", from=\"2020/04/01\",to=\"2022/04/04\")\n#> [1] \"AMZN\"\napple2 <- new.env()\ngetSymbols(\"AAPL\", env=apple2)\n#> [1] \"AAPL\"\nclass(apple2)\n#> [1] \"environment\"\napple3<-apple2[[\"AAPL\"]]\ngetSymbols(\"BTC-USD\")\n#> [1] \"BTC-USD\"\nbit<-BTC-USD[,4]\n# `BTC-USD`\nbit<-get(\"BTC-USD\")\nhead(bit)\n#>            BTC-USD.Open BTC-USD.High BTC-USD.Low BTC-USD.Close BTC-USD.Volume\n#> 2014-09-17      465.864      468.174     452.422       457.334       21056800\n#> 2014-09-18      456.860      456.860     413.104       424.440       34483200\n#> 2014-09-19      424.103      427.835     384.532       394.796       37919700\n#> 2014-09-20      394.673      423.296     389.883       408.904       36863600\n#> 2014-09-21      408.085      412.426     393.181       398.821       26580100\n#> 2014-09-22      399.100      406.916     397.130       402.152       24127600\n#>            BTC-USD.Adjusted\n#> 2014-09-17          457.334\n#> 2014-09-18          424.440\n#> 2014-09-19          394.796\n#> 2014-09-20          408.904\n#> 2014-09-21          398.821\n#> 2014-09-22          402.152\ngetSymbols(\"AAPL\", src=\"av\", api.key=\"yourKey\", output.size=\"full\",\nperiodicity=\"intraday\",interval=\"5min\")\n#> [1] \"AAPL\"\nap_div<-getDividends(\"AAPL\")"},{"path":"graphs.html","id":"nasdaq-data-link-api","chapter":"3 APIS and R graphs","heading":"3.1.2 Nasdaq Data Link API","text":"need create account (suggest free academic account) Nasdaq Data Link (NDL) (Quandl).class objects (NDL) Data Frames, add argument: type=“xts” get “xts” object.example, “Emerging Markets High Grade Corporate Bond Index Yield”","code":"#>            [,1]\n#> 1998-12-31 8.48\n#> 1999-01-04 8.48\n#> 1999-01-05 8.46\n#> 1999-01-06 8.37\n#> 1999-01-07 8.43\n#> 1999-01-08 8.40\nlibrary(Quandl)\neurex<-Quandl(\"ML/EMHGY\", api_key=\"type your own api_key here\",type=\"xts\")\nhead(eurex)"},{"path":"graphs.html","id":"in-house-api","chapter":"3 APIS and R graphs","heading":"3.1.3 In-house Api","text":"Harvesting web rvest (get tickers).code read characters web pages. case, get tickers yahoo finance.","code":"\nlibrary(xml2)\nlibrary(rvest)\n# the page of of the criptocurrencies\n#yf <- \"https://finance.yahoo.com/cryptocurrencies/\"\n\n# for the IPC components\nyf <- \"https://finance.yahoo.com/quote/%5EMXX/components?p=%5EMXX\"\nhtml <- read_html(yf)\n# To get the node a, wich contains characters \nnode <- html_nodes(html,\"a\")\n\n# To read the text in the node\nnode<-html_text(node, trim=TRUE)\n\n# To get the elements that have USD (the tickers). For the IPC tickers, replace \"USD\" with \".MX\". For other tickers, print the node object and look for patterns or select by rows.\n#tickers<-grep(pattern = \"USD\", x = node, value = TRUE)\n\ntickers<-grep(pattern = \".MX\", x = node, value = TRUE)\n\n# to get only the first 5 tickers\n\ntickers1<-tickers\ntickers1\n#>  [1] \"IENOVA.MX\"     \"FEMSAUBD.MX\"   \"ALPEKA.MX\"     \"GAPB.MX\"      \n#>  [5] \"KIMBERA.MX\"    \"MEGACPO.MX\"    \"GFNORTEO.MX\"   \"AMXL.MX\"      \n#>  [9] \"CUERVO.MX\"     \"TLEVISACPO.MX\" \"ASURB.MX\"      \"GCARSOA1.MX\"  \n#> [13] \"ALSEA.MX\"      \"BIMBOA.MX\"     \"PINFRA.MX\"     \"OMAB.MX\"      \n#> [17] \"CEMEXCPO.MX\"   \"GRUMAB.MX\"     \"GCC.MX\"        \"GMEXICOB.MX\"  \n#> [21] \"AC.MX\"         \"BBAJIOO.MX\"    \"LABB.MX\"       \"BOLSAA.MX\"    \n#> [25] \"GENTERA.MX\"    \"LIVEPOLC1.MX\"  \"MEXCHEM.MX\"    \"KOFL.MX\"      \n#> [29] \"PEOLES.MX\"     \"SITESB1.MX\"\ngetSymbols(tickers1[1:2]) \n#> [1] \"IENOVA.MX\"   \"FEMSAUBD.MX\""},{"path":"graphs.html","id":"basic-r-graphs","chapter":"3 APIS and R graphs","heading":"3.2 Basic R-Graphs","text":"basic function creating graph : plot(x, type = “h”, col = “red”, lwd = 10, “xlab”,“ylab”).example use 100 rows “AAPL” ticker.help tue function “plot.xy” “plot.default” can see arguments.\nPlot HSI close priceTo add anhother time series, example: apple*1.1Note: applying function plot, add another line, must data frame, otherwise may appear plot.adding leggends, function legends work data frames, xts., transform dataframe\n.data.frame(object).add legend, works data frames objects, xts. need transform data frame.","code":"\n\ngetSymbols(\"AAPL\")\n#> [1] \"AAPL\"\napp<-AAPL[1:100,4]\nplot(app,type=\"l\",col=\"green\", main = \"APPL\")\nplot(app,type=\"l\",col=\"green\", main = \"APPL\")\n\nlines(app[,1]*1.1,col=\"red\")\nappdf<-as.data.frame(app)\napp_1df<-as.data.frame(app[,1]*1.1)\n\nplot(appdf[,1],type=\"l\",col=\"green\", main = \"APPL\") \nlines(app_1df[,1],col=\"red\") \nlegend(x= \"topleft\", legend = c(\"app\",\"app*1.1\"),lty = 1,lwd=2,col=c(\"green\",\"red\"))"},{"path":"logit.html","id":"logit","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4 Machine learning with market direction prediction: Logit","text":"Machine Learning (ML) application Artificial Intelligence (AI) provides AI system ability automatically learn environment apply lessons make better decisions. variety algorithms machine learning uses iteratively learn, describe improve data, spot patterns, perform actions patterns (Tatsat, Puri & Lookabaugh, 2021).chapter covers machine learning market direction prediction. particular, forecast market moves either upward downward.logistic regression (Logit) Linear Discriminant Analysis (LDA) models help us fit model using binary behavior () forecast market direction. Logistic regression.","code":""},{"path":"logit.html","id":"data-preparation","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4.1 Data preparation","text":"following commands create variable direction either direction (1) direction (0). words, direction signal o buying signal buying. example, direction variable created short SMA greater long SMA zero otherwise.First make plot.Now create signal.machine learning example, predict BNB price, model :\\[ signal_{t}=\\alpha\\ +\\beta1\\ macd_{t-1}+\\beta2\\ rsi_{t-2} +\\beta3\\ bb +\\ e \\]difference independent variable, case signal.separate sample training testing. training data set used building model process, testing dataset used evaluation purposes.","code":"\nlibrary(\"quantmod\")\nticker<-\"BNB-USD\"\ndata<-getSymbols(ticker,from=\"2021-08-01\",to=\"2022-04-18\",warnings =FALSE,auto.assign=FALSE)\ndata<-data[,4]\ncolnames(data)<-\"bnb\"\nlag2<-12\nlag3<-9\nlag4<-26\navg<-SMA(data[,1],lag2) # var1\navg2<-SMA(data[,1],lag4) # var1\ndata2<-cbind(data,avg,avg2)\ndata2<-na.omit(data2)\n#par(mfrow=c(2,1))\nplo0<-as.data.frame(data2[,1])\nplo<-as.data.frame(data2[,2])\nplo1<-as.data.frame(data2[,3])\n\nplot(plo0[,1],col=\"blue\",type=\"l\")\nlines(plo[,1],col=\"red\",type=\"l\")\nlines(plo1[,1],col=\"green\",type=\"l\")\nlegend(x= \"topleft\", legend = c(\"actual\",\"short-sma\",\"long-sma\"),lty = 1,lwd=2,col=c(\"blue\",\"red\",\"green\"))\nsignal<-ifelse(data2[,\"SMA\"]>data2[,\"SMA.1\"],1,0)\nplot(signal)\nstd<- rollapply(data[,1],lag2,sd) # var2\ncolnames(std)<-\"std\"\n\nmacd<- MACD(data[,1], lag2,lag3,lag4, \"SMA\") # var2\nmacd2<- MACD(data[,1], 11,25,8, \"SMA\") \ncolnames(macd)[2]<-\"macd_signal\"\n\n\nrsi<-  RSI(data[,1],lag2,\"SMA\")# var3\nrsi2<-  RSI(data[,1],13,\"SMA\")# var3\n\nbb <- BBands(data2[,1], n = 10, maType=\"SMA\", sd = 2) \n\n# Agregar el nombre de signal en lugar de sig\ndata2<-cbind(signal,std,macd,rsi,bb)\ncolnames(data2)[1]<-\"signal\"\ndata2<-na.omit(data2)\nN<-dim(data2)[1]\nn_train<-round(N*.8,0)\npart<-index(data2)[n_train]\n#This is the test data set.\ntrain<-subset(data2,\n  +index(data2)>=index(data2)[1] &\n  +index(data2)<=part)\n\n# The subset of the training data set.\ntest<-subset(data2,\n  +index(data2)>=part+1 &\n  +index(data2)<=\"2022-04-18\")\ny1<-test[,1] "},{"path":"logit.html","id":"logistic-regression","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4.2 Logistic Regression","text":"linear regression assumes response variable Y quantitative. many situations, response variable instead qualitative. example, eye color qualitative, taking values blue, brown, green. Often qualitative variables referred categorical ; use terms interchangeably.binary response model, interest lies primarily response probability. However, can use OLS estimate model, linear binary response model. apply Logistic regression.glm(y ~.,data= ,family=binomial())expect forecast 0,1 result, signal. transform probabilistic model.\nexp(x)/(1+exp(x))Even result probability, 0 1, require result 0,1. transform , creating binary variable, takes value 1 probability higher 0.5, zero lower 0.5.","code":"\nmodel<-glm(signal~.,data= train,family=binomial())\npred<-predict(model,test)\npred\n#> 2022-03-05 2022-03-06 2022-03-07 2022-03-08 2022-03-09 2022-03-10 2022-03-11 \n#>  1.4071871  0.7838451  1.1688270  1.5835724  0.6400374  1.1018749  1.5094064 \n#> 2022-03-12 2022-03-13 2022-03-14 2022-03-15 2022-03-16 2022-03-17 2022-03-18 \n#> -1.2026024 -2.0134675 -2.4429877 -1.9810255 -0.7260440 -1.5145946 -0.3496281 \n#> 2022-03-19 2022-03-20 2022-03-21 2022-03-22 2022-03-23 2022-03-24 2022-03-25 \n#> -0.7131193 -0.6484040 -1.3564305  1.4355937  1.6560035  2.0060744  3.8742966 \n#> 2022-03-26 2022-03-27 2022-03-28 2022-03-29 2022-03-30 2022-03-31 2022-04-01 \n#>  3.5987035  3.9839923  4.1817522  4.1877168  3.9090570  2.9184529  3.7917209 \n#> 2022-04-02 2022-04-03 2022-04-04 2022-04-05 2022-04-06 2022-04-07 2022-04-08 \n#>  3.4630973  2.7885380  2.6969620  2.6106934  4.1880525  3.2083981  3.0614675 \n#> 2022-04-09 2022-04-10 2022-04-11 2022-04-12 2022-04-13 2022-04-14 2022-04-15 \n#>  2.6747753  2.6183627  1.9073456  2.0025636  0.6854552  1.0431508 -0.2146935 \n#> 2022-04-16 2022-04-17 2022-04-18 \n#> -0.1493334  0.3593882  0.6847538\nprob<-exp(pred)/(1+exp(pred))\npredf<-ifelse(prob>.5,1,0)\nplot(predf)\n# comparar vs el dato real que estaé en y1"},{"path":"logit.html","id":"confusion-matrix","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4.3 Confusion matrix","text":"measure accuracy prediction, categorical variables, 0,1, confusion matrix table indicates possible categories predicted values, actual values.True Positive (TP): Correctly classified class interest. True Negative (TN) Correctly classified class interest. False Positive (FP) Incorrectly classified class interest. False Negative (FN): Incorrectly classified class interest.confusion matrix, one mesures interest accuracy, defined :\\[ accuracy =\\frac{TP+TN}{TP+TN+FP+FN}\\]formula, terms TP, TN, FP, FN refer number times model’s predictions fell categories. accuracy therefore proportion represents number true positives true negatives, divided total number predictions.factor(x,levels=c(1,0))\nconfusionMatrix(pred,real)SensitivityFinding useful classifier often involves balance predictions overly conservative overly aggressive. example, e-mail filter guarantee eliminate every spam message aggressively eliminating nearly every ham message time. hand, guaranteeing ham message inadvertently filtered might require us allow unacceptable amount spam pass filter. pair performance measures captures trade : sensitivity specificity.sensitivity model (also called true positive rate) measures proportion positive examples correctly classified. Therefore, shown following formula, calculated number true positives divided total number positives, correctly classified (true positives) well incorrectly classified (false negatives):\\[sensitivity =\\frac{TP}{TP+FN}\\]","code":"\nlibrary(caret)\npredf2<-as.data.frame(predf)\npredf3<-factor(predf2[,1],levels=c(1,0))   \nreal<-factor(y1,levels=c(1,0))\nconfusionMatrix(predf3,real) \n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction  1  0\n#>          1 25  8\n#>          0  0 12\n#>                                         \n#>                Accuracy : 0.8222        \n#>                  95% CI : (0.6795, 0.92)\n#>     No Information Rate : 0.5556        \n#>     P-Value [Acc > NIR] : 0.0001572     \n#>                                         \n#>                   Kappa : 0.625         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.0133283     \n#>                                         \n#>             Sensitivity : 1.0000        \n#>             Specificity : 0.6000        \n#>          Pos Pred Value : 0.7576        \n#>          Neg Pred Value : 1.0000        \n#>              Prevalence : 0.5556        \n#>          Detection Rate : 0.5556        \n#>    Detection Prevalence : 0.7333        \n#>       Balanced Accuracy : 0.8000        \n#>                                         \n#>        'Positive' Class : 1             \n#> "},{"path":"logit.html","id":"linear-discriminant-analysis-lda","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4.4 Linear Discriminant Analysis LDA","text":"need another method, logistic regression?\nseveral reasons:• classes well-separated, parameter estimates \nlogistic regression model surprisingly unstable. Linear discriminant\nanalysis suffer problem.• number observations n small distribution predictors X approximately normal classes, linear discriminant model stable logistic regression model.• Finally, 2 categories, example, c(-1,0,1).Signal creation, now 3 categories c(-1,0,1)1 es señal de compra, -1 de venta (o venta en corto), y cero es hacer nada (ni comprar ni vender).Combinig data2 signalTraining test partitionLDA model prediction\nlda(x~.,data= , prior = c(1,1,1)/3)","code":"\nlibrary(\"quantmod\")\nticker<-\"BNB-USD\"\ndata<-getSymbols(ticker,from=\"2018-08-01\",to=\"2022-04-18\",warnings =FALSE,auto.assign=FALSE)\ndata<-data[,4]\ncolnames(data)<-\"bnb\"\nlag2<-12\nlag3<-18\nlag4<-26\navg<-SMA(data[,1],lag2) # var1\navg2<-SMA(data[,1],lag3) # var1\navg3<-SMA(data[,1],lag4) # var1\ndata2<-cbind(data,avg,avg2,avg3)\ndata2<-na.omit(data2)\n\nstd<- rollapply(data[,1],lag2,sd) # var2\ncolnames(std)<-\"std\"\nmacd<- MACD(data[,1], lag2,lag3,lag4, \"SMA\") # var2\ncolnames(macd)[2]<-\"macd_signal\"\nrsi<-  RSI(data[,1],lag2,\"SMA\")# var3\nbb <- BBands(data2[,1], n = 10, maType=\"SMA\", sd = 2) \n\ndata2<-cbind(data,std,macd,rsi,bb)\n#colnames(data2)[1]<-\"signal\"\ndata2<-na.omit(data2)\nsignal <- ifelse(data2[,1]> data2[,'up'] & data2[,'macd']> data2[,'macd_signal'],1,ifelse(data2[,1]< data2[,'dn'] & data2[,'macd'] <data2[,'macd_signal'],-1,0))\nplot(signal)\n#We first replace bnp by signal \ndata2<-data2[,-1]\n\n# Eliminate up, because is causing issues (correlated with mavg, and does not allow estimate the model)\ndata2<-data2[,-6]\ndata2<-cbind(signal,data2)\ncolnames(data2)[1]<-\"signal\"\nN<-dim(data2)[1]\nn_train<-round(N*.8,0)\npart<-index(data2)[n_train]\n#This is the test data set.\ntrain<-subset(data2,\n  +index(data2)>=index(data2)[1] &\n  +index(data2)<=part)\n\n# The subset of the training data set.\ntest<-subset(data2,\n  +index(data2)>=part+1 &\n  +index(data2)<=\"2022-04-18\")\n\ntrain<-train[,-6]\ntest<-test[,-6]\n\ny1<-test[,1] # contiene la varaible que voy a pronosticar\n#test<-test[,-1] # las variables independientes, que voy a usar para haver mi pronóstico\nlibrary(MASS)\nmodellda<-lda(signal~.,data= train, prior = c(1,1,1)/3)\npred<-predict(modellda,test)\npred<-pred[[\"class\"]]\nclass(pred)\n#> [1] \"factor\"\n\nlibrary(caret) \nreal<-factor(y1,levels=c(-1,0,1))\nconfusionMatrix(pred,real) \n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction  -1   0   1\n#>         -1   8  18   0\n#>         0    0 199   0\n#>         1    0  27  11\n#> \n#> Overall Statistics\n#>                                           \n#>                Accuracy : 0.8289          \n#>                  95% CI : (0.7778, 0.8724)\n#>     No Information Rate : 0.9278          \n#>     P-Value [Acc > NIR] : 1               \n#>                                           \n#>                   Kappa : 0.4079          \n#>                                           \n#>  Mcnemar's Test P-Value : NA              \n#> \n#> Statistics by Class:\n#> \n#>                      Class: -1 Class: 0 Class: 1\n#> Sensitivity            1.00000   0.8156  1.00000\n#> Specificity            0.92941   1.0000  0.89286\n#> Pos Pred Value         0.30769   1.0000  0.28947\n#> Neg Pred Value         1.00000   0.2969  1.00000\n#> Prevalence             0.03042   0.9278  0.04183\n#> Detection Rate         0.03042   0.7567  0.04183\n#> Detection Prevalence   0.09886   0.7567  0.14449\n#> Balanced Accuracy      0.96471   0.9078  0.94643"},{"path":"logit.html","id":"bibliography","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4.5 Bibliography","text":"Tatsat, H; Puri, S; Lookabaugh, B. (2021).Machine Learning Data Science Blueprints Finance. Sebastopol, CA. : O’Reilly Media.","code":""},{"path":"big-data-and-machine-learning.html","id":"big-data-and-machine-learning","chapter":"5 Big data and machine learning","heading":"5 Big data and machine learning","text":"Predicting market direction price quite challenging task market data involves lots noise. market moves either upward downward, nature market movement binary (Jeet Vat, 2017).chapter, use OLS, predict 1 day open price, price 19 April 2022, cryptocurency, case Bianance, “BNB-USD”. ideas code adapted Jeet Vat (2017).Binance Launched July 2017, Binance biggest cryptocurrency exchange globally based daily trading volume. Binance aims bring cryptocurrency exchanges forefront financial activity globally. idea behind Binance’s name show new paradigm global finance — Binary Finance, Binance.","code":""},{"path":"big-data-and-machine-learning.html","id":"data-preparation-1","chapter":"5 Big data and machine learning","heading":"5.1 Data preparation","text":"going start OLS (Ordinary least square) model. independent variables using lags independent variable.simplicity, suppose predict 19 April 2022 BNB price, make regression, OLS, following model:\\[bnb_{t}=\\alpha\\ +\\beta1\\ bnb_{t-1}+\\beta2\\ bnb_{t-2} + e \\]\\(\\alpha\\) intercept, \\(beta\\) parameters estimated, \\(bnb_{t-1}\\) bnb price traiding previouse day, case 18 April 2022, \\(bnb_{t-2}\\) bnb price day , 17 April 2022, “e” error term regression. words, price today explained price yesterday day yesterday.data<-stats::lag(y,lag)Para realizar la regresión por OLS\nlm(bnb~.,data=data)Also, suppose found following result regression:\n\\[bnb_{t}=\\ 0.1253373\\ +\\ 0.9955164\\ bnb_{t-1}\\ -0.0047468\\ bnb_{t-2}\\]case, forecast 2022-04-14 :0.1253373 + 0.9955164 * 9.05+ -0.0047468*9.28 = 9.0907102The parameters \\(bnb_{t-1}\\) \\(bnb_{t-2}\\) ultimate ante penultimate known prices, following chunk print last known prices:However, last example exposition purposes. reality, need test independent variables besides lags dependent variable. session, besides lags values close prices dependent variable, going add variables used technical analysis, moving average, standard deviation, RSI (see appendix detailed explanation), MACD, , predictive power market direction. indicators can constructed using following commands:","code":"\nlibrary(\"quantmod\")\nticker<-\"CEMEXCPO.MX\"\ny<-getSymbols(ticker,from=\"2021-01-01\",to=\"2022-04-18\",warnings =FALSE,auto.assign=FALSE)\n\ny<-y[,1]\ncolnames(y)<-\"bnb\"\nhead(y)\n#>              bnb\n#> 2021-01-04 10.32\n#> 2021-01-05 10.38\n#> 2021-01-06 10.71\n#> 2021-01-07 11.61\n#> 2021-01-08 11.72\n#> 2021-01-11 11.54\nlag<-1\nlag1<-2\ndata<-stats::lag(y,lag)\ndata2<-stats::lag(y,lag1)\ndata<-cbind(y,data,data2)\ncolnames(data)[2:3]<-c(\"bnb_1\",\"bnb_2\")\nmodel<-lm(bnb~.,data=data)\nsummary(model)\n#> \n#> Call:\n#> lm(formula = bnb ~ ., data = data)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.10065 -0.19085 -0.03598  0.20284  0.91287 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.125337   0.131553   0.953    0.341    \n#> bnb_1        0.995516   0.056048  17.762   <2e-16 ***\n#> bnb_2       -0.004747   0.056243  -0.084    0.933    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.3295 on 319 degrees of freedom\n#>   (2 observations deleted due to missingness)\n#> Multiple R-squared:  0.973,  Adjusted R-squared:  0.9729 \n#> F-statistic:  5753 on 2 and 319 DF,  p-value: < 2.2e-16\ntail(y)\n#>              bnb\n#> 2022-04-06 10.20\n#> 2022-04-07  9.87\n#> 2022-04-08  9.63\n#> 2022-04-11  9.13\n#> 2022-04-12  9.28\n#> 2022-04-13  9.05"},{"path":"big-data-and-machine-learning.html","id":"variable-creation","chapter":"5 Big data and machine learning","heading":"5.2 Variable creation","text":"SMA Calculate moving averages\nSMA(x, n = 10, …), x time serie, n Number periods average overSMA Calculate moving averages\nSMA(x, n = 10, …), x time serie, n Number periods average overThe rollapply function applying function rolling margins array, case used make moving standard deviation.\nrollapply(x,n,sd), sd standard deviationThe rollapply function applying function rolling margins array, case used make moving standard deviation.\nrollapply(x,n,sd), sd standard deviationThe MACD moving average converge diverge (see Appendix)MACD moving average converge diverge (see Appendix)MACD(x, nFast = 12, nSlow = 26, nSig = 9, maType=SMA EMA)MACD(x, nFast = 12, nSlow = 26, nSig = 9, maType=SMA EMA)RSI relative strength index\nRSI(x, n = 14, maType=SMA EMA)RSI relative strength index\nRSI(x, n = 14, maType=SMA EMA)model :\n\\[bnb_{t}=\\alpha\\ +\\beta1\\ bnb_{t-1}+\\beta2\\ bnb_{t-2} +\\beta3\\ sma +\\\\ \\beta4\\ std\\ +\\beta5\\ macd\\ + \\beta6\\ rsi +\\ e\\]see, lags new variables, many missing values early dates, apply na.omit, eliminate rows nas.","code":"\nlag2<-6\nlag3<-9\nlag4<-26\navg<-SMA(data[,1],lag2) # var1\nstd<- rollapply(data[,1],lag2,sd) # var2\ncolnames(std)<-\"std\"\n\nmacd<- MACD(data[,1], lag2,lag3,lag4, \"SMA\") # var2\ncolnames(macd)[2]<-\"macd_signal\"\n  \nrsi<-  RSI(data[,1],lag2,\"SMA\")# var3\n\n\ndata2<-cbind(data,avg,std,macd,rsi)\nhead(data2)\n#>              bnb bnb_1 bnb_2      SMA       std macd macd_signal rsi\n#> 2021-01-04 10.32    NA    NA       NA        NA   NA          NA  NA\n#> 2021-01-05 10.38 10.32    NA       NA        NA   NA          NA  NA\n#> 2021-01-06 10.71 10.38 10.32       NA        NA   NA          NA  NA\n#> 2021-01-07 11.61 10.71 10.38       NA        NA   NA          NA  NA\n#> 2021-01-08 11.72 11.61 10.71       NA        NA   NA          NA  NA\n#> 2021-01-11 11.54 11.72 11.61 11.04667 0.6480638   NA          NA  NA\ndata2<-na.omit(data2)"},{"path":"big-data-and-machine-learning.html","id":"sub-samples","chapter":"5 Big data and machine learning","heading":"5.3 Sub samples","text":"separate sample training testing. training data set used building model process, testing dataset used evaluation purposes.code automate sub-sample creation, usually split 80% training set 20% test set.case, “2022-01-31” date represents 80% observations, starting date “2022-01-01”.use function subset:name<-subset(object,\n+index(object)>=“YY-mm-dd” &\n+index(object)<=“YY-mm-dd”)forecast vs real data, going takeout real data BNB prices test set, store object call y1.","code":"\nN<-dim(data2)[1]\nn_train<-round(N*.8,0)\npart<-index(data2)[n_train]\npart\n#> [1] \"2022-01-19\"\n\n#This is the test data set.\ntrain<-subset(data2,\n  +index(data2)>=index(data2)[1] &\n  +index(data2)<=part)\n\n# The subset of the training data set.\ntest<-subset(data2,\n  +index(data2)>=part+1 &\n  +index(data2)<=\"2022-04-18\")\ny1<-test[,1] \nhead(test)\n#>              bnb bnb_1 bnb_2      SMA       std       macd macd_signal\n#> 2022-01-20 13.60 13.81 13.98 13.79333 0.1376469  1.3139639  0.35574084\n#> 2022-01-21 12.99 13.60 13.81 13.66167 0.3566184  0.5931441  0.38949477\n#> 2022-01-24 11.95 12.99 13.60 13.37167 0.7823405 -0.5824040  0.37404430\n#> 2022-01-25 11.75 11.95 12.99 13.01333 0.9633829 -1.9506069  0.28418240\n#> 2022-01-26 12.45 11.75 11.95 12.75833 0.8524416 -2.7895361  0.14200830\n#> 2022-01-27 12.58 12.45 11.75 12.55333 0.6792545 -3.4441501 -0.05296758\n#>                  rsi\n#> 2022-01-20 60.504202\n#> 2022-01-21 21.167883\n#> 2022-01-24 12.500000\n#> 2022-01-25  3.463203\n#> 2022-01-26 23.890785\n#> 2022-01-27 28.719723"},{"path":"big-data-and-machine-learning.html","id":"making-the-model","chapter":"5 Big data and machine learning","heading":"5.4 Making the model","text":"estimate OLS model aplying function lm,lm(bnb~.,data=train)make prediction, need apply function predict, test set.first prediction, 2022-04-04, 433.6627, \\[ bnb_{t}=\\ -20.28047\\ +\\ 0.06146\\ bnb_{t-1}\\ -0.27979\\ bnb_{t-2} + \\\\1.15805\\ sma +\\ 0.33934206\\ std -3.76813\\ macd\\\\ -6.68031\\ macd\\_ signal\\ + 0.74661\\ rsi \\]\n.","code":"\nmodel1<-lm(bnb~.,data=train)\nsummary(model1)\n#> \n#> Call:\n#> lm(formula = bnb ~ ., data = train)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.69820 -0.10951 -0.00933  0.11320  0.60485 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.5477293  0.1620272  -3.380 0.000853 ***\n#> bnb_1        0.1457424  0.0561005   2.598 0.009999 ** \n#> bnb_2       -0.2267351  0.0577742  -3.925 0.000116 ***\n#> SMA          1.0653057  0.0755343  14.104  < 2e-16 ***\n#> std          0.1499802  0.1105334   1.357 0.176180    \n#> macd        -0.1363421  0.0185553  -7.348 3.68e-12 ***\n#> macd_signal -0.0078826  0.0297987  -0.265 0.791615    \n#> rsi          0.0157337  0.0008957  17.566  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.1936 on 225 degrees of freedom\n#> Multiple R-squared:  0.9794, Adjusted R-squared:  0.9788 \n#> F-statistic:  1531 on 7 and 225 DF,  p-value: < 2.2e-16\npred<-predict(model1,test)\nhead(pred)\n#> 2022-01-20 2022-01-21 2022-01-24 2022-01-25 2022-01-26 2022-01-27 \n#>   13.77998   13.15960   12.89724   12.57447   12.92974   12.99951\npred\n#> 2022-01-20 2022-01-21 2022-01-24 2022-01-25 2022-01-26 2022-01-27 2022-01-28 \n#>  13.779981  13.159602  12.897245  12.574469  12.929737  12.999507  12.685198 \n#> 2022-01-31 2022-02-01 2022-02-02 2022-02-03 2022-02-04 2022-02-08 2022-02-09 \n#>  12.674351  13.164583  13.231589  12.902274  12.376268  12.398194  12.288838 \n#> 2022-02-10 2022-02-11 2022-02-14 2022-02-15 2022-02-16 2022-02-17 2022-02-18 \n#>  12.189159  11.870961  11.616433  11.602455  11.849330  11.805935  11.508132 \n#> 2022-02-21 2022-02-22 2022-02-23 2022-02-24 2022-02-25 2022-02-28 2022-03-01 \n#>  11.610196  11.626087  11.632577  10.649812  10.687183  10.913722  10.774856 \n#> 2022-03-02 2022-03-03 2022-03-04 2022-03-07 2022-03-08 2022-03-09 2022-03-10 \n#>  10.635500  10.604802  11.168214  10.184670   9.857020  10.413452  10.581255 \n#> 2022-03-11 2022-03-14 2022-03-15 2022-03-16 2022-03-17 2022-03-18 2022-03-22 \n#>  10.225329  10.223843  10.369493  10.614031  10.502113  10.582636  11.071733 \n#> 2022-03-23 2022-03-24 2022-03-25 2022-03-28 2022-03-29 2022-03-30 2022-03-31 \n#>  11.033770  11.012177  10.943852  10.833943  10.842324  10.863476  10.648959 \n#> 2022-04-01 2022-04-04 2022-04-05 2022-04-06 2022-04-07 2022-04-08 2022-04-11 \n#>  10.758467  10.740732  10.560178  10.382403   9.955414   9.879946   9.675480 \n#> 2022-04-12 2022-04-13 \n#>   9.624368   9.583836"},{"path":"big-data-and-machine-learning.html","id":"accuracy-of-the-prediction","chapter":"5 Big data and machine learning","heading":"5.5 Accuracy of the prediction","text":"Lest make plot forecast vs real value BNB.Finaly, measure accuracy prediction, apply Root Mean Square Error (RMSE). gives idea much error system typically makes predictions. formula RMSE :\\[RMSE =\\frac{1}{n}\\ \\sum_{=1}^{n} (y_{}-\\hat{f(x_{}))^{2}} \\]\n$ $ prediction ith observation (actual), $ y_{} $ observation ith independent variable, n number observations.\\[\\hat{f(x_{})}=\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n}\\]RMSE computed using training data used fit model, accurately referred training RMSE., RMSE close zero, better.\nsqrt(mean((real-forecast)^2,na.rm = T ))","code":"\npred2<-as.data.frame(pred)\ny2<-as.data.frame(y1)\nall<-cbind(y2,pred2)\nplot(all[,1],type = \"l\",col=\"blue\",ylab=\"x\")\nlines(all[,2],col=\"green\")\nlegend(x= \"topleft\", legend = c(\"real\",\"prediction\"),lty = 1,lwd=2,col=c(\"blue\",\"green\"))\nsqrt(mean((all[,1]-all[,2])^2,na.rm = T ))\n#> [1] 0.310193"},{"path":"big-data-and-machine-learning.html","id":"appendix","chapter":"5 Big data and machine learning","heading":"5.6 Appendix","text":"MACD signals (investopedia).Moving Average Convergence Divergence (MACD) trend-following momentum indicator shows relationship two moving averages security’s price. MACD calculated subtracting 26-period Exponential Moving Average (EMA) 12-period EMA.result calculation MACD line. nine-day EMA MACD called “signal line,” plotted MACD line, can signal buy sell. Traders may buy security MACD crosses signal line sell - short - security MACD crosses signal line.exponential moving average (EMA) type moving average (MA) places greater weight significance recent data points. exponential moving average also referred exponentially weighted moving average. exponentially weighted moving average reacts significantly recent price changes simple moving average (SMA), applies equal weight observations period.next example, default, function MACD creates 12 days EMA 26-days EMA.relative strength index (RSI)momentum indicator measures magnitude recent price changes evaluate overbought oversold conditions price stock asset. RSI displayed oscillator (line graph moves two extremes) can reading 0 100. indicator originally developed J. Welles Wilder Jr. introduced seminal 1978 book, New Concepts Technical Trading Systems.Relative Strength Index (RSI) calculates ratio recent upward price movements absolute price movement. Developed J. Welles Wilder. RSI calculation RSI = 100 - 100 / ( 1 + RS ), RS smoothed ratio ‘average’ gains ‘average’ losses. ‘averages’ aren’t true averages, since ’re divided value n number periods gains/losses.Traditional interpretation usage RSI values 70 indicate security becoming overbought overvalued may primed trend reversal corrective pullback (drop stock) price. RSI reading 30 indicates oversold undervalued condition (Investopedia).","code":""},{"path":"big-data-and-machine-learning.html","id":"bibliography-1","chapter":"5 Big data and machine learning","heading":"5.7 Bibliography","text":"Jeet, P Vat, P. (2017). Learning Quantitative finance R (2017), Packt Publishing, Birmingham, UK.","code":""},{"path":"credit-analysis.html","id":"credit-analysis","chapter":"6 Credit analysis","heading":"6 Credit analysis","text":"chapter use following libraries:chapter, cover credit allocation analysis (loan origination). database credit.xlsx historical information\nLendingclub, https://www.lendingclub.com/ fintech marketplace bank\nscale. spreadsheets, find variable description.\noriginal data set least 2 million observations 150\nvariables. Inside file “credit.xlsx,” find 873\nobservations (rows) 71 columns. row represents \nLendingclub client. previously made data cleaning (missing\nvalues, correlated variables, Zero- Near Zero-Variance Predictors).\nknow data cleaning, see 2nd chapter book.next output, see variables Lendingclub’s customers \ngranted loan. example, variable term term, \nyears, loan, “annual_inc,” customer’s annual income\ngot loan.variable “Default”, winch originally name “loan_status”, \ntwo labels:“Charge ” means credit grantor wrote account \nreceivables loss closed future charges. \naccount displays status “charge ,” closed future use,\nalthough customer still owns debt. example, \nconsider Charged equivalent Default Fully Paid default.previous output, show “Default” variable class \n“character,” function apply accept numeric\nfactor variables. transform variable “factor.”","code":"\nlibrary(openxlsx)\nlibrary(caret)\nlibrary(MASS)\ndata<-read.csv(\"https://raw.githubusercontent.com/abernal30/BookAFP/main/data/credit.csv\")\nstr(data[,1:10])\n#> 'data.frame':    873 obs. of  10 variables:\n#>  $ Default            : chr  \"Fully Paid\" \"Fully Paid\" \"Fully Paid\" \"Fully Paid\" ...\n#>  $ term               : int  1 1 2 2 1 1 1 1 1 1 ...\n#>  $ installment        : num  123 820 433 290 405 ...\n#>  $ grade              : int  3 3 2 6 3 2 2 1 2 3 ...\n#>  $ emp_title          : num  299 209 623 126 633 636 481 540 631 314 ...\n#>  $ emp_length         : int  3 3 3 5 6 3 3 8 3 5 ...\n#>  $ home_ownership     : int  1 1 1 1 3 1 1 3 1 1 ...\n#>  $ annual_inc         : num  55000 65000 63000 104433 34000 ...\n#>  $ verification_status: int  1 1 1 2 2 1 1 1 1 1 ...\n#>  $ purpose            : int  3 10 4 6 3 3 6 2 2 9 ...\ntable(data[,\"Default\"])\n#> \n#> Charged Off  Fully Paid \n#>         145         728\ndata[,\"Default\"]<-factor(data[,\"Default\"])"},{"path":"credit-analysis.html","id":"prediction-with-the-logit-model","chapter":"6 Credit analysis","heading":"6.0.1 Prediction with the Logit model","text":"First, split data set training test, 80% training\n20% test data set. explanation procedure, see\nchapter Machine learning market direction prediction: Logit.use function sample data set time series. \nrandomly generates dim[1]*n numbers full data set. dim[1]\nnumber rows full data set, n %, case,\n80%.function “sample” generates random numbers, use\n“set.seed” specify seeds control results; words, \nalways get results, even generate random numbers.run following logit model:\\[Default=\\alpha_{0}\\ +\\beta_{1}\\ term_{1}+\\beta_{2}\\ grade_{2}+...+\\beta_{n}\\ variable_{n}+e\\]\n\\(e\\) error term.next code run logit model using train set:next code run logit model variables, using\ntrain set :\\[Default=\\alpha_{0}\\ +\\beta\\ X+e\\] \\(X\\) represents \nvariables inside data bases, except “Default”.next step make prediction test data set, based \n“model_all”.“type = response” argument transform results \nprobability.previous output, see prediction type\n“Fully Paid” “Charged ,” transform forecast \ncategories. set threshold 0.5; prediction higher \n0.5, transform “Fully Paid,” otherwise “Charged\n.” common practice wonder threshold 0.5.\nStill, importantly, estimating prediction accuracy \nmodel, change threshold improve prediction\nperformance.","code":"\nset.seed (1)\ndim<-dim(data)\ntrain_sample<-sample(dim[1],dim[1]*0.8)\n\ntrain <- data[train_sample, ]\ntest  <- data[-train_sample, ]\nmodel<-glm(Default ~ term+grade ,data= train ,family=binomial())\nsummary(model)\n#> \n#> Call:\n#> glm(formula = Default ~ term + grade, family = binomial(), data = train)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.3183   0.3755   0.4647   0.5723   1.3079  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  3.75411    0.34435  10.902  < 2e-16 ***\n#> term        -0.69219    0.24816  -2.789  0.00528 ** \n#> grade       -0.44522    0.09073  -4.907 9.25e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 631.12  on 697  degrees of freedom\n#> Residual deviance: 572.50  on 695  degrees of freedom\n#> AIC: 578.5\n#> \n#> Number of Fisher Scoring iterations: 4\nmodel_all<-glm(Default ~. ,data=train ,family=binomial())\npredict<-predict(model_all, newdata = test,type = \"response\")\nhead(predict)\n#>           10           18           21           23           24           26 \n#> 1.000000e+00 2.220446e-16 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00\npredicf_char<-ifelse(predict>.5,\"Fully Paid\",\"Charged Off\")\nhead(predicf_char)\n#>            10            18            21            23            24 \n#>  \"Fully Paid\" \"Charged Off\"  \"Fully Paid\"  \"Fully Paid\"  \"Fully Paid\" \n#>            26 \n#>  \"Fully Paid\""},{"path":"credit-analysis.html","id":"measuring-model-performance","chapter":"6 Credit analysis","heading":"6.1 Measuring model performance","text":"measure performance prediction, use confusion\nMatrix. , need transform prediction factor.confusion Matrix categorizes predictions according whether\nmatch actual value. One table’s dimensions indicates \npossible categories predicted values, shows \nreal (reference) values.measures “confusionMatrix” functions show. \nchapter, concerned Accuracy Sensitivity.accuracy , therefore, proportion representing number true\npositives negatives divided total number predictions. \ncase, mode Accuracy 0.9485714The sensitivity model (also called true positive rate) measures\nproportion positive examples correctly classified. \nexample, end “confusionMatrix” output see \n‘Positive’ Class “Charged ”. mode Sensitivity 0.9642857.","code":"\n\npredict_factor<-factor(predicf_char)\ncaret::confusionMatrix(predict_factor,test[,\"Default\"])$table\n#>              Reference\n#> Prediction    Charged Off Fully Paid\n#>   Charged Off          27          8\n#>   Fully Paid            1        139\nconfusionMatrix(predict_factor,test[,\"Default\"])\n#> Confusion Matrix and Statistics\n#> \n#>              Reference\n#> Prediction    Charged Off Fully Paid\n#>   Charged Off          27          8\n#>   Fully Paid            1        139\n#>                                           \n#>                Accuracy : 0.9486          \n#>                  95% CI : (0.9046, 0.9762)\n#>     No Information Rate : 0.84            \n#>     P-Value [Acc > NIR] : 8.743e-06       \n#>                                           \n#>                   Kappa : 0.8263          \n#>                                           \n#>  Mcnemar's Test P-Value : 0.0455          \n#>                                           \n#>             Sensitivity : 0.9643          \n#>             Specificity : 0.9456          \n#>          Pos Pred Value : 0.7714          \n#>          Neg Pred Value : 0.9929          \n#>              Prevalence : 0.1600          \n#>          Detection Rate : 0.1543          \n#>    Detection Prevalence : 0.2000          \n#>       Balanced Accuracy : 0.9549          \n#>                                           \n#>        'Positive' Class : Charged Off     \n#> "},{"path":"credit-analysis.html","id":"prediction-with-linear-discriminant-analysis-lda","chapter":"6 Credit analysis","heading":"6.2 Prediction with Linear Discriminant Analysis (LDA)","text":"need another method already logistic model?\nseveral reasons (James et al. 2017):• classes well-separated, parameter estimates \nlogistic regression model surprisingly unstable. linear\nDiscriminant Analysis method suffer problem.• number observations small distribution \nindependent variables approximately normal class, linear\ndiscriminant model stable logistic regression\nmodel.chapter, use LDA model compare Accuracy\nLogit model.next code estimates LDA model makes prediction:object “pred_lda” R-list, contains prediction, \nalso many statistics, apply “confusionMatrix” need\nget prediction results:Accuracy LDA model 0.9542857, Sensitivity 0.8571429.case, LDA Accuracy higher logit model; \nsensitivity opposite. Depending interested , \nmodel better predicts performance. “credit allocation,” \nusually concerned ‘Positive’ cases, case, “Charged\n,” default risk.","code":"\nmodel_lda<-MASS::lda(Default~.,data=train)\npred_lda<-predict(model_lda, newdata = test)\nconfusionMatrix(pred_lda[[\"class\"]],test[,\"Default\"])\n#> Confusion Matrix and Statistics\n#> \n#>              Reference\n#> Prediction    Charged Off Fully Paid\n#>   Charged Off          24          4\n#>   Fully Paid            4        143\n#>                                           \n#>                Accuracy : 0.9543          \n#>                  95% CI : (0.9119, 0.9801)\n#>     No Information Rate : 0.84            \n#>     P-Value [Acc > NIR] : 2.372e-06       \n#>                                           \n#>                   Kappa : 0.8299          \n#>                                           \n#>  Mcnemar's Test P-Value : 1               \n#>                                           \n#>             Sensitivity : 0.8571          \n#>             Specificity : 0.9728          \n#>          Pos Pred Value : 0.8571          \n#>          Neg Pred Value : 0.9728          \n#>              Prevalence : 0.1600          \n#>          Detection Rate : 0.1371          \n#>    Detection Prevalence : 0.1600          \n#>       Balanced Accuracy : 0.9150          \n#>                                           \n#>        'Positive' Class : Charged Off     \n#> "},{"path":"credit-analysis.html","id":"cross-validation.","chapter":"6 Credit analysis","heading":"6.2.1 3 Cross validation.","text":"Cross-validation “resampling” method. involves repeatedly\ndrawing samples training set refitting model interest sample obtain additional information model. approach may allow us get information available fitting model using original training sample.Instead dividing sample , approach involves\nrandomly k-fold CV splits set observations k groups, \nfolds, approximately equal size.words, procedure validate Accuracy/sensitivity stable split sample training test several times. Caret function “train” can optimize Accuracy.start applying logit model:LDA:results consistent previous result; Accuracy \nLDA higher Logit one.improve accuracy, also apply variable selection methods,\nglmStepAIC.Warning: following code takes 20 minutes run, depending\nprocessor.accuracy 0.9599 higher Logit model, includes variables 0.9470.get final model final variables, apply following:want use variables improve model:example, run LDA model variables :got higher accuracy.","code":"\n# similar to the previous models\ngbmFit1 <- train(Default ~ ., data = train,\n                  \n                 method = \"glm\",\n                 \n# in here we have the tuning parameters, in this case we use the \"cv\" method for cross, which is the number of times the model split the sample.                   \n                            trControl = trainControl(method = \"cv\", number = 10),\n                        trace=0,   metric=\"Accuracy\")\ngbmFit1\n#> Generalized Linear Model \n#> \n#> 698 samples\n#>  70 predictor\n#>   2 classes: 'Charged Off', 'Fully Paid' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 627, 628, 628, 629, 628, 629, ... \n#> Resampling results:\n#> \n#>   Accuracy   Kappa    \n#>   0.9470376  0.8144761\n# similar to the previous models\ngbmFit1 <- train(Default ~ ., data = train,\n                  \n                 method = \"lda\",\n                 \n# in here we have the tuning parameters, in this case we use the \"cv\" method for cross, which is the number of times the model split the sample.                   \n                            trControl = trainControl(method = \"cv\", number = 10),\n                        trace=0,   metric=\"Accuracy\")\ngbmFit1\n#> Linear Discriminant Analysis \n#> \n#> 698 samples\n#>  70 predictor\n#>   2 classes: 'Charged Off', 'Fully Paid' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 628, 629, 629, 628, 628, 628, ... \n#> Resampling results:\n#> \n#>   Accuracy   Kappa    \n#>   0.9527329  0.8275475\ngbmFit1 <- train(Default ~ ., data = train, method = \"glmStepAIC\",\n              \n                 trControl = trainControl(method = \"cv\", number = 10),\n                        trace=0,   metric=\"Accuracy\")\n    gbmFit1## Generalized Linear Model with Stepwise Feature Selection\n##\n## 698 samples \n## 70 predictor \n## 2 classes: 'Charged Off', 'Fully Paid'\n##\n## No pre-processing Resampling: Cross-Validated (10 fold)\n## Summary of sample sizes: 628, 629, 628, 628, 628, 628, ... \n## Resampling results:\n##\n## Accuracy   Kappa\n## 0.9599149 0.8585923\nstep_var<-rownames(data.frame(gbmFit1$finalModel$coefficients))[-1] \nstep_var#>  [1] \"term\"                  \"purpose\"               \"revol_bal\"            \n#>  [4] \"total_rec_int\"         \"recoveries\"            \"last_pymnt_amnt\"      \n#>  [7] \"last_fico_range_high\"  \"open_act_il\"           \"total_cu_tl\"          \n#> [10] \"num_accts_ever_120_pd\" \"num_sats\"              \"num_tl_op_past_12m\"   \n#> [13] \"total_bc_limit\"\ntrain_step<-cbind(train[,\"Default\"],train[,step_var])\ncolnames(train_step)[1]<-\"Default\"\ntest_step<-cbind(test[,\"Default\"],test[,step_var])\ncolnames(test_step)[1]<-\"Default\"\ngbmFit1 <- train(Default ~ ., data = train_step,\n                  \n                 method = \"lda\",\n                 \n# in here we have the tuning parameters, in this case we use the \"cv\" method for cross, which is the number of times the model split the sample.                   \n                            trControl = trainControl(method = \"cv\", number = 10),\n                        trace=0,   metric=\"Accuracy\")\ngbmFit1\n#> Linear Discriminant Analysis \n#> \n#> 698 samples\n#>  13 predictor\n#>   2 classes: 'Charged Off', 'Fully Paid' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 628, 628, 627, 629, 628, 628, ... \n#> Resampling results:\n#> \n#>   Accuracy   Kappa    \n#>   0.9612612  0.8568619"},{"path":"rational-agent-and-behavioral-finance-in-investment.html","id":"rational-agent-and-behavioral-finance-in-investment","chapter":"7 Rational agent and behavioral finance in investment","heading":"7 Rational agent and behavioral finance in investment","text":"chapter use following libraries:Around 1970, economists argued efficient market instantaneously reflect available information particular financial security, called Efficient Market Hypothesis EMH (Fama 1970). , arbitrage opportunities difficult exist, used argue markets needed predictable. Academics reasonably content EMH in1987 stock market behavior 1987 bizarre. year began Dow Jones Industrial Average’s historic collapse. interesting 1987 trading folklore activities leading academic economists fit behavioral finance point view, EMH point view. Economists actively discussing acting financial markets seemed believe markets predictable, key principle modern behavioral finance (Burton Shah 2013).designing systematic trading platforms, traders trading systems aim generate trading signals consistently produce positive outcomes many trades. Usually, trades test successfully trading systems large amounts past historical data. scientific method analyzing particular financial security may lie determining whether security price changes random . price changes random, probability detecting consistently profitable trading opportunity particular security small. hand, price changes nonrandom, financial security persistent predictability analyzed . , possible measure relative availability trading opportunities market inefficiency tests (Aldridge 2010). summary, tests detect new information takes slowly asset prices, arbitrage opportunities exist, market inefficient.chapter, apply test proposed (Wooldridge 2020) identify arbitrage opportunities find inefficient markets.","code":"\nlibrary(quantmod)\nlibrary(lmtest)\nlibrary(openxlsx)\nlibrary(dplyr)"},{"path":"rational-agent-and-behavioral-finance-in-investment.html","id":"emh-test-on-historical-returns-for-one-asset","chapter":"7 Rational agent and behavioral finance in investment","heading":"7.1 EMH test on historical returns for one asset","text":"Suppose \\(y_{t}\\) daily price S&P500. strict form efficient markets hypothesis establishes historical information index day t help predict index. use past information \\(y_{t}\\):\\[y_t=\\beta_0 +\\beta_1\\ y_{t-1} + \\beta_2\\ y_{t-2}+u_t, \\]market efficient following true:\\[E(\\beta_0 +\\beta_1\\ y_{t-1} + \\beta_2\\ y_{t-2}+u_t )= E(y_t)\\]term left expected value \\(y_{t}\\), given historical information index \\(y_{t-1} ,y_{t-2},....\\). words, expected value depend historical information. However, previous equation false, implies use information predict current price. previous equation false, use information past predict current price.Suppose want make EMH test returns S&P 500 index. next code, download S&P 500 index “2019-10-20” “2022-10-20”. Remember yahoo finance, ticker name “^GSPC”.take close price index, apply function Delt estimate returns.\\[y_t= \\beta_0 +\\beta_1\\ y_{t-1} + \\beta_2\\ y_{t-2}+u_t\\]run previous model, need create variables store data frame. apply function lag create lagged variables-convenience, change column names.apply OLS regression:Remember, significant beta1 coefficient reject EMH.example, use past information price moving average forecast Biance price.\\[bnb_{t}=\\alpha\\ +\\beta1\\ bnb_{t-1}+\\beta2\\ bnb_{t-2} +\\beta3\\ sma +\\ e\\]","code":"\nticker<-\"^GSPC\"\ngetSymbols(ticker, from=\"2019-10-20\", to=\"2022-10-20\")\n#> [1] \"^GSPC\"\nclose<-GSPC[,4]\nret<-Delt(close)\nlag1<-stats::lag(ret[,1],1)\nlag2<-stats::lag(ret[,1],2)\nret<-cbind(ret, lag1,lag2)\ncolnames(ret)<-c(\"SP500\",\"SP500_lag1\",\"SP500_lag2\")\nmodel<-lm(SP500 ~.,data =ret)\nsummary(model)\n#> \n#> Call:\n#> lm(formula = SP500 ~ ., data = ret)\n#> \n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -0.111441 -0.005786  0.000537  0.007425  0.092978 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.0004309  0.0005521   0.780 0.435371    \n#> SP500_lag1  -0.1990963  0.0362312  -5.495 5.35e-08 ***\n#> SP500_lag2   0.1248616  0.0362416   3.445 0.000602 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.01514 on 750 degrees of freedom\n#>   (3 observations deleted due to missingness)\n#> Multiple R-squared:  0.06649,    Adjusted R-squared:  0.06401 \n#> F-statistic: 26.71 on 2 and 750 DF,  p-value: 6.219e-12"},{"path":"rational-agent-and-behavioral-finance-in-investment.html","id":"emh-test-for-variance-for-one-asset","chapter":"7 Rational agent and behavioral finance in investment","heading":"7.2 EMH test for variance for one asset","text":"financial time series, stock returns, expected returns depend past returns (Market efficient), variance returns . example, model:\\[r_t= \\beta_0 +\\beta_1\\ r_{t-1}+u_t\\]apply test verify variance returns effect o returns:\\[u^2_{t}= \\delta_0 +\\delta_1\\ r_{t-1}+e_t\\]previous model heteroskedasticity test, applya Breusch-Pagan test heteroskedasticity, using function bptest(model)Suppose test significant former result (p-value <10%). EMH, use variance standard deviation make prediction.example, use past information standard deviation moving average, \\(sdma\\), forecast Biance price.\\[bnb_{t}=\\alpha\\ +\\beta1\\ bnb_{t-1}+\\beta2\\ bnb_{t-2} +\\beta3\\ sma +\\beta4\\ sdma +\\ e\\]See chapter 5 book example. “Big data machine learning”\nhttps://www.arturo-bernal.com/book/AFP/big-data--machine-learning.htmlWe show another example next section.","code":"\nlmtest::bptest(model)\n#> \n#>  studentized Breusch-Pagan test\n#> \n#> data:  model\n#> BP = 27.216, df = 2, p-value = 1.23e-06"},{"path":"rational-agent-and-behavioral-finance-in-investment.html","id":"emh-tests-for-the-variance-for-many-assets-to-build-a-portfolio","chapter":"7 Rational agent and behavioral finance in investment","heading":"7.3 EMH tests for the variance for many assets to build a portfolio","text":"next session, cover subject market anomalies. cover momentum anomalies. However, strategy constructing portfolio stocks. However, first, need filter stocks can make prediction. filter variance test efficient markets hypothesis.First read data, convenience, transform data frame xts.estimated returns, Delt.\napply(data, 2, func)Xts : becasue loos xts property:following code test made one stock previous second section chapter:next chunk shows “Loop ” show similar procedure previous chunk. knowledge requires level course’s contents. expect develop process final exam following procedure previous chunk, building R function together “apply” function, creating “Loop .”following code helps us count number tickers use make prediction, applying “ifelse” function combing object containing EMH test.Finally, filter get tickers category predict.Also take historical information filtered stocks","code":"\ndf<-read.csv(\"https://raw.githubusercontent.com/abernal30/BookAFP/main/data/df_dates.csv\")\ndf<-as.data.frame(df)\ndate<-as.Date(df[,1],format=\"%m/%d/%Y\") \ndatax<- xts(df[,-1],\n         order.by = date)\nreturn_all<-apply(datax, 2, Delt)\nreturn_all_xts<-xts(return_all,\n         order.by = as.Date(date))\nhead(return_all_xts[,1:4]) \n#>              AAPL.Close   MSFT.Close    GOOG.Close  GOOGL.Close\n#> 2020-01-02           NA           NA            NA           NA\n#> 2020-01-03 -0.009722044 -0.012451750 -0.0049072022 -0.005231342\n#> 2020-01-06  0.007968248  0.002584819  0.0246570974  0.026654062\n#> 2020-01-07 -0.004703042 -0.009117758 -0.0006240057 -0.001931646\n#> 2020-01-08  0.016086289  0.015928379  0.0078803309  0.007117757\n#> 2020-01-09  0.021240806  0.012492973  0.0110444988  0.010497921\ni<-1\n\n#For the next chunk:\n\n#---------------copy from here------- \ndata<-return_all_xts[, i]\nlag1 <- stats::lag(data,1)\nlag2 <- stats::lag(data,2)\nret <- data.frame(cbind(data, lag1,lag2))\ncolnames(ret)<-c(\"SP500\",\"SP500_lag1\",\"SP500_lag2\")\n\n#-------------to here ------------\n\nmodel <- lm(SP500 ~.,data =ret)\nbp <- bptest(model)\nbp <- bp[[\"p.value\"]]\nbp\n#>         BP \n#> 0.00134487\nbp_all_Loop<-data.frame()\ndim<-dim(return_all_xts)[2] \nfor (i in 1:dim){\n \n#----copy here -----\n\n  data<-return_all_xts[, i]\nlag1 <- stats::lag(data,1)\nlag2 <- stats::lag(data,2)\nret <- data.frame(cbind(data, lag1,lag2))\ncolnames(ret)<-c(\"SP500\",\"SP500_lag1\",\"SP500_lag2\")\n\n# ----to Here----------------\n\nmodel <- lm(SP500 ~.,data =ret)\n  bp <- bptest(model)\n  bp <- bp[[\"p.value\"]]\n  bp_all_Loop[i,1]<-colnames(return_all_xts)[i]\n  bp_all_Loop[i,2]<-bp\n}\ncolnames(bp_all_Loop)<-c(\"Ticker\",\"p-value\")\nhead(bp_all_Loop)\n#>        Ticker     p-value\n#> 1  AAPL.Close 0.001344870\n#> 2  MSFT.Close 0.002359318\n#> 3  GOOG.Close 0.007415325\n#> 4 GOOGL.Close 0.009713821\n#> 5  AMZN.Close 0.542408713\n#> 6  TSLA.Close 0.086492404\ntail(bp_all_Loop)\n#>          Ticker      p-value\n#> 95    TXN.Close 0.0002357208\n#> 96    CRM.Close 0.8711756478\n#> 97    BMY.Close 0.1610277136\n#> 98    UPS.Close 0.9590241181\n#> 99  RLLCF.Close 0.8083656314\n#> 100  QCOM.Close 0.0539758548\npred<-ifelse(bp_all_Loop[,\"p-value\"]<0.1,\"Predict\",\"No Predict\" )\n\nbp_all_Loop_1<- cbind(bp_all_Loop,pred)\n\nhead(bp_all_Loop_1)\n#>        Ticker     p-value       pred\n#> 1  AAPL.Close 0.001344870    Predict\n#> 2  MSFT.Close 0.002359318    Predict\n#> 3  GOOG.Close 0.007415325    Predict\n#> 4 GOOGL.Close 0.009713821    Predict\n#> 5  AMZN.Close 0.542408713 No Predict\n#> 6  TSLA.Close 0.086492404    Predict\ntail(bp_all_Loop_1)\n#>          Ticker      p-value       pred\n#> 95    TXN.Close 0.0002357208    Predict\n#> 96    CRM.Close 0.8711756478 No Predict\n#> 97    BMY.Close 0.1610277136 No Predict\n#> 98    UPS.Close 0.9590241181 No Predict\n#> 99  RLLCF.Close 0.8083656314 No Predict\n#> 100  QCOM.Close 0.0539758548    Predict\nbp_all_Loop_f<- bp_all_Loop_1 %>%\n  dplyr::filter(pred == \"Predict\") \nhead(bp_all_Loop_f)\n#>        Ticker      p-value    pred\n#> 1  AAPL.Close 1.344870e-03 Predict\n#> 2  MSFT.Close 2.359318e-03 Predict\n#> 3  GOOG.Close 7.415325e-03 Predict\n#> 4 GOOGL.Close 9.713821e-03 Predict\n#> 5  TSLA.Close 8.649240e-02 Predict\n#> 6 BRK.A.Close 3.950538e-06 Predict\ntail(bp_all_Loop_f)  \n#>        Ticker      p-value    pred\n#> 71  WFC.Close 6.487655e-07 Predict\n#> 72 C.PJ.Close 3.292293e-07 Predict\n#> 73   PM.Close 2.047688e-07 Predict\n#> 74  LIN.Close 3.799114e-02 Predict\n#> 75  TXN.Close 2.357208e-04 Predict\n#> 76 QCOM.Close 5.397585e-02 Predict\nticker<-bp_all_Loop_f[,\"Ticker\"]\nemh<-return_all_xts[,ticker]\nhead(emh[,1:4])\n#>              AAPL.Close   MSFT.Close    GOOG.Close  GOOGL.Close\n#> 2020-01-02           NA           NA            NA           NA\n#> 2020-01-03 -0.009722044 -0.012451750 -0.0049072022 -0.005231342\n#> 2020-01-06  0.007968248  0.002584819  0.0246570974  0.026654062\n#> 2020-01-07 -0.004703042 -0.009117758 -0.0006240057 -0.001931646\n#> 2020-01-08  0.016086289  0.015928379  0.0078803309  0.007117757\n#> 2020-01-09  0.021240806  0.012492973  0.0110444988  0.010497921\ntail(emh[,1:4])\n#>              AAPL.Close   MSFT.Close    GOOG.Close  GOOGL.Close\n#> 2022-05-19 -0.024641392 -0.003699634 -0.0147285646 -0.013543429\n#> 2022-05-20  0.001747288 -0.002291226 -0.0129350191 -0.013371513\n#> 2022-05-23  0.040119232  0.032031977  0.0215299497  0.023689766\n#> 2022-05-24 -0.019215988 -0.003951656 -0.0514075636 -0.049494164\n#> 2022-05-25  0.001139947  0.011170149 -0.0008165988 -0.001556952\n#> 2022-05-26  0.023199508  0.012875229  0.0232096155  0.018784556"},{"path":"applied-market-anomalies.-momentum-market-anomalies.html","id":"applied-market-anomalies.-momentum-market-anomalies","chapter":"8 Applied market anomalies. Momentum market anomalies","heading":"8 Applied market anomalies. Momentum market anomalies","text":"","code":"\nlibrary(xts)\nlibrary(dplyr)\nlibrary(PerformanceAnalytics)\nlibrary(quantmod)\nlibrary(ggplot2)"},{"path":"applied-market-anomalies.-momentum-market-anomalies.html","id":"overview-of-the-momentum-market-anomalies","chapter":"8 Applied market anomalies. Momentum market anomalies","heading":"8.1 Overview of the momentum market anomalies","text":"chapter 7, applied EMH test variance many assets building portfolio. remember, filter get stocks EMH fails, test suggests inefficient market, can make prediction based past information.final goal create portfolio filtered stocks submit trading platform, interactive brokers. chapter, make another filter applying momentum effect, market anomaly. follow methodology article (Cervantes, Montoya, Bernal 2016).Short run momentum (3 12 months). strategy consists buying assets prices trending selling . idea asset prices previous higher returns (winners), short run, continue high returns, asset prices previous lower returns (losses) continue lower returns. strategy applyed portfolio, consist taking long positions previous higher returns (winners), short positions previous lower returns (losses) ones. applied portfolio, strategy consists taking long positions previous higher returns (winners) short ones lower returns (losses).Long run momentum reversal effect (2 5 years). expect opposite effect long run. reversal effect occurs asset prices previous higher returns (winners) reverse show lower returns. asset prices previous lower returns (losses) higher returns.chapter, apply Short run momentum strategy. look stocks apply trading strategies using historical information (made EMH test). reach end, apply called algorithm trading. case, create signal buy(long position ) sell (long position) based variables created past information. estimate strategy return stock filter stocks consistently show wines losers.","code":""},{"path":"applied-market-anomalies.-momentum-market-anomalies.html","id":"data-preparation-2","chapter":"8 Applied market anomalies. Momentum market anomalies","heading":"8.2 Data preparation","text":"Nov 11, downloaded tickers yahoo finance Screener, like one:\nFigure 8.1: …\nresult, got tickers prices estimated returns. next code, import prices returns.following transform data frame date xts.want apply short momentum strategy 30 days. words, want verify , winners assess, can buy stock, make profit 30 days , expect stock price increasing. example, suppose algorithm generates buy signal, strategy imply buy stock today, sell 30 days .","code":"\nknitr::include_graphics('images/yf.jpg')\nlibrary(xts)\n# The Returns \nret_o<-read.csv(\"https://raw.githubusercontent.com/abernal30/BookAFP/main/data/ret_momentum_orig.csv\")\n\n# I deleted these two columns because they were causing errors in the code because of the missing values.\ndel<-c(17,117)\nret_o<-ret_o[-del]\n\n# We will need also the original prices. \nprice_o<-read.csv(\"https://raw.githubusercontent.com/abernal30/BookAFP/main/data/price_momentum_orig.csv\")\nprice_o<-price_o[-del]\nhead(price_o[,1:3])\n#>         Date SHOP.Close SQ.Close\n#> 1 11/11/2019     30.332    63.79\n#> 2 11/12/2019     30.556    61.54\n#> 3 11/13/2019     31.431    61.51\n#> 4 11/14/2019     31.245    62.99\n#> 5 11/15/2019     31.238    64.70\n#> 6 11/18/2019     32.298    65.74\n#  Instead of doing the procedure twice, we create function, call it my_xts, that transform into xts a data frame that has as first column the date.\n\nmy_xts<-function(data,date_format){\n  date<-data[,1]\n  datax<- xts(data[,-1],\n              order.by = as.Date(date,date_format))\n  datax\n}\n\n#-------- ends here---------\n\n\n#---- Here we apply the function\n\n# You need to look at the date format, Example: \"%Y-%m-%d\" \"%y/%m/%d\",  Y for 4 digits year\n#  (y for 2 digits), m for month and d for day.\n  \n# In the previous chunk we can see that the date format is %m/%d/%Y\n\n# Here we use the function \nprice_xts<-my_xts(price_o,\"%m/%d/%Y\")\nret_xts<-my_xts(ret_o,\"%m/%d/%Y\")\nhead(ret_xts[,1:3])\n#>              SHOP.Close     SQ.Close     SE.Close\n#> 2019-11-12  0.007384907 -0.035271986  0.185806387\n#> 2019-11-13  0.028635947 -0.000487537 -0.042981450\n#> 2019-11-14 -0.005917693  0.024061194  0.049459864\n#> 2019-11-15 -0.000224036  0.027147086 -0.014626138\n#> 2019-11-18  0.033932997  0.016074205  0.006871907\n#> 2019-11-19 -0.020744319  0.009583283  0.010373928"},{"path":"applied-market-anomalies.-momentum-market-anomalies.html","id":"signal-creation-for-one-stock","chapter":"8 Applied market anomalies. Momentum market anomalies","heading":"8.3 Signal creation for one stock","text":"exposition proposes, start applying strategy first stock data set, subsequent section apply code making procedure stocks data set.create signal, follow book (Jeet Vats 2017). idea combining two technical analysis indicators, Moving Average Converge Diverge (MACD) Bollinger Bands (BB) (see Appendix).Firts create MACD plot observations.use ggplot library making graph.Regarding MACD, usually traders may buy security MACD (red) crosses signal line (blue) sell - short - security MACD (red) crosses signal line (blue).Bollinger Bands also technical analysis indicator.BB case, usually traders consider price (blue) continually touches upper Bollinger Band (red), can indicate overbought signal, continually touching lower band (red) indicates oversold signal.now combine indicators create signal following way: price higher “” band, macd higher signal buy signal (1). price lower “dn” band, macd lower signal sell, short sale, signal (-1). wise neutral signal, nothing (0).","code":"\nmacd<-MACD(price_xts[,1] , nFast = 12  , nSlow =26 , nSig = 9, maType=\"SMA\")\ndata<-macd \nmacd_df<-as.data.frame(data[30:100,])\ndate<-as.Date(rownames(macd_df),format = \"%Y-%m-%d\")\nmacd_df<-cbind(date,macd_df)\nggplot(macd_df, aes(x = date)) +\ngeom_line(aes(y = macd, colour =\"macd\", col=\"blue\")) +\ngeom_line(aes(y = signal,colour=\"signal\",col=\"red\"))\nbb<-BBands(price_xts[,1] , n = 20, maType=\"SMA\", sd = 2)\nbb<-cbind(bb,price_xts[,1])\n\nbb_df<-as.data.frame(bb[30:100,])\ndate<-as.Date(rownames(bb_df),format = \"%Y-%m-%d\")\nbb_df<-cbind(date,bb_df)\n\nggplot(bb_df, aes(x = date)) +\ngeom_line(aes(y = dn, colour =\"bands\",col=col2)) +\ngeom_line(aes(y = up, colour =\"bands\",col=col2)) +\ngeom_line(aes(y =SHOP.Close ,colour=\"SHOP.Close\",col=col1))\ni<-1 # This is because we are aplying the signal on the first stock, column\n\ndata<-price_xts[,i]\nsignal <- ifelse(data > bb[,'up'] & macd[,'macd'] >macd[,'signal'],1,ifelse(data[,i]< bb[,'dn'] &macd[,'macd'] <macd[,'signal'],-1,0))\n\nsignal_df<-as.data.frame(signal[30:100,])\ndate<-as.Date(rownames(signal_df),format = \"%Y-%m-%d\")\nsignal_df<-cbind(date,signal_df)\n\nggplot(signal_df, aes(x = date)) +\ngeom_line(aes(y =SHOP.Close, colour=SHOP.Close))"},{"path":"applied-market-anomalies.-momentum-market-anomalies.html","id":"back-testing","chapter":"8 Applied market anomalies. Momentum market anomalies","heading":"8.4 Back testing","text":"back-test momentum strategy, divide data set training testing, example, machine learning. (Jeet Vats 2017), call in_sample out_sample.Remember want verify strategy return 30 days buy (sell) stock. prove , two alternatives:Alternative 1: Wait time, 30 days, verify strategy works ; work, calibrate strategy prove , waiting another 30 days .Alternative 2 (one apply): Assuming still don’t know happened last 30 days. can test strategy compare happens 30 days. work, must calibrate strategy .total, creating 90 days window case. training sample 60 days window, 90 days date last price known 60 days day. test sample also 60 days window, 60 days date last price known.following figure, explain .\nFigure 8.2: …\nRegarding 60 days windows training sample, strategy involves estimating moving averages 26 days; , need window period estimation 30-day window creating signals.Regarding test sample, suppose algorithm generates signal like green orange dots. backtesting implies estimating return 30 days . know 30-day last price, may wait days almost month verify strategy return. Finally, test set 60 days window , finale stage, compare return window return train set window verify consistency make filter.need create signal price_train data set, time column 5.o estimate strategy return, need verify happens 30 days buy(sell) asset (30 days 30 days momentum strategy). need take price day buy (sale) training data set price 30 days test data set.search signal object gets buying (1) sale (-1) signals, stores dates signal, moment_days (30 case), days , return. example, row 44 sell signal.case, day signal 2022-09-02. next code creates data frame store date, signal, date 30 days signal.moment let return column empty, NA.Now estimate return particular signal. estimate annualized geometric return:\\[ geometric\\ return =\\prod_{=1}^{n} (1+HPR)^{scale/n}\\]\\(\\prod_{=1}^{n} (1+HPR)\\) product (1+HPR). Also, n number observations, scale number periods year (daily scale = 252, monthly scale = 12, quarterly scale = 4) HPR Holding Period Return:\\[HPR=  \\frac{Price_{t} - Price_{t-1}}{Price_{t-1}}\\]First, make sample window 2022-09-02 2022-10-02.Now estimate arithmetic return sum sample, apply Return.annualized function.next code procedure signals stock analyzing.","code":"\nknitr::include_graphics('images/grap1.jpg')\n\ndim<-dim(price_xts) #dimension\ndim_ret<-dim(ret_xts)\nmomen_days<- 30 # This is the period  of the momentum strategy\n#days<-momen_days*2\n\n# momen_days in this case a 90 days window\nstart_train<-dim[1]-momen_days*3\n\n# in this case 60 days after strat train\nend_train<-start_train+momen_days*2\n\n# in this case 30 days after strat train\nstart_test<-start_train+momen_days\n\n# the most current price\nend_test<-dim_ret[1]\n\nprice_train<-price_xts[start_train:end_train,] #cinitial date\nprice_test<-price_xts[start_test:end_test,] #cinitial date\n\n\nret_train<-ret_xts[start_train:end_train,] #cinitial date\nret_test<-ret_xts[start_test:end_test,] #cinitial date\ni<-5 # Is column number that we can change\n\nmacd<-MACD(price_train[, i] , nFast =12  , nSlow =26 , nSig = 9, maType=\"SMA\")\n  \nbb<-BBands(price_train[, i], n = 20, maType=\"SMA\", sd = 2)\n\nsignal <- ifelse(price_train[,i]> bb[,'up'] & macd[,'macd'] >macd[,'signal'],1,ifelse(price_train[,i]< bb[,'dn'] &macd[,'macd'] <macd[,'signal'],-1,0))\n\nsignal_df<-as.data.frame(signal)\n\ndate<-as.Date(rownames(signal_df),format = \"%Y-%m-%d\")\nsignal_df<-cbind(date,signal_df)\n\nggplot(signal_df, aes(x = date)) +\ngeom_line(aes(y =NIO.Close, colour=NIO.Close))\nsignal[44,1][1]\n#>            NIO.Close\n#> 2022-09-02        -1\ndf<-data.frame(sig_data=\"2022-09-02\",signal=signal[44,1][1], sig_data_30_after=as.Date(\"2022-09-02\")+30,return=NA,ticker=colnames(signal))\ncolnames(df)[2]<-\"Signal\"\ndf\n#>              sig_data Signal sig_data_30_after return    ticker\n#> 2022-09-02 2022-09-02     -1        2022-10-02     NA NIO.Close\n\nsub_price_test<-window(price_test [, i], start =df[,\"sig_data\"], end = df[,\"sig_data_30_after\"])\nhead(sub_price_test)\n#>            NIO.Close\n#> 2022-09-02     17.73\n#> 2022-09-06     17.11\n#> 2022-09-07     17.48\n#> 2022-09-08     17.68\n#> 2022-09-09     19.16\n#> 2022-09-12     21.75\ntail(sub_price_test)\n#>            NIO.Close\n#> 2022-09-23     17.64\n#> 2022-09-26     17.62\n#> 2022-09-27     17.19\n#> 2022-09-28     17.33\n#> 2022-09-29     15.58\n#> 2022-09-30     15.77\nsub_ret<-Delt(sub_price_test[,1])\nreti<-Return.annualized(sub_ret,geometric = T,scale= 252)\nreti[1]\n#> [1] -0.788549\ndf[,\"return\"] <- reti[1]*df[,\"Signal\"]\ncolnames(df)[4]<-\"return\"\ndf\n#>              sig_data Signal sig_data_30_after   return    ticker\n#> 2022-09-02 2022-09-02     -1        2022-10-02 0.788549 NIO.Close\n# Remember that in an R function, you do not need to change the content of the function; only use it. Except if you have the knowledge to do it, which we do not expect in this course\n\n#-------Do not change from her-----------\n\n# This function has only one argument, a data frame with the signals we created in the previous chunk. It searches on the signal object and gets only for buying (1) or sale (-1) signals, and stores the dates of the signal, and moment_days (30 in this case) days after. \n\nstrat_results<-function(signal,data){\n\n  signal_na<-na.omit(signal)\n  sig_date<-as.Date(c())\n  sig_sig<-c()\n  tick<-colnames(signal)\n  \nfor (i in 1:length(signal_na)){\n    if (signal_na[i,1]==1 | signal_na[i,1]== -1 ) {\n      sig_date<-c(sig_date,index(signal_na[i]))\n      sig_sig<-c(sig_sig,signal_na[i])\n    }\n}\n\nma<-\nle2 <-length(sig_date)\nma<-matrix(0,1,le2)\n\nres_signal0<-data.frame(sig_date=sig_date,sig_sig=sig_sig,momen_days_sig=c(1:le2),strat_ret=ma[1,],ticker=tick)\n\nres_signal0[,\"momen_days_sig\"]<-as.Date(res_signal0[,\"sig_date\"]+momen_days)\n\n#res_signal0<-mr\ndi<-dim(res_signal0)\n\nfor (iz in 1:di[1]) {\n\nsub<-window(data[,i], start =res_signal0[iz,\"sig_date\"], end = res_signal0[iz,\"momen_days_sig\"])\nsub_ret<-Delt(sub)\n\nret_annual<-t(Return.annualized(sub_ret,geometric = T,scale= 252))\n\nres_signal0[iz,\"strat_ret\"]<-ret_annual[,1]*res_signal0[iz,\"sig_sig\"]\n}\nres_signal0\n\n}\n\nmr<-strat_results(signal, price_test)\nmr\n#>     sig_date sig_sig momen_days_sig   strat_ret    ticker\n#> 1 2022-08-23      -1     2022-09-22  0.95452790 NIO.Close\n#> 2 2022-09-02      -1     2022-10-02  0.78848347 NIO.Close\n#> 3 2022-09-06      -1     2022-10-06 -0.02053551 NIO.Close\n#> 4 2022-09-13       1     2022-10-13 -0.58211173 NIO.Close"},{"path":"applied-market-anomalies.-momentum-market-anomalies.html","id":"appendix-1","chapter":"8 Applied market anomalies. Momentum market anomalies","heading":"8.5 Appendix","text":"","code":""},{"path":"applied-market-anomalies.-momentum-market-anomalies.html","id":"the-macd-and-signals-from-investopedia.","chapter":"8 Applied market anomalies. Momentum market anomalies","heading":"8.5.1 The MACD and signals (from investopedia).","text":"Moving Average Convergence Divergence (MACD) trend-following momentum indicator shows relationship two moving averages security’s price. MACD calculated subtracting 26-period Exponential Moving Average (EMA) 12-period EMA.result calculation MACD line. nine-day EMA MACD called “signal line,” plotted MACD line, can signal buy sell. Traders may buy security MACD crosses signal line sell - short - security MACD crosses signal line.exponential moving average (EMA) type moving average (MA) places greater weight significance recent data points. exponential moving average also referred exponentially weighted moving average. exponentially weighted moving average reacts significantly recent price changes simple moving average (SMA), applies equal weight observations period.next example, default, function MACD creates 12 days EMA 26-days EMA.","code":""},{"path":"applied-market-anomalies.-momentum-market-anomalies.html","id":"the-bollinger-bands-from-investopedia","chapter":"8 Applied market anomalies. Momentum market anomalies","heading":"8.5.2 The Bollinger Bands (from investopedia)","text":"technical analysis tool developed John Bollinger generating oversold overbought signals.three lines compose Bollinger Bands: simple moving average (middle band) upper lower band.upper lower bands typically 2 standard deviations +/- 20-day simple moving average (center line), can modified.price continually touches upper Bollinger Band, can indicate overbought signal continually touching lower band indicates oversold signal.","code":""},{"path":"portfolio-management-algorithms.html","id":"portfolio-management-algorithms","chapter":"9 Portfolio management algorithms","heading":"9 Portfolio management algorithms","text":"chapter, take filtered stocks “Rational agents theory behavioral finance theories”. remember, previous chapter applied momentum strategy.file df_merge.xlsx find estimations in_sample out_sample.mention previous chapter, performance -sample data pretty like -sample data, assume parameters rule set good generalization power can used live trading. session, filter stocks similar -sample -sample data. purpose, took difference Sharpe ratios in_sample out_sample. also need define threshold tolerance difference. example, take stocks difference less 20% absolute value.\ndf %>%\nfilter(Sharpe_diff < n & Sharpe_diff > -n)\nn thresholdThe momentum strategy consists buying stocks instrument trending selling . case, order sample in_Sharpe, split sample 3 tranches. first stocks taking long positions 3rd one shorts positions.df %>% arrange(desc(col))next code make split winnersThe next code make split losersFinally, combine tranches 1 3 one single objectWe generate thousands simulations portfolio weights, need generate aleatory numbers weights. long position, weights must positive short position, weight must negative. , first trancheThe function runif create random numbers apply function first tranche, runif(n, 0, 1), n number simulations want. need generate number random weights rend_win object.Regarding set.seed(42), runif generate aleatory numbers. useful take # set.seed(42) get result. everyone gets results, insert # .simplicity, generate one portfolio weights simulation, late generate .\nrunif.short position weights must negative. , 3rd tranche:Finally, combine weights.portfolio standard deviation result covariance multiplied portfolio weights.estimate covariance matrix, tickers tranches 1 3. covariance need returs tickers .filtered stocks, get returns stocks, taking returns estimated last session, :Also, filter get filtered stocks, co_allportfolio covariance\ncov(df,use=“complete.obs”)portfolio_std =cov%% weigths\n%% para multiplicar matricesBut need annualized portfolio_std\ntwe<-t(weigths)\nportfolio_std_1=(twe%%portfolio_std252)^.5The following code annualized returns in_sample data, momentum portfolio.","code":"\ndf_merge<-read.csv(\"https://raw.githubusercontent.com/abernal30/BookAFP/main/data/df_merge.csv\")\ntreh<-0.2\n\n  df_merge2<- df_merge   %>%\n  filter(Sharpe_diff < treh & Sharpe_diff > -treh)\ndf_merge2\n#>          ticker    in_return     in_sd in_Sharpe   out_return     out_sd\n#> 1    AAPL.Close -0.054771548 0.2747365   -0.1994 -0.082373325 0.23299813\n#> 2    AMZN.Close -0.016509915 0.2788159   -0.0592 -0.035770361 0.29049652\n#> 3    TSLA.Close  0.564098218 0.5666686    0.9955  0.532664333 0.45951725\n#> 4     TSM.Close -0.135791198 0.3021663   -0.4494 -0.121677232 0.22938977\n#> 5     BAC.Close  0.027169757 0.3355819    0.0810 -0.011570730 0.20220638\n#> 6   NSRGF.Close -0.223354678 0.1760606   -1.2686 -0.210133138 0.15186225\n#> 7   LVMUY.Close -0.108547037 0.2736889   -0.3966 -0.101687079 0.24380399\n#> 8  BAC.PK.Close  0.017871194 0.1272181    0.1405  0.001189315 0.06644728\n#> 9   RHHBF.Close -0.667692583 0.3264243   -2.0455 -0.660057680 0.33908716\n#> 10     KO.Close -0.038464295 0.2080766   -0.1849 -0.023579715 0.12021278\n#> 11     TM.Close -0.091717493 0.2144692   -0.4276 -0.118928664 0.21121348\n#> 12  RYDAF.Close  0.017644987 0.3660373    0.0482  0.001445751 0.20941036\n#> 13   SHEL.Close -0.010895257 0.3851020   -0.0283 -0.003778604 0.17708269\n#> 14    BHP.Close -0.028648261 0.3246614   -0.0882 -0.029134810 0.25596044\n#> 15     VZ.Close -0.119001044 0.1588481   -0.7491 -0.102977092 0.12531984\n#> 16    AZN.Close -0.207450467 0.2140546   -0.9691 -0.193951418 0.17725594\n#> 17  AZNCF.Close -0.372264173 0.2180492   -1.7072 -0.346987471 0.21398241\n#> 18   ADBE.Close -0.044939517 0.2979994   -0.1508 -0.049712045 0.24744804\n#> 19  NVSEF.Close -0.095764445 0.2272564   -0.4214 -0.034737903 0.11387070\n#> 20    DIS.Close -0.045685926 0.2775004   -0.1646 -0.079449603 0.21852916\n#> 21   ORCL.Close -0.121939597 0.2788517   -0.4373 -0.095171013 0.25138906\n#> 22 WFC.PR.Close -0.002421536 0.1492000   -0.0162  0.002284474 0.05670749\n#> 23   TMUS.Close -0.216877202 0.2408982   -0.9003 -0.195021363 0.18670398\n#>    out_Sharpe Sharpe_diff\n#> 1     -0.3535      0.1541\n#> 2     -0.1231      0.0639\n#> 3      1.1592     -0.1637\n#> 4     -0.5304      0.0810\n#> 5     -0.0572      0.1382\n#> 6     -1.3837      0.1151\n#> 7     -0.4171      0.0205\n#> 8      0.0179      0.1226\n#> 9     -1.9466     -0.0989\n#> 10    -0.1961      0.0112\n#> 11    -0.5631      0.1355\n#> 12     0.0069      0.0413\n#> 13    -0.0213     -0.0070\n#> 14    -0.1138      0.0256\n#> 15    -0.8217      0.0726\n#> 16    -1.0942      0.1251\n#> 17    -1.6216     -0.0856\n#> 18    -0.2009      0.0501\n#> 19    -0.3051     -0.1163\n#> 20    -0.3636      0.1990\n#> 21    -0.3786     -0.0587\n#> 22     0.0403     -0.0565\n#> 23    -1.0445      0.1442\ndf_filtered<- df_merge2 %>% arrange(desc(in_Sharpe))\n# to gt the names of those stocks\nco<-df_filtered[,1]\nle<-length(co)\nn<-round(le/3,0)  \nwin<-co[1:n] # long positions\nn\n#> [1] 8\nloss<-co[(le-n):le]\nloss # short positions\n#> [1] \"TM.Close\"    \"ORCL.Close\"  \"TSM.Close\"   \"VZ.Close\"    \"TMUS.Close\" \n#> [6] \"AZN.Close\"   \"NSRGF.Close\" \"AZNCF.Close\" \"RHHBF.Close\"\nco_all<-c(win,loss)\nco_all\n#>  [1] \"TSLA.Close\"   \"BAC.PK.Close\" \"BAC.Close\"    \"RYDAF.Close\"  \"WFC.PR.Close\"\n#>  [6] \"SHEL.Close\"   \"AMZN.Close\"   \"BHP.Close\"    \"TM.Close\"     \"ORCL.Close\"  \n#> [11] \"TSM.Close\"    \"VZ.Close\"     \"TMUS.Close\"   \"AZN.Close\"    \"NSRGF.Close\" \n#> [16] \"AZNCF.Close\"  \"RHHBF.Close\"\nw<- 1.2 # long position weight\nw_short<- 1-w \nset.seed(42)\n#runif \nru<-runif(n , 0, 1)\n# weigths sum\nsu<-sum(ru)\n# runif/sum and trasnsforming into data frame\nwe_win<-data.frame(ru*w/su)\n#colnanmes weigth\ncolnames(we_win)<-\"we\"\n# row names from win\nrownames(we_win)<-win\nru<-runif(length(loss), 0, 1)\nset.seed(42)\nsu<-sum(ru)\n# runif/sum and trasnsforming into data frame\nwe_loss<-data.frame(ru*w_short/su) \n#colnanmes weigth\ncolnames(we_loss)<-\"we\"\n# row names from loss\nrownames(we_loss)<-loss\nsum(we_loss) # set.seed(42)\n#> [1] -0.2\nwe_loss\n#>                      we\n#> TM.Close    -0.02150707\n#> ORCL.Close  -0.02308076\n#> TSM.Close   -0.01498448\n#> VZ.Close    -0.02354061\n#> TMUS.Close  -0.03059711\n#> AZN.Close   -0.00836163\n#> NSRGF.Close -0.01513346\n#> AZNCF.Close -0.03077199\n#> RHHBF.Close -0.03202288\nwe_all<-rbind(we_win,we_loss)\nwe_all\n#>                       we\n#> TSLA.Close    0.21952864\n#> BAC.PK.Close  0.22487269\n#> BAC.Close     0.06866573\n#> RYDAF.Close   0.19928491\n#> WFC.PR.Close  0.15400152\n#> SHEL.Close    0.12456895\n#> AMZN.Close    0.17676122\n#> BHP.Close     0.03231633\n#> TM.Close     -0.02150707\n#> ORCL.Close   -0.02308076\n#> TSM.Close    -0.01498448\n#> VZ.Close     -0.02354061\n#> TMUS.Close   -0.03059711\n#> AZN.Close    -0.00836163\n#> NSRGF.Close  -0.01513346\n#> AZNCF.Close  -0.03077199\n#> RHHBF.Close  -0.03202288\ndata<-read.csv(\"https://raw.githubusercontent.com/abernal30/BookAFP/main/data/dfx_2.csv\")\ndate<-data[,1]\ndata<-data[,-1]\ndatax<- xts(data,\n         order.by = as.Date(date))\ndatax<-na.omit(datax)\nret<-apply(datax,2,Delt)\nretx<- xts(ret,\n         order.by = as.Date(date))\nretx<-na.omit(retx)\nhead(retx[,1:5])\n#>              AAPL.Close   MSFT.Close    GOOG.Close  GOOGL.Close   AMZN.Close\n#> 2020-01-03 -0.009722044 -0.012451750 -0.0049072022 -0.005231342 -0.012139050\n#> 2020-01-06  0.007968248  0.002584819  0.0246570974  0.026654062  0.014885590\n#> 2020-01-07 -0.004703042 -0.009117758 -0.0006240057 -0.001931646  0.002091556\n#> 2020-01-08  0.016086289  0.015928379  0.0078803309  0.007117757 -0.007808656\n#> 2020-01-09  0.021240806  0.012492973  0.0110444988  0.010497921  0.004799272\n#> 2020-01-10  0.002260711 -0.004627059  0.0069726829  0.006458647 -0.009410597\nretx_all<-retx[,co_all]\ncovar<-cov(retx_all,use=\"complete.obs\")\nportfolio_std =covar %*% we_all[,1]\nportfolio_std\n#>                      [,1]\n#> TSLA.Close   7.294191e-04\n#> BAC.PK.Close 1.071041e-04\n#> BAC.Close    3.251108e-04\n#> RYDAF.Close  4.231321e-04\n#> WFC.PR.Close 1.213717e-04\n#> SHEL.Close   4.451261e-04\n#> AMZN.Close   2.318799e-04\n#> BHP.Close    3.398317e-04\n#> TM.Close     1.541582e-04\n#> ORCL.Close   1.497664e-04\n#> TSM.Close    2.662127e-04\n#> VZ.Close     6.751342e-05\n#> TMUS.Close   2.029555e-04\n#> AZN.Close    1.142982e-04\n#> NSRGF.Close  9.421821e-05\n#> AZNCF.Close  7.306948e-05\n#> RHHBF.Close  8.447179e-05\ntwe<-t(we_all[,1])\nportfolio_std_1=(twe%*%portfolio_std*252)^.5\nrownames(df_merge2)<-df_merge2[,1]\nret_a<-df_merge2[co_all,2]\n\nret_a_f<-twe %*%ret_a \nret_a_f\n#>           [,1]\n#> [1,] 0.1818727"},{"path":"references.html","id":"references","chapter":"10 References","heading":"10 References","text":"","code":""}]
