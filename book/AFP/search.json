[{"path":"index.html","id":"algorithms-and-financial-programming-in-r","chapter":"Algorithms and Financial Programming in R","heading":"Algorithms and Financial Programming in R","text":"book Algorithms Financial Programming R!work Aturo Bernal\nVisit GitHub repository site.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"started writing book guidance undergraduate courses related algorithms financial programming. extend content become book wish start learning program R want expand knowledge programming applications finance. codes book R written RStudio Markdowns worldwide known programming language.book aims get introductory programming competencies, well topics data cleaning, data analysis, machine learning, apply concepts finance.R files stored GitHub repository site.","code":""},{"path":"preface.html","id":"outline","chapter":"Preface","heading":"Outline","text":"","code":""},{"path":"r-basics.html","id":"r-basics","chapter":"1 R Basics","heading":"1 R Basics","text":"section covers topics required following chapters. suggest covering section someone yet gain previous knowledge R programming.","code":""},{"path":"r-basics.html","id":"r-markdown","chapter":"1 R Basics","heading":"1.1 1 R Markdown","text":"book, work R Markdowns, document format embed code chunks (R languages) documents. importantly, allows printing (knitr) authoring languages, including LaTeX, HTML, Text.See markdown content (Yihui Xie Grolemund 2019).","code":""},{"path":"r-basics.html","id":"coding-basics","chapter":"1 R Basics","heading":"1.2 2 Coding basics","text":"entities can create manipulate R called objects. may include variables, arrays numbers, character strings, functions, general structures. create objects applying assignment operator (‘<-’). consists two characters ‘<’ (“less ”) ‘-’ (“minus”) occurring strictly side--side, ‘points’ object receiving value expression (Team 2022).also apply operator ‘=’; however, experience, functions use “=” operator inside, programming language can interpret “=” operator variable creation.example, create object “”; winch assigned value 4.delete object environment. can also use function rm(). However, suggest using R-studio. introduction RStudio, suggest reviewing chapter 1 book (Ismay Kim 2019)","code":"\na<- 4 \nrm(a)"},{"path":"r-basics.html","id":"atomic-structures","chapter":"1 R Basics","heading":"1.3 Atomic structures","text":"objects frequently used finance numeric, character, vectors logical. known “atomic” structures since components identical. rest objects, like matrix Data frames, built atomic objects.type character (strings) objects using either matching double (“) single (’) quotes. example:use function “print” print object write object name.review object class, use function “class”:following example numeric objects.","code":"\nticker<-\"APPL\" \nticker\n#> [1] \"APPL\"\n# or \nprint(ticker)\n#> [1] \"APPL\"\nclass(ticker)\n#> [1] \"character\"\nnum<-4\nprint(num)\n#> [1] 4\n\n# To print the class of the object\nprint(class(num))\n#> [1] \"numeric\""},{"path":"r-basics.html","id":"vectors","chapter":"1 R Basics","heading":"1.4 Vectors","text":"R, vectors consist ordered collection numbers characters. -n programming languages, list. R, list another kind object.finance applications, use vectors store ticker names (character vectors) store stock price (numeric vector). built vectors applying function concatenate “c()”. example:can see, object class numeric vector taking class atomic objects, case, numeric. example character vector:Selecting element vector.select element, use brackets: “[]”. example, select first element vector “v2”:Also, select sub-sample:former example, just select sub-sample, object “v2” hasn’t changed (see environment, didn´t create object). want change object, need create new one.example, want delete element, use minus sign “-”. example 2nd element “v2”:case, object “v2” changed. Also, object “v2” now environment. Vectors mutable; winch means change element vector, example, changing element “Amazon” “Meta”:like add new element, example “Amazon_new”, need apply “c” function:","code":"\nv1<-c(160,165,167,145,145)\n\nprint(v1)\n#> [1] 160 165 167 145 145\nclass(v1)\n#> [1] \"numeric\"\nv2<-c(\"Apple\",\"Meta\",\"Amazon\")\nprint(v2)\n#> [1] \"Apple\"  \"Meta\"   \"Amazon\"\nclass(v2)\n#> [1] \"character\"\nv2[1]\n#> [1] \"Apple\"\nv2[1:2]\n#> [1] \"Apple\" \"Meta\"\nv2<-v2[-2]\nv2\n#> [1] \"Apple\"  \"Amazon\"\nv2[2]<-\"Meta\"\nv2\n#> [1] \"Apple\" \"Meta\"\nv2<-c(v2,\"Amazon_new\")\nv2\n#> [1] \"Apple\"      \"Meta\"       \"Amazon_new\""},{"path":"r-basics.html","id":"data-frames","chapter":"1 R Basics","heading":"1.5 Data frames","text":"Finance common use Data Frames, tabular-form data objects column can different form, , numeric character.example use data frame created library Wooldridge manipulations.Get data frame k401k library WooldridgeRemember library set functions someone created. Wooldridge library many data sets econometrics book author (Wooldridge 2020).import library, apply function library()import databases library, library must imported, just calling data set name, case “k401k”.can see, object class Data Frame.function “colnames” shows names column data frame. case, character vector:Sometimes convenient change column row names data frame; example, change name first column “prate_1”. case, use function “colnames” select, brackets, column number want change. changing column names vector, need establish assignment operator “<-”.show change row name, use “rownames” function. convenience, select first five rows.apply procedure made “colnames” function modify row data frame.Selecting rows columnsThere many ways select column row data frame.Selecting rows columns position, example, selecting first row, column 5. data frame two dimensions, rows columns, selecting also use brackets, separating rows columns aSelecting columns $ symbolMerging two data frames columns.Suppose following Data Frame:Print dimension data frame, applying function paste, print dim:Applying function cbind merge two data frames call object df3Note method duplicates column age.takeoff one columns, select number position adding minus symbolCreate new variable, tot_part_age (totpart/age) variable row names index, call index, data frame. Insert object df3.Eliminate 2nd row object df3 call df4.Apply function cbind merge df3 df4It show debug “Error data.frame(…, check.names = FALSE) :\narguments imply differing number rows: 6, 5”, means number rows .Careful: number rows data frame multiple another, coincidence, “cbind” function merge. However, R going fill missing values repeating values data frame.Try now function merge(x,y,.x=,.y=,=T F, .x=T F, .y=T F)merge function needs pivot reference variable make merge. case, column index identification id (share variable). id must unique value row must present data frames. Also, need specify want keep data data frame x y.","code":"\nlibrary(wooldridge) \nk4<-k401k\nclass(k4)\n#> [1] \"data.frame\"\ncolnames(k4)\n#> [1] \"prate\"   \"mrate\"   \"totpart\" \"totelg\"  \"age\"     \"totemp\"  \"sole\"   \n#> [8] \"ltotemp\"\ncolnames(k4)[1]<-\"prate_1\" \ncolnames(k4)\n#> [1] \"prate_1\" \"mrate\"   \"totpart\" \"totelg\"  \"age\"     \"totemp\"  \"sole\"   \n#> [8] \"ltotemp\"\nrownames(k4)[1:5]\n#> [1] \"1\" \"2\" \"3\" \"4\" \"5\"\nk4[1,5]\n#> [1] 8\nk4$age[1:10]\n#>  [1]  8  6 10  7 28  7 31 13 21 10\ndf1<-k4[1:6,c(\"prate_1\",\"totpart\",\"age\")]\ndf2<-k4[1:6,c(\"age\",\"totemp\")]\n\ndf3<-cbind(df1,df2)\nhead(df3,10) \n#>   prate_1 totpart age age totemp\n#> 1    26.1    1653   8   8   8709\n#> 2   100.0     262   6   6    315\n#> 3    97.6     166  10  10    275\n#> 4   100.0     257   7   7    500\n#> 5    82.5     591  28  28    933\n#> 6   100.0      92   7   7    143\ndim<-dim(df3) \ndim\n#> [1] 6 5\ndf1<-k4[1:6,c(\"prate_1\",\"totpart\",\"age\")]\ndf2<-k4[1:6,c(\"age\",\"totemp\")]\ndf3<-cbind(df1,df2)\ndf3\n#>   prate_1 totpart age age totemp\n#> 1    26.1    1653   8   8   8709\n#> 2   100.0     262   6   6    315\n#> 3    97.6     166  10  10    275\n#> 4   100.0     257   7   7    500\n#> 5    82.5     591  28  28    933\n#> 6   100.0      92   7   7    143\ndf3<-df3[,-3]\ndf3\n#>   prate_1 totpart age totemp\n#> 1    26.1    1653   8   8709\n#> 2   100.0     262   6    315\n#> 3    97.6     166  10    275\n#> 4   100.0     257   7    500\n#> 5    82.5     591  28    933\n#> 6   100.0      92   7    143\ndf3[\" tot_part_age\"]<-(df3[,\"totpart\"]/df3[,\"age\"])\ndf3[\"index\"]<-rownames(df3)\ndf4<-df3[-2,]\ndf5<-cbind(df3,df4)\ndf5<-merge(df3,df4,by.x=\"age\",by.y=\"age\")\ndf5\n#>   age prate_1.x totpart.x totemp.x  tot_part_age.x index.x prate_1.y totpart.y\n#> 1   7     100.0       257      500        36.71429       4     100.0       257\n#> 2   7     100.0       257      500        36.71429       4     100.0        92\n#> 3   7     100.0        92      143        13.14286       6     100.0       257\n#> 4   7     100.0        92      143        13.14286       6     100.0        92\n#> 5   8      26.1      1653     8709       206.62500       1      26.1      1653\n#> 6  10      97.6       166      275        16.60000       3      97.6       166\n#> 7  28      82.5       591      933        21.10714       5      82.5       591\n#>   totemp.y  tot_part_age.y index.y\n#> 1      500        36.71429       4\n#> 2      143        13.14286       6\n#> 3      500        36.71429       4\n#> 4      143        13.14286       6\n#> 5     8709       206.62500       1\n#> 6      275        16.60000       3\n#> 7      933        21.10714       5"},{"path":"r-basics.html","id":"xts-objects","chapter":"1 R Basics","heading":"1.6 xts objects","text":"xts class object provides uniform handling R’s different time-based data classes. Also, APIs, “quantmod”, download data xts format. example, library “xts” write xlsx file data set “sample_matrix.”next section covered read “xlsx” file.default, object class data frame. feature “xts” objects row names date objects. first, replace numerical row names dates inside object.useful functions can use “xts” objects, example, transforming weekly, monthly, quarterly, yearly, etc.Making sub sample:Note two examples, use “apply.monthly” function object like data_df_2, work rownames dates, can´t apply subset function object; generate empty object.","code":"\n#data(sample_matrix)\n#sm<-get(\"sample_matrix\")\n#data_df<-data.frame(sample_matrix)\n#date<-rownames(data_df)\n#data_df<-cbind(date,data_df)\n#write.xlsx(data_df,\"data/data_df.xlsx\")\nlibrary(openxlsx)\ndata_df<-read.xlsx(\"data/data_df.xlsx\")\ndate<-data_df[,1]\nrownames(data_df)<-date\n# Also I eliminate the dates in row one. \ndata_df_2<-data_df[,-1]\nlibrary(xts)\ndata_xts<- as.xts(data_df_2)\ndata_xt_m<-apply.monthly(data_xts,mean)\nsub_set<-subset(data_xts,\n  +index(data_xts)>=\"2007-05-01\" &\n  +index(data_xts)<=\"2007-06-30\")"},{"path":"r-basics.html","id":"reading-and-writing-csv-and-xlsx","chapter":"1 R Basics","heading":"1.7 Reading and writing CSV and xlsx","text":"libraries write open xlsx CSV file. suggest using “openxlsx”.open file use, File must directory need specify directory location; otherwise, error:","code":"\nwrite.xlsx(df5,\"data/df5.xlsx\")\nwrite.csv(df5,\"data/df5.csv\")\nlibrary(openxlsx)\nfdf5_x<-read.xlsx(\"data/df5.xlsx\")\nfdf5_c<-read.csv(\"data/df5.csv\")"},{"path":"clean.html","id":"clean","chapter":"2 Big data and data cleaning with datapro","heading":"2 Big data and data cleaning with datapro","text":"chapter, use library write data processing, “datapro” install run following code chunk:\ninstall run:library(devtools):remotes::install_github(“datanalyticss/datapro”)ordevtools::install_github(“datanalyticss/datapro”)chapter, use file credit_semioriginal.xlsx, historical information lendingclub, https://www.lendingclub.com/ fintech marketplace bank scale. original data set least 2 million observations 150 variables. find credit_semioriginal.xlsx first 1,000 observations 150 variables. Using 2 million rows sample make processor low, challenge try original data set see big data .dataset source:\nhttps://www.kaggle.com/wordsforthewise/lending-clubReview data structure credit data set descriptive statistics, first 10 columns.see numerical columns categorical. categorical mean elements characters.","code":"\nlibrary(datapro)\ndata<-read.csv(\"https://raw.githubusercontent.com/abernal30/BookAFP/main/data/credit_semioriginal.csv\")\nstr(data[,1:10])\n#> 'data.frame':    1000 obs. of  10 variables:\n#>  $ loan_amnt      : int  3600 24700 20000 35000 10400 11950 20000 20000 10000 8000 ...\n#>  $ funded_amnt    : int  3600 24700 20000 35000 10400 11950 20000 20000 10000 8000 ...\n#>  $ funded_amnt_inv: int  3600 24700 20000 35000 10400 11950 20000 20000 10000 8000 ...\n#>  $ term           : chr  \"36 months\" \"36 months\" \"60 months\" \"60 months\" ...\n#>  $ int_rate       : num  14 12 10.8 14.8 22.4 ...\n#>  $ installment    : num  123 820 433 830 290 ...\n#>  $ grade          : chr  \"C\" \"C\" \"B\" \"C\" ...\n#>  $ sub_grade      : chr  \"C4\" \"C1\" \"B4\" \"C5\" ...\n#>  $ emp_title      : chr  \"leadman\" \"Engineer\" \"truck driver\" \"Information Systems Officer\" ...\n#>  $ emp_length     : chr  \"10+ years\" \"10+ years\" \"10+ years\" \"10+ years\" ..."},{"path":"clean.html","id":"categorical-into-numerical-filtering-and-coditionals","chapter":"2 Big data and data cleaning with datapro","heading":"2.1 Categorical into numerical: filtering and coditionals","text":"several reasons transform numerical column variable categorical. detailed explanation suggest review chapter “Handling Text Categorical Attributes” book “Machine learning introductory guide R”. moment functions use chapter work variables categorical.see “loan_status” variable categorical. First review many categories column loan_status :data[,col][!duplicated(data[,“col”])]data name dataframe col column nameAnother possibility applying function categ library dataproThere 5 categories, going transform column verification_status numeric:Create filter, way loan_status contains Fully Paid Charged .data %>%\nfilter(col== “categ1” |col== “categ2”)result, now 873 rows.Besides “loan_status” three several categorical columns, example term, winch 2 categories:method use transform simple, example “36 months” take value one “60 months” value 2. column 3 categories, 3rd categories take value 3 .former example easy 3 categories, however, otherWe use charname function see many categorical variables . print first rows using head function.33 categorical columns. function “tonum” transform categorical column numeric, example transforming column “grade”, following categories:need specify data source column name.Finally, sure want transform data set numerical, function “asnum” reviews detect categorical columns transform numeric, result get data frame. apply function review now winch categorical columns, get ., reason, get error, run following code:","code":"\ncol <- \"loan_status\"\ndata[,col][!duplicated(data[,col])]\n#> [1] \"Fully Paid\"         \"Current\"            \"Charged Off\"       \n#> [4] \"In Grace Period\"    \"Late (31-120 days)\"\ncateg(data,col)\n#> [1] \"Fully Paid\"         \"Current\"            \"Charged Off\"       \n#> [4] \"In Grace Period\"    \"Late (31-120 days)\"#> [1] 5\nlibrary(dplyr)\n#col <- \"loan_status\"\ndata1 <- data %>%\n  filter(data[,\"loan_status\"] == \"Fully Paid\" | data[,\"loan_status\"] == \"Charged Off\")#> [1] 873\ncol <- \"term\"\ncat <- categ(data,col)\ncat\n#> [1] \"36 months\" \"60 months\"\nncat <- c(1:length(cat))\nncat\n#> [1] 1 2\ncat[1]\n#> [1] \"36 months\"\ncol_cat <- ifelse(data1[, col] == cat[1],ncat[1],data1[, col])\nhead(col_cat)\n#> [1] \"1\"         \"1\"         \"60 months\" \"60 months\" \"1\"         \"1\"\ncol_cat <- ifelse(data1[, col] == cat[1],ncat[1],ncat[2])\nhead(col_cat)\n#> [1] 1 1 2 2 1 1\ntail(col_cat)\n#> [1] 1 1 1 1 1 1\ndata1[1,\"mths_since_recent_bc\"]*2\n#> [1] 8\nhead(charname(data1))\n#> [1] \"term\"           \"grade\"          \"sub_grade\"      \"emp_title\"     \n#> [5] \"emp_length\"     \"home_ownership\"\ntail(charname(data1))\n#> [1] \"hardship_loan_status\"      \"disbursement_method\"      \n#> [3] \"debt_settlement_flag\"      \"debt_settlement_flag_date\"\n#> [5] \"settlement_status\"         \"settlement_date\"#> [1] 33\ncol <- \"grade\"\n\ncat <- categ(data,col)\n#cat <- data[,col][!duplicated(data[,col])]\ncat\n#> [1] \"C\" \"B\" \"F\" \"A\" \"E\" \"D\" \"G\"\ncol_cat2 <- tonum(data1,col)\nhead(col_cat2[,1:5])\n#>   loan_amnt funded_amnt funded_amnt_inv      term int_rate\n#> 1      3600        3600            3600 36 months    13.99\n#> 2     24700       24700           24700 36 months    11.99\n#> 3     20000       20000           20000 60 months    10.78\n#> 4     10400       10400           10400 60 months    22.45\n#> 5     11950       11950           11950 36 months    13.44\n#> 6     20000       20000           20000 36 months     9.17\n# Warning: this code may take a few minutes to finish, depending on the processor.\ndata2 <- datapro::asnum(data1)\nhead(charname(data2))#> [1] \"There are no character columns\"\ndata2<-read.csv(\"https://raw.githubusercontent.com/abernal30/BookAFP/main/data/credit_semioriginal_num.csv\")"},{"path":"clean.html","id":"missing-values","chapter":"2 Big data and data cleaning with datapro","heading":"2.2 Missing values","text":"treat missing values, suggest taking one following alternatives combination : ) eliminating columns significant amount missing values; ii) eliminating row missing(s) value(s) () located; iii) replace missing values Na´s statistic.firs alternative, lets first apply function “summaryna” detect columns 50 percent missing values:case 29 columns 50 percent missing values. like eliminate columns apply following:confirm, apply function summarynafor second alternative, eliminating rows missing(s) value(s) () located; applying na.omit function. However, careful, case row data frame least one missing value, cace delete rows data frame, like case:third alternative replacing missing values metric. us function “repnas”, object data3 wich already drop columns 50 percent missing values:","code":"\n\nna_perc <- datapro::summaryna(data2,.5)\nhead(na_perc)\n#>                                Percentage of NAs Column number\n#> mths_since_last_record                 0.8064147            27\n#> mths_since_last_major_derog            0.7090493            50\n#> annual_inc_joint                       0.9919817            53\n#> dti_joint                              0.9919817            54\n#> mths_since_recent_bc_dlq               0.7502864            86\n#> mths_since_recent_revol_delinq         0.6575029            88\ndata3 <- data2[,-na_perc[,2]]\nsummaryna(data3,.5)\n#> [1] \"There are no columns with missing values\"\ndata3_1 <- na.omit(data2)\n\ndata3 <- datapro::repnas(data3,\"median\")"},{"path":"clean.html","id":"zero--and-near-zero-variance-predictors","chapter":"2 Big data and data cleaning with datapro","heading":"2.3 Zero- and Near Zero-Variance Predictors","text":"wil use library (Kuhn 2019) section. Zero- Near Zero-Variance Predictors variables columns single unique value, winch refereed “zero-variance predictor”. Also, variables might unique values occur low frequencies. cases may cause troubles estimating econometric machine learning model.function nearZeroVar shows columns number Zero- Near Zero-Variance Predictors.understand better “nearZeroVar” function , lets estimate metrics settlement_date columns, first apply function “table”, gives frequency per category:851 rows label 1, 1 rows label 2 .“frequency ratio” frequency prevalent value second frequent value. near one well-behaved predictors large highly-unbalanced, “grade” column :estimate “frequency ratio” e apply “.max” function gives position frequency prevalent value:get frequent value:second frequent value beThen, “frequency ratio” :default, threshold 19 (95/5), terms object “nzv” show column “frequency ratio” higher 19.Also, nearZeroVar function shows “percent unique values,” number unique values divided total number rows data frame (times 100). approaches zero granularity data increases.percent unique number categories, case 851 column estimated applying first function “length”:number rows data frame, obtain applying fucntio “dim”:“percent unique values” :object “nzv” shows “frequency ratio” “percent unique values”; however, apply filter get columns “frequency ratio” “percent unique values” higher respective threshold apply “nearZeroVar” time without argument “saveMetrics= TRUE”:object nzv_2 shows position colums tresholds higher, create object excluding columns.","code":"\nlibrary(caret)\nnzv <- nearZeroVar(data3,saveMetrics= TRUE)\nhead(nzv)\n#>                 freqRatio percentUnique zeroVar   nzv\n#> loan_amnt        1.173077    24.7422680   FALSE FALSE\n#> funded_amnt      1.173077    24.7422680   FALSE FALSE\n#> funded_amnt_inv  1.173077    24.7422680   FALSE FALSE\n#> term             3.546875     0.2290951   FALSE FALSE\n#> int_rate         1.129032     3.7800687   FALSE FALSE\n#> installment      1.125000    70.4467354   FALSE FALSE\ntail(nzv)\n#>                           freqRatio percentUnique zeroVar  nzv\n#> hardship_loan_status      289.00000     0.5727377   FALSE TRUE\n#> disbursement_method         0.00000     0.1145475    TRUE TRUE\n#> debt_settlement_flag       38.68182     0.2290951   FALSE TRUE\n#> debt_settlement_flag_date 170.20000     1.4891180   FALSE TRUE\n#> settlement_status          77.36364     0.4581901   FALSE TRUE\n#> settlement_date           283.66667     1.8327606   FALSE TRUE\nt<-table(data3[,col])\nt\n#> \n#>   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16 \n#> 851   1   1   1   2   1   1   2   2   1   1   1   1   3   2   2\nw <- which.max(t)\nw\n#> 1 \n#> 1\nt[w]\n#>   1 \n#> 851\nmax(t[-w])\n#> [1] 3\nt[w]/max(t[-w])\n#>        1 \n#> 283.6667\nlength(table(data3[,col])) \n#> [1] 16\ndim(data3)[1]\n#> [1] 873\n(length(table(data3[,col]))/dim(data3)[1])*100\n#> [1] 1.832761\nnzv_2 <- nearZeroVar(data3)\nnzv_2 \n#>  [1]  14  26  32  33  34  39  44  48  49  50  51  52  74  75  93  94  95 100 105\n#> [20] 106 107 108 109 110 111 112 113 114 115 116 117\ndata4<-data3[,-nzv_2]"},{"path":"clean.html","id":"collinearity","chapter":"2 Big data and data cleaning with datapro","heading":"2.4 Collinearity","text":"Collinearity two variables closely related one another. presence collinearity can pose problems model estimation, regression, difficult separate individual effects collinear variables response (James et al. 2017).function “cor” estimate correlation matrix, function “findCorrelation” shows correlated variables “n” (cutoff argument). case, apply cut-80%.argument “names = T” get column’s names correlated variables. Still, wish cut variables data frame, add argument, getting column numbers:cut variables data4 following way:","code":"#>  [1] \"open_acc\"                   \"num_sats\"                  \n#>  [3] \"total_rev_hi_lim\"           \"total_bc_limit\"            \n#>  [5] \"total_rec_prncp\"            \"acc_open_past_24mths\"      \n#>  [7] \"total_pymnt_inv\"            \"total_pymnt\"               \n#>  [9] \"loan_amnt\"                  \"funded_amnt\"               \n#> [11] \"funded_amnt_inv\"            \"num_tl_op_past_12m\"        \n#> [13] \"sub_grade\"                  \"int_rate\"                  \n#> [15] \"num_rev_accts\"              \"num_bc_sats\"               \n#> [17] \"tot_hi_cred_lim\"            \"total_bal_ex_mort\"         \n#> [19] \"num_actv_rev_tl\"            \"fico_range_low\"            \n#> [21] \"last_fico_range_high\"       \"tot_cur_bal\"               \n#> [23] \"revol_util\"                 \"total_il_high_credit_limit\"\n#> [25] \"bc_util\"                    \"collection_recovery_fee\"\nhc<-findCorrelation(descrCor, cutoff = .8)\nhc\n#>  [1] 25 78 54 85 32 58 31 30  1  2  3 79  8  5 76 72 83 84 71 22 39 42 28 86 61\n#> [26] 35\ndata_corr <- data4[ , -hc]\nhead(data_corr[,1:5])\n#>   term installment grade emp_title emp_length\n#> 1    1      123.03     3       300          4\n#> 2    1      820.28     3       210          4\n#> 3    2      432.66     2       624          4\n#> 4    2      289.91     6       127          6\n#> 5    1      405.18     3       634          7\n#> 6    1      637.58     2       637          4"},{"path":"graphs.html","id":"graphs","chapter":"3 APIS and R graphs","heading":"3 APIS and R graphs","text":"","code":""},{"path":"graphs.html","id":"apis-application-programming-interface","chapter":"3 APIS and R graphs","heading":"3.1 API´s (Application Programming Interface)","text":"","code":""},{"path":"graphs.html","id":"quantmod-api","chapter":"3 APIS and R graphs","heading":"3.1.1 Quantmod API","text":"Quantitative Financial Modelling FrameworkThe [quantmod] package R designed assist quantitative trader developing, testing, deploying statistically based trading models.function getSymbols wrapper load data various sources, local remote. One popular default yahoo fiance,\ngetSymbols(“Symbol”):can see, object class xts.eliminate warnings, add argument warnings = FGetting data specific date:getSymbols(“symbol”, =“YY/m/d”,=“YY/m/d”): YY= 4 digit year, m= 2 digit month, d= 2 digit day.previous way download data store information object environment, default assign name object, previous example “AMZN”.Another way store different name, form example name “data”, following:object class list. get information apply following code.previous method useful tickers like Bitcoin:apparently name object environment “BTC-USD”, want modify name, show debug like: “object ‘BTC’ found”even looks “BTC-USD” name kind brackets:Another alternative, helpful apply loops get information environment :like download intra-day data, get data Alphavantage, applying function getSymbols.get dividends, xts object ticker name, needs R environment, case “AAPL”:","code":"\nlibrary(quantmod)\ngetSymbols(\"AAPL\")\n#> [1] \"AAPL\"\nclass(AAPL)\n#> [1] \"xts\" \"zoo\"\ngetSymbols(\"AMZN\", from=\"2020/04/01\",to=\"2022/04/04\")\n#> [1] \"AMZN\"\napple2 <- new.env()\ngetSymbols(\"AAPL\", env=apple2)\n#> [1] \"AAPL\"\nclass(apple2)\n#> [1] \"environment\"\napple3<-apple2[[\"AAPL\"]]\ngetSymbols(\"BTC-USD\")\n#> [1] \"BTC-USD\"\nbit<-BTC-USD[,4]\n# `BTC-USD`\nbit<-get(\"BTC-USD\")\nhead(bit)\n#>            BTC-USD.Open BTC-USD.High BTC-USD.Low BTC-USD.Close BTC-USD.Volume\n#> 2014-09-17      465.864      468.174     452.422       457.334       21056800\n#> 2014-09-18      456.860      456.860     413.104       424.440       34483200\n#> 2014-09-19      424.103      427.835     384.532       394.796       37919700\n#> 2014-09-20      394.673      423.296     389.883       408.904       36863600\n#> 2014-09-21      408.085      412.426     393.181       398.821       26580100\n#> 2014-09-22      399.100      406.916     397.130       402.152       24127600\n#>            BTC-USD.Adjusted\n#> 2014-09-17          457.334\n#> 2014-09-18          424.440\n#> 2014-09-19          394.796\n#> 2014-09-20          408.904\n#> 2014-09-21          398.821\n#> 2014-09-22          402.152\ngetSymbols(\"AAPL\", src=\"av\", api.key=\"yourKey\", output.size=\"full\",\nperiodicity=\"intraday\",interval=\"5min\")\n#> [1] \"AAPL\"\nap_div<-getDividends(\"AAPL\")"},{"path":"graphs.html","id":"nasdaq-data-link-api","chapter":"3 APIS and R graphs","heading":"3.1.2 Nasdaq Data Link API","text":"need create account (suggest free academic account) Nasdaq Data Link (NDL) (Quandl).class objects (NDL) Data Frames, add argument: type=“xts” get “xts” object.example, “Emerging Markets High Grade Corporate Bond Index Yield”","code":"#>            [,1]\n#> 1998-12-31 8.48\n#> 1999-01-04 8.48\n#> 1999-01-05 8.46\n#> 1999-01-06 8.37\n#> 1999-01-07 8.43\n#> 1999-01-08 8.40\nlibrary(Quandl)\neurex<-Quandl(\"ML/EMHGY\", api_key=\"type your own api_key here\",type=\"xts\")\nhead(eurex)"},{"path":"graphs.html","id":"in-house-api","chapter":"3 APIS and R graphs","heading":"3.1.3 In-house Api","text":"Harvesting web rvest (get tickers).code read characters web pages. case, get tickers yahoo finance.","code":"\nlibrary(xml2)\nlibrary(rvest)\n# the page of of the criptocurrencies\n#yf <- \"https://finance.yahoo.com/cryptocurrencies/\"\n\n# for the IPC components\nyf <- \"https://finance.yahoo.com/quote/%5EMXX/components?p=%5EMXX\"\nhtml <- read_html(yf)\n# To get the node a, wich contains characters \nnode <- html_nodes(html,\"a\")\n\n# To read the text in the node\nnode<-html_text(node, trim=TRUE)\n\n# To get the elements that have USD (the tickers). For the IPC tickers, replace \"USD\" with \".MX\". For other tickers, print the node object and look for patterns or select by rows.\n#tickers<-grep(pattern = \"USD\", x = node, value = TRUE)\n\ntickers<-grep(pattern = \".MX\", x = node, value = TRUE)\n\n# to get only the first 5 tickers\n\ntickers1<-tickers\ntickers1\n#>  [1] \"IENOVA.MX\"     \"GENTERA.MX\"    \"GRUMAB.MX\"     \"BOLSAA.MX\"    \n#>  [5] \"AC.MX\"         \"BBAJIOO.MX\"    \"MEGACPO.MX\"    \"BIMBOA.MX\"    \n#>  [9] \"OMAB.MX\"       \"ALPEKA.MX\"     \"AMXL.MX\"       \"ALSEA.MX\"     \n#> [13] \"FEMSAUBD.MX\"   \"GFNORTEO.MX\"   \"KIMBERA.MX\"    \"ASURB.MX\"     \n#> [17] \"GCARSOA1.MX\"   \"PINFRA.MX\"     \"GAPB.MX\"       \"CUERVO.MX\"    \n#> [21] \"LABB.MX\"       \"GCC.MX\"        \"CEMEXCPO.MX\"   \"GMEXICOB.MX\"  \n#> [25] \"TLEVISACPO.MX\" \"LIVEPOLC1.MX\"  \"MEXCHEM.MX\"    \"KOFL.MX\"      \n#> [29] \"PEOLES.MX\"     \"SITESB1.MX\"\ngetSymbols(tickers1[1:2]) \n#> [1] \"IENOVA.MX\"  \"GENTERA.MX\""},{"path":"graphs.html","id":"basic-r-graphs","chapter":"3 APIS and R graphs","heading":"3.2 Basic R-Graphs","text":"basic function creating graph : plot(x, type = “h”, col = “red”, lwd = 10, “xlab”,“ylab”).example use 100 rows “AAPL” ticker.help tue function “plot.xy” “plot.default” can see arguments.\nPlot HSI close priceTo add anhother time series, example: apple*1.1Note: applying function plot, add another line, must data frame, otherwise may appear plot.adding leggends, function legends work data frames, xts., transform dataframe\n.data.frame(object).add legend, works data frames objects, xts. need transform data frame.","code":"\n\ngetSymbols(\"AAPL\")\n#> [1] \"AAPL\"\napp<-AAPL[1:100,4]\nplot(app,type=\"l\",col=\"green\", main = \"APPL\")\nplot(app,type=\"l\",col=\"green\", main = \"APPL\")\n\nlines(app[,1]*1.1,col=\"red\")\nappdf<-as.data.frame(app)\napp_1df<-as.data.frame(app[,1]*1.1)\n\nplot(appdf[,1],type=\"l\",col=\"green\", main = \"APPL\") \nlines(app_1df[,1],col=\"red\") \nlegend(x= \"topleft\", legend = c(\"app\",\"app*1.1\"),lty = 1,lwd=2,col=c(\"green\",\"red\"))"},{"path":"logit.html","id":"logit","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4 Machine learning with market direction prediction: Logit","text":"Machine Learning (ML) application Artificial Intelligence (AI) provides AI system ability automatically learn environment apply lessons make better decisions. variety algorithms machine learning uses iteratively learn, describe improve data, spot patterns, perform actions patterns (Tatsat, Puri & Lookabaugh, 2021).chapter covers machine learning market direction prediction. particular, forecast market moves either upward downward.logistic regression (Logit) Linear Discriminant Analysis (LDA) models help us fit model using binary behavior () forecast market direction. Logistic regression.","code":""},{"path":"logit.html","id":"data-preparation","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4.1 Data preparation","text":"following commands create variable direction either direction (1) direction (0). words, direction signal o buying signal buying. example, direction variable created short SMA greater long SMA zero otherwise.First make plot.Now create signal.machine learning example, predict BNB price, model :\\[ signal_{t}=\\alpha\\ +\\beta1\\ macd_{t-1}+\\beta2\\ rsi_{t-2} +\\beta3\\ bb +\\ e \\]difference independent variable, case signal.separate sample training testing. training data set used building model process, testing dataset used evaluation purposes.","code":"\nlibrary(\"quantmod\")\nticker<-\"BNB-USD\"\ndata<-getSymbols(ticker,from=\"2021-08-01\",to=\"2022-04-18\",warnings =FALSE,auto.assign=FALSE)\ndata<-data[,4]\ncolnames(data)<-\"bnb\"\nlag2<-12\nlag3<-9\nlag4<-26\navg<-SMA(data[,1],lag2) # var1\navg2<-SMA(data[,1],lag4) # var1\ndata2<-cbind(data,avg,avg2)\ndata2<-na.omit(data2)\n#par(mfrow=c(2,1))\nplo0<-as.data.frame(data2[,1])\nplo<-as.data.frame(data2[,2])\nplo1<-as.data.frame(data2[,3])\n\nplot(plo0[,1],col=\"blue\",type=\"l\")\nlines(plo[,1],col=\"red\",type=\"l\")\nlines(plo1[,1],col=\"green\",type=\"l\")\nlegend(x= \"topleft\", legend = c(\"actual\",\"short-sma\",\"long-sma\"),lty = 1,lwd=2,col=c(\"blue\",\"red\",\"green\"))\nsignal<-ifelse(data2[,\"SMA\"]>data2[,\"SMA.1\"],1,0)\nplot(signal)\nstd<- rollapply(data[,1],lag2,sd) # var2\ncolnames(std)<-\"std\"\n\nmacd<- MACD(data[,1], lag2,lag3,lag4, \"SMA\") # var2\nmacd2<- MACD(data[,1], 11,25,8, \"SMA\") \ncolnames(macd)[2]<-\"macd_signal\"\n\n\nrsi<-  RSI(data[,1],lag2,\"SMA\")# var3\nrsi2<-  RSI(data[,1],13,\"SMA\")# var3\n\nbb <- BBands(data2[,1], n = 10, maType=\"SMA\", sd = 2) \n\n# Agregar el nombre de signal en lugar de sig\ndata2<-cbind(signal,std,macd,rsi,bb)\ncolnames(data2)[1]<-\"signal\"\ndata2<-na.omit(data2)\nN<-dim(data2)[1]\nn_train<-round(N*.8,0)\npart<-index(data2)[n_train]\n#This is the test data set.\ntrain<-subset(data2,\n  +index(data2)>=index(data2)[1] &\n  +index(data2)<=part)\n\n# The subset of the training data set.\ntest<-subset(data2,\n  +index(data2)>=part+1 &\n  +index(data2)<=\"2022-04-18\")\ny1<-test[,1] "},{"path":"logit.html","id":"logistic-regression","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4.2 Logistic Regression","text":"linear regression assumes response variable Y quantitative. many situations, response variable instead qualitative. example, eye color qualitative, taking values blue, brown, green. Often qualitative variables referred categorical ; use terms interchangeably.binary response model, interest lies primarily response probability. However, can use OLS estimate model, linear binary response model. apply Logistic regression.glm(y ~.,data= ,family=binomial())expect forecast 0,1 result, signal. transform probabilistic model.\nexp(x)/(1+exp(x))Even result probability, 0 1, require result 0,1. transform , creating binary variable, takes value 1 probability higher 0.5, zero lower 0.5.","code":"\nmodel<-glm(signal~.,data= train,family=binomial())\npred<-predict(model,test)\npred\n#> 2022-03-05 2022-03-06 2022-03-07 2022-03-08 2022-03-09 2022-03-10 2022-03-11 \n#>  1.4071871  0.7838451  1.1688270  1.5835724  0.6400374  1.1018749  1.5094064 \n#> 2022-03-12 2022-03-13 2022-03-14 2022-03-15 2022-03-16 2022-03-17 2022-03-18 \n#> -1.2026024 -2.0134675 -2.4429877 -1.9810255 -0.7260440 -1.5145946 -0.3496281 \n#> 2022-03-19 2022-03-20 2022-03-21 2022-03-22 2022-03-23 2022-03-24 2022-03-25 \n#> -0.7131193 -0.6484040 -1.3564305  1.4355937  1.6560035  2.0060744  3.8742966 \n#> 2022-03-26 2022-03-27 2022-03-28 2022-03-29 2022-03-30 2022-03-31 2022-04-01 \n#>  3.5987035  3.9839923  4.1817522  4.1877168  3.9090570  2.9184529  3.7917209 \n#> 2022-04-02 2022-04-03 2022-04-04 2022-04-05 2022-04-06 2022-04-07 2022-04-08 \n#>  3.4630973  2.7885380  2.6969620  2.6106934  4.1880525  3.2083981  3.0614675 \n#> 2022-04-09 2022-04-10 2022-04-11 2022-04-12 2022-04-13 2022-04-14 2022-04-15 \n#>  2.6747753  2.6183627  1.9073456  2.0025636  0.6854552  1.0431508 -0.2146935 \n#> 2022-04-16 2022-04-17 2022-04-18 \n#> -0.1493334  0.3593882  0.6847538\nprob<-exp(pred)/(1+exp(pred))\npredf<-ifelse(prob>.5,1,0)\nplot(predf)\n# comparar vs el dato real que estaé en y1"},{"path":"logit.html","id":"confusion-matrix","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4.3 Confusion matrix","text":"measure accuracy prediction, categorical variables, 0,1, confusion matrix table indicates possible categories predicted values, actual values.True Positive (TP): Correctly classified class interest. True Negative (TN) Correctly classified class interest. False Positive (FP) Incorrectly classified class interest. False Negative (FN): Incorrectly classified class interest.confusion matrix, one mesures interest accuracy, defined :\\[ accuracy =\\frac{TP+TN}{TP+TN+FP+FN}\\]formula, terms TP, TN, FP, FN refer number times model’s predictions fell categories. accuracy therefore proportion represents number true positives true negatives, divided total number predictions.factor(x,levels=c(1,0))\nconfusionMatrix(pred,real)SensitivityFinding useful classifier often involves balance predictions overly conservative overly aggressive. example, e-mail filter guarantee eliminate every spam message aggressively eliminating nearly every ham message time. hand, guaranteeing ham message inadvertently filtered might require us allow unacceptable amount spam pass filter. pair performance measures captures trade : sensitivity specificity.sensitivity model (also called true positive rate) measures proportion positive examples correctly classified. Therefore, shown following formula, calculated number true positives divided total number positives, correctly classified (true positives) well incorrectly classified (false negatives):\\[sensitivity =\\frac{TP}{TP+FN}\\]","code":"\nlibrary(caret)\npredf2<-as.data.frame(predf)\npredf3<-factor(predf2[,1],levels=c(1,0))   \nreal<-factor(y1,levels=c(1,0))\nconfusionMatrix(predf3,real) \n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction  1  0\n#>          1 25  8\n#>          0  0 12\n#>                                         \n#>                Accuracy : 0.8222        \n#>                  95% CI : (0.6795, 0.92)\n#>     No Information Rate : 0.5556        \n#>     P-Value [Acc > NIR] : 0.0001572     \n#>                                         \n#>                   Kappa : 0.625         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.0133283     \n#>                                         \n#>             Sensitivity : 1.0000        \n#>             Specificity : 0.6000        \n#>          Pos Pred Value : 0.7576        \n#>          Neg Pred Value : 1.0000        \n#>              Prevalence : 0.5556        \n#>          Detection Rate : 0.5556        \n#>    Detection Prevalence : 0.7333        \n#>       Balanced Accuracy : 0.8000        \n#>                                         \n#>        'Positive' Class : 1             \n#> "},{"path":"logit.html","id":"linear-discriminant-analysis-lda","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4.4 Linear Discriminant Analysis LDA","text":"need another method, logistic regression?\nseveral reasons:• classes well-separated, parameter estimates \nlogistic regression model surprisingly unstable. Linear discriminant\nanalysis suffer problem.• number observations n small distribution predictors X approximately normal classes, linear discriminant model stable logistic regression model.• Finally, 2 categories, example, c(-1,0,1).Signal creation, now 3 categories c(-1,0,1)1 es señal de compra, -1 de venta (o venta en corto), y cero es hacer nada (ni comprar ni vender).Combinig data2 signalTraining test partitionLDA model prediction\nlda(x~.,data= , prior = c(1,1,1)/3)","code":"\nlibrary(\"quantmod\")\nticker<-\"BNB-USD\"\ndata<-getSymbols(ticker,from=\"2018-08-01\",to=\"2022-04-18\",warnings =FALSE,auto.assign=FALSE)\ndata<-data[,4]\ncolnames(data)<-\"bnb\"\nlag2<-12\nlag3<-18\nlag4<-26\navg<-SMA(data[,1],lag2) # var1\navg2<-SMA(data[,1],lag3) # var1\navg3<-SMA(data[,1],lag4) # var1\ndata2<-cbind(data,avg,avg2,avg3)\ndata2<-na.omit(data2)\n\nstd<- rollapply(data[,1],lag2,sd) # var2\ncolnames(std)<-\"std\"\nmacd<- MACD(data[,1], lag2,lag3,lag4, \"SMA\") # var2\ncolnames(macd)[2]<-\"macd_signal\"\nrsi<-  RSI(data[,1],lag2,\"SMA\")# var3\nbb <- BBands(data2[,1], n = 10, maType=\"SMA\", sd = 2) \n\ndata2<-cbind(data,std,macd,rsi,bb)\n#colnames(data2)[1]<-\"signal\"\ndata2<-na.omit(data2)\nsignal <- ifelse(data2[,1]> data2[,'up'] & data2[,'macd']> data2[,'macd_signal'],1,ifelse(data2[,1]< data2[,'dn'] & data2[,'macd'] <data2[,'macd_signal'],-1,0))\nplot(signal)\n#We first replace bnp by signal \ndata2<-data2[,-1]\n\n# Eliminate up, because is causing issues (correlated with mavg, and does not allow estimate the model)\ndata2<-data2[,-6]\ndata2<-cbind(signal,data2)\ncolnames(data2)[1]<-\"signal\"\nN<-dim(data2)[1]\nn_train<-round(N*.8,0)\npart<-index(data2)[n_train]\n#This is the test data set.\ntrain<-subset(data2,\n  +index(data2)>=index(data2)[1] &\n  +index(data2)<=part)\n\n# The subset of the training data set.\ntest<-subset(data2,\n  +index(data2)>=part+1 &\n  +index(data2)<=\"2022-04-18\")\n\ntrain<-train[,-6]\ntest<-test[,-6]\n\ny1<-test[,1] # contiene la varaible que voy a pronosticar\n#test<-test[,-1] # las variables independientes, que voy a usar para haver mi pronóstico\nlibrary(MASS)\nmodellda<-lda(signal~.,data= train, prior = c(1,1,1)/3)\npred<-predict(modellda,test)\npred<-pred[[\"class\"]]\nclass(pred)\n#> [1] \"factor\"\n\nlibrary(caret) \nreal<-factor(y1,levels=c(-1,0,1))\nconfusionMatrix(pred,real) \n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction  -1   0   1\n#>         -1   8  18   0\n#>         0    0 199   0\n#>         1    0  27  11\n#> \n#> Overall Statistics\n#>                                           \n#>                Accuracy : 0.8289          \n#>                  95% CI : (0.7778, 0.8724)\n#>     No Information Rate : 0.9278          \n#>     P-Value [Acc > NIR] : 1               \n#>                                           \n#>                   Kappa : 0.4079          \n#>                                           \n#>  Mcnemar's Test P-Value : NA              \n#> \n#> Statistics by Class:\n#> \n#>                      Class: -1 Class: 0 Class: 1\n#> Sensitivity            1.00000   0.8156  1.00000\n#> Specificity            0.92941   1.0000  0.89286\n#> Pos Pred Value         0.30769   1.0000  0.28947\n#> Neg Pred Value         1.00000   0.2969  1.00000\n#> Prevalence             0.03042   0.9278  0.04183\n#> Detection Rate         0.03042   0.7567  0.04183\n#> Detection Prevalence   0.09886   0.7567  0.14449\n#> Balanced Accuracy      0.96471   0.9078  0.94643"},{"path":"logit.html","id":"bibliography","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4.5 Bibliography","text":"Tatsat, H; Puri, S; Lookabaugh, B. (2021).Machine Learning Data Science Blueprints Finance. Sebastopol, CA. : O’Reilly Media.","code":""},{"path":"big-data-and-machine-learning.html","id":"big-data-and-machine-learning","chapter":"5 Big data and machine learning","heading":"5 Big data and machine learning","text":"Predicting market direction price quite challenging task market data involves lots noise. market moves either upward downward, nature market movement binary (Jeet Vat, 2017).chapter, use OLS, predict 1 day open price, price 19 April 2022, cryptocurency, case Bianance, “BNB-USD”. ideas code adapted Jeet Vat (2017).Binance Launched July 2017, Binance biggest cryptocurrency exchange globally based daily trading volume. Binance aims bring cryptocurrency exchanges forefront financial activity globally. idea behind Binance’s name show new paradigm global finance — Binary Finance, Binance.","code":""},{"path":"big-data-and-machine-learning.html","id":"data-preparation-1","chapter":"5 Big data and machine learning","heading":"5.1 Data preparation","text":"going start OLS (Ordinary least square) model. independent variables using lags independent variable.simplicity, suppose predict 19 April 2022 BNB price, make regression, OLS, following model:\\[bnb_{t}=\\alpha\\ +\\beta1\\ bnb_{t-1}+\\beta2\\ bnb_{t-2} + e \\]\\(\\alpha\\) intercept, \\(beta\\) parameters estimated, \\(bnb_{t-1}\\) bnb price traiding previouse day, case 18 April 2022, \\(bnb_{t-2}\\) bnb price day , 17 April 2022, “e” error term regression. words, price today explained price yesterday day yesterday.data<-stats::lag(y,lag)Para realizar la regresión por OLS\nlm(bnb~.,data=data)Also, suppose found following result regression:\\[bnb_{t}=\\ 52.42583\\ +\\ 0.88235\\ bnb_{t-1}\\ -0.01226\\ bnb_{t-2} \\]case, forecast 19 April 2022 :Given last price 406.3009.However, last example exposition purposes. reality, need test independent variables besides lags dependent variable. session, besides lags values close prices dependent variable, going add variables used technical analysis, moving average, standard deviation, RSI (see appendix detailed explanation), MACD, , predictive power market direction. indicators can constructed using following commands:","code":"\nlibrary(\"quantmod\")\nticker<-\"LALAB.MX\"\ny<-getSymbols(ticker,from=\"2021-01-01\",to=\"2022-04-18\",warnings =FALSE,auto.assign=FALSE)\n\ny<-y[,1]\ncolnames(y)<-\"bnb\"\nhead(y)\n#>              bnb\n#> 2021-01-04 15.54\n#> 2021-01-05 15.23\n#> 2021-01-06 15.75\n#> 2021-01-07 15.81\n#> 2021-01-08 15.90\n#> 2021-01-11 15.64\nlag<-1\nlag1<-2\ndata<-stats::lag(y,lag)\ndata2<-stats::lag(y,lag1)\ndata<-cbind(y,data,data2)\ncolnames(data)[2:3]<-c(\"bnb_1\",\"bnb_2\")\nmodel<-lm(bnb~.,data=data)\nsummary(model)\n#> \n#> Call:\n#> lm(formula = bnb ~ ., data = data)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.25123 -0.13298  0.00724  0.08428  1.92931 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.40151    0.20665   1.943   0.0529 .  \n#> bnb_1        0.88236    0.05571  15.837   <2e-16 ***\n#> bnb_2        0.09303    0.05572   1.670   0.0960 .  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.2664 on 319 degrees of freedom\n#>   (2 observations deleted due to missingness)\n#> Multiple R-squared:  0.9486, Adjusted R-squared:  0.9482 \n#> F-statistic:  2941 on 2 and 319 DF,  p-value: < 2.2e-16\ntail(y)\n#>              bnb\n#> 2022-04-06 17.25\n#> 2022-04-07 17.03\n#> 2022-04-08 17.20\n#> 2022-04-11 17.00\n#> 2022-04-12 16.80\n#> 2022-04-13 16.50\nbnb_t1<-406.3009 # price 18 April 2022\nbnb_t2<-417.4115 # price 17 April 2022\n\n# The prediction manually would be:\n47.13630+0.90113*bnb_t1-0.01765*bnb_t1\n#> [1] 406.095"},{"path":"big-data-and-machine-learning.html","id":"variable-creation","chapter":"5 Big data and machine learning","heading":"5.2 Variable creation","text":"SMA Calculate moving averages\nSMA(x, n = 10, …), x time serie, n Number periods average overSMA Calculate moving averages\nSMA(x, n = 10, …), x time serie, n Number periods average overThe rollapply function applying function rolling margins array, case used make moving standard deviation.\nrollapply(x,n,sd), sd standard deviationThe rollapply function applying function rolling margins array, case used make moving standard deviation.\nrollapply(x,n,sd), sd standard deviationThe MACD moving average converge diverge (see Appendix)MACD moving average converge diverge (see Appendix)MACD(x, nFast = 12, nSlow = 26, nSig = 9, maType=SMA EMA)MACD(x, nFast = 12, nSlow = 26, nSig = 9, maType=SMA EMA)RSI relative strength index\nRSI(x, n = 14, maType=SMA EMA)RSI relative strength index\nRSI(x, n = 14, maType=SMA EMA)model :\n\\[bnb_{t}=\\alpha\\ +\\beta1\\ bnb_{t-1}+\\beta2\\ bnb_{t-2} +\\beta3\\ sma +\\\\ \\beta4\\ std\\ +\\beta5\\ macd\\ + \\beta6\\ rsi +\\ e\\]see, lags new variables, many missing values early dates, apply na.omit, eliminate rows nas.","code":"\nlag2<-6\nlag3<-9\nlag4<-26\navg<-SMA(data[,1],lag2) # var1\nstd<- rollapply(data[,1],lag2,sd) # var2\ncolnames(std)<-\"std\"\n\nmacd<- MACD(data[,1], lag2,lag3,lag4, \"SMA\") # var2\ncolnames(macd)[2]<-\"macd_signal\"\n  \nrsi<-  RSI(data[,1],lag2,\"SMA\")# var3\n\n\ndata2<-cbind(data,avg,std,macd,rsi)\nhead(data2)\n#>              bnb bnb_1 bnb_2    SMA       std macd macd_signal rsi\n#> 2021-01-04 15.54    NA    NA     NA        NA   NA          NA  NA\n#> 2021-01-05 15.23 15.54    NA     NA        NA   NA          NA  NA\n#> 2021-01-06 15.75 15.23 15.54     NA        NA   NA          NA  NA\n#> 2021-01-07 15.81 15.75 15.23     NA        NA   NA          NA  NA\n#> 2021-01-08 15.90 15.81 15.75     NA        NA   NA          NA  NA\n#> 2021-01-11 15.64 15.90 15.81 15.645 0.2393951   NA          NA  NA\ndata2<-na.omit(data2)"},{"path":"big-data-and-machine-learning.html","id":"sub-samples","chapter":"5 Big data and machine learning","heading":"5.3 Sub samples","text":"separate sample training testing. training data set used building model process, testing dataset used evaluation purposes.code automate sub-sample creation, usually split 80% training set 20% test set.case, “2022-01-31” date represents 80% observations, starting date “2022-01-01”.use function subset:name<-subset(object,\n+index(object)>=“YY-mm-dd” &\n+index(object)<=“YY-mm-dd”)forecast vs real data, going takeout real data BNB prices test set, store object call y1.","code":"\nN<-dim(data2)[1]\nn_train<-round(N*.8,0)\npart<-index(data2)[n_train]\npart\n#> [1] \"2022-01-19\"\n\n#This is the test data set.\ntrain<-subset(data2,\n  +index(data2)>=index(data2)[1] &\n  +index(data2)<=part)\n\n# The subset of the training data set.\ntest<-subset(data2,\n  +index(data2)>=part+1 &\n  +index(data2)<=\"2022-04-18\")\ny1<-test[,1] \nhead(test)\n#>              bnb bnb_1 bnb_2      SMA       std      macd macd_signal       rsi\n#> 2022-01-20 16.79 16.80 16.91 17.19167 0.4656785 -1.077298  -0.1888210  3.816726\n#> 2022-01-21 17.36 16.79 16.80 17.12667 0.3938356 -1.109900  -0.2228045 38.036818\n#> 2022-01-24 16.03 17.36 16.79 16.83167 0.4478131 -1.976833  -0.2798782 19.587642\n#> 2022-01-25 16.17 16.03 17.36 16.67667 0.4948401 -1.715669  -0.3048719 30.212749\n#> 2022-01-26 16.03 16.17 16.03 16.53000 0.5401851 -1.470294  -0.3000546 30.869562\n#> 2022-01-27 16.20 16.03 16.17 16.43000 0.5357239 -1.017469  -0.2740690 37.288156"},{"path":"big-data-and-machine-learning.html","id":"making-the-model","chapter":"5 Big data and machine learning","heading":"5.4 Making the model","text":"estimate OLS model aplying function lm,lm(bnb~.,data=train)make prediction, need apply function predict, test set.first prediction, 2022-04-04, 433.6627, \\[ bnb_{t}=\\ -20.28047\\ +\\ 0.06146\\ bnb_{t-1}\\ -0.27979\\ bnb_{t-2} + \\\\1.15805\\ sma +\\ 0.33934206\\ std -3.76813\\ macd\\\\ -6.68031\\ macd\\_ signal\\ + 0.74661\\ rsi \\]\n.","code":"\nmodel1<-lm(bnb~.,data=train)\nsummary(model1)\n#> \n#> Call:\n#> lm(formula = bnb ~ ., data = train)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.69108 -0.09940  0.02329  0.10403  1.10734 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.0910104  0.1643070  -0.554   0.5802    \n#> bnb_1        0.3459223  0.0643449   5.376 1.90e-07 ***\n#> bnb_2       -0.2987632  0.0692119  -4.317 2.37e-05 ***\n#> SMA          0.9308127  0.0896208  10.386  < 2e-16 ***\n#> std          0.4042500  0.0749164   5.396 1.72e-07 ***\n#> macd        -0.0430484  0.0225301  -1.911   0.0573 .  \n#> macd_signal  0.0607983  0.0542453   1.121   0.2636    \n#> rsi          0.0075120  0.0007548   9.952  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.1813 on 225 degrees of freedom\n#> Multiple R-squared:  0.9802, Adjusted R-squared:  0.9796 \n#> F-statistic:  1594 on 7 and 225 DF,  p-value: < 2.2e-16\npred<-predict(model1,test)\nhead(pred)\n#> 2022-01-20 2022-01-21 2022-01-24 2022-01-25 2022-01-26 2022-01-27 \n#>   16.92244   17.11869   16.96135   16.27277   16.59502   16.44019\npred\n#> 2022-01-20 2022-01-21 2022-01-24 2022-01-25 2022-01-26 2022-01-27 2022-01-28 \n#>   16.92244   17.11869   16.96135   16.27277   16.59502   16.44019   16.35616 \n#> 2022-01-31 2022-02-01 2022-02-02 2022-02-03 2022-02-04 2022-02-08 2022-02-09 \n#>   15.66042   15.86946   15.64015   15.54924   15.46544   15.57716   15.43428 \n#> 2022-02-10 2022-02-11 2022-02-14 2022-02-15 2022-02-16 2022-02-17 2022-02-18 \n#>   15.59519   15.64140   15.68066   15.65542   15.82226   16.17687   16.51849 \n#> 2022-02-21 2022-02-22 2022-02-23 2022-02-24 2022-02-25 2022-02-28 2022-03-01 \n#>   16.47065   16.31094   16.30532   16.60728   16.31808   16.30070   16.22510 \n#> 2022-03-02 2022-03-03 2022-03-04 2022-03-07 2022-03-08 2022-03-09 2022-03-10 \n#>   16.41384   15.92332   15.85047   15.72339   15.66347   15.35515   15.42859 \n#> 2022-03-11 2022-03-14 2022-03-15 2022-03-16 2022-03-17 2022-03-18 2022-03-22 \n#>   15.37003   15.35250   15.19145   15.87888   16.21718   16.00074   16.10055 \n#> 2022-03-23 2022-03-24 2022-03-25 2022-03-28 2022-03-29 2022-03-30 2022-03-31 \n#>   16.37870   16.38080   15.99185   15.87494   16.45360   16.65301   16.40199 \n#> 2022-04-01 2022-04-04 2022-04-05 2022-04-06 2022-04-07 2022-04-08 2022-04-11 \n#>   16.64081   16.76660   16.68875   16.92925   17.12510   17.01967   17.08268 \n#> 2022-04-12 2022-04-13 \n#>   16.98242   16.84476"},{"path":"big-data-and-machine-learning.html","id":"accuracy-of-the-prediction","chapter":"5 Big data and machine learning","heading":"5.5 Accuracy of the prediction","text":"Lest make plot forecast vs real value BNB.Finaly, measure accuracy prediction, apply Root Mean Square Error (RMSE). gives idea much error system typically makes predictions. formula RMSE :\\[RMSE =\\frac{1}{n}\\ \\sum_{=1}^{n} (y_{}-\\hat{f(x_{}))^{2}} \\]\n$ $ prediction ith observation (actual), $ y_{} $ observation ith independent variable, n number observations.\\[\\hat{f(x_{})}=\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n}\\]RMSE computed using training data used fit model, accurately referred training RMSE., RMSE close zero, better.\nsqrt(mean((real-forecast)^2,na.rm = T ))","code":"\npred2<-as.data.frame(pred)\ny2<-as.data.frame(y1)\nall<-cbind(y2,pred2)\nplot(all[,1],type = \"l\",col=\"blue\",ylab=\"x\")\nlines(all[,2],col=\"green\")\nlegend(x= \"topleft\", legend = c(\"real\",\"prediction\"),lty = 1,lwd=2,col=c(\"blue\",\"green\"))\nsqrt(mean((all[,1]-all[,2])^2,na.rm = T ))\n#> [1] 0.242318"},{"path":"big-data-and-machine-learning.html","id":"appendix","chapter":"5 Big data and machine learning","heading":"5.6 Appendix","text":"MACD signals (investopedia).Moving Average Convergence Divergence (MACD) trend-following momentum indicator shows relationship two moving averages security’s price. MACD calculated subtracting 26-period Exponential Moving Average (EMA) 12-period EMA.result calculation MACD line. nine-day EMA MACD called “signal line,” plotted MACD line, can signal buy sell. Traders may buy security MACD crosses signal line sell - short - security MACD crosses signal line.exponential moving average (EMA) type moving average (MA) places greater weight significance recent data points. exponential moving average also referred exponentially weighted moving average. exponentially weighted moving average reacts significantly recent price changes simple moving average (SMA), applies equal weight observations period.next example, default, function MACD creates 12 days EMA 26-days EMA.relative strength index (RSI)momentum indicator measures magnitude recent price changes evaluate overbought oversold conditions price stock asset. RSI displayed oscillator (line graph moves two extremes) can reading 0 100. indicator originally developed J. Welles Wilder Jr. introduced seminal 1978 book, New Concepts Technical Trading Systems.Relative Strength Index (RSI) calculates ratio recent upward price movements absolute price movement. Developed J. Welles Wilder. RSI calculation RSI = 100 - 100 / ( 1 + RS ), RS smoothed ratio ‘average’ gains ‘average’ losses. ‘averages’ aren’t true averages, since ’re divided value n number periods gains/losses.Traditional interpretation usage RSI values 70 indicate security becoming overbought overvalued may primed trend reversal corrective pullback (drop stock) price. RSI reading 30 indicates oversold undervalued condition (Investopedia).","code":""},{"path":"big-data-and-machine-learning.html","id":"bibliography-1","chapter":"5 Big data and machine learning","heading":"5.7 Bibliography","text":"Jeet, P Vat, P. (2017). Learning Quantitative finance R (2017), Packt Publishing, Birmingham, UK.","code":""},{"path":"credit-analysis.html","id":"credit-analysis","chapter":"6 Credit analysis","heading":"6 Credit analysis","text":"chapter use following libraries:chapter, cover credit allocation analysis (loan origination). database credit.xlsx historical information\nLendingclub, https://www.lendingclub.com/ fintech marketplace bank\nscale. spreadsheets, find variable description.\noriginal data set least 2 million observations 150\nvariables. Inside file “credit.xlsx,” find 873\nobservations (rows) 71 columns. row represents \nLendingclub client. previously made data cleaning (missing\nvalues, correlated variables, Zero- Near Zero-Variance Predictors).\nknow data cleaning, see 2nd chapter book.next output, see variables Lendingclub’s customers \ngranted loan. example, variable term term, \nyears, loan, “annual_inc,” customer’s annual income\ngot loan.variable “Default”, winch originally name “loan_status”, \ntwo labels:“Charge ” means credit grantor wrote account \nreceivables loss closed future charges. \naccount displays status “charge ,” closed future use,\nalthough customer still owns debt. example, \nconsider Charged equivalent Default Fully Paid default.previous output, show “Default” variable class \n“character,” function apply accept numeric\nfactor variables. transform variable “factor.”","code":"\nlibrary(openxlsx)\nlibrary(caret)\nlibrary(MASS)\ndata<-openxlsx::read.xlsx(\"data/credit.xlsx\")\nstr(data[,1:10])\n#> 'data.frame':    873 obs. of  10 variables:\n#>  $ Default            : chr  \"Fully Paid\" \"Fully Paid\" \"Fully Paid\" \"Fully Paid\" ...\n#>  $ term               : num  1 1 2 2 1 1 1 1 1 1 ...\n#>  $ installment        : num  123 820 433 290 405 ...\n#>  $ grade              : num  3 3 2 6 3 2 2 1 2 3 ...\n#>  $ emp_title          : num  299 209 623 126 633 636 481 540 631 314 ...\n#>  $ emp_length         : num  3 3 3 5 6 3 3 8 3 5 ...\n#>  $ home_ownership     : num  1 1 1 1 3 1 1 3 1 1 ...\n#>  $ annual_inc         : num  55000 65000 63000 104433 34000 ...\n#>  $ verification_status: num  1 1 1 2 2 1 1 1 1 1 ...\n#>  $ purpose            : num  3 10 4 6 3 3 6 2 2 9 ...\ntable(data[,\"Default\"])\n#> \n#> Charged Off  Fully Paid \n#>         145         728\ndata[,\"Default\"]<-factor(data[,\"Default\"])"},{"path":"credit-analysis.html","id":"prediction-with-the-logit-model","chapter":"6 Credit analysis","heading":"6.0.1 Prediction with the Logit model","text":"First, split data set training test, 80% training\n20% test data set. explanation procedure, see\nchapter Machine learning market direction prediction: Logit.use function sample data set time series. \nrandomly generates dim[1]*n numbers full data set. dim[1]\nnumber rows full data set, n %, case,\n80%.function “sample” generates random numbers, use\n“set.seed” specify seeds control results; words, \nalways get results, even generate random numbers.run following logit model:\\[Default=\\alpha_{0}\\ +\\beta_{1}\\ term_{1}+\\beta_{2}\\ grade_{2}+...+\\beta_{n}\\ variable_{n}+e\\]\n\\(e\\) error term.next code run logit model using train set:next code run logit model variables, using\ntrain set :\\[Default=\\alpha_{0}\\ +\\beta\\ X+e\\] \\(X\\) represents \nvariables inside data bases, except “Default”.next step make prediction test data set, based \n“model_all”.“type = response” argument transform results \nprobability.previous output, see prediction type\n“Fully Paid” “Charged ,” transform forecast \ncategories. set threshold 0.5; prediction higher \n0.5, transform “Fully Paid,” otherwise “Charged\n.” common practice wonder threshold 0.5.\nStill, importantly, estimating prediction accuracy \nmodel, change threshold improve prediction\nperformance.","code":"\nset.seed (1)\ndim<-dim(data)\ntrain_sample<-sample(dim[1],dim[1]*0.8)\n\ntrain <- data[train_sample, ]\ntest  <- data[-train_sample, ]\nmodel<-glm(Default ~ term+grade ,data= train ,family=binomial())\nsummary(model)\n#> \n#> Call:\n#> glm(formula = Default ~ term + grade, family = binomial(), data = train)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.3183   0.3755   0.4647   0.5723   1.3079  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  3.75411    0.34435  10.902  < 2e-16 ***\n#> term        -0.69219    0.24816  -2.789  0.00528 ** \n#> grade       -0.44522    0.09073  -4.907 9.25e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 631.12  on 697  degrees of freedom\n#> Residual deviance: 572.50  on 695  degrees of freedom\n#> AIC: 578.5\n#> \n#> Number of Fisher Scoring iterations: 4\nmodel_all<-glm(Default ~. ,data=train ,family=binomial())\npredict<-predict(model_all, newdata = test,type = \"response\")\nhead(predict)\n#>           10           18           21           23           24           26 \n#> 1.000000e+00 2.220446e-16 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00\npredicf_char<-ifelse(predict>.5,\"Fully Paid\",\"Charged Off\")\nhead(predicf_char)\n#>            10            18            21            23            24 \n#>  \"Fully Paid\" \"Charged Off\"  \"Fully Paid\"  \"Fully Paid\"  \"Fully Paid\" \n#>            26 \n#>  \"Fully Paid\""},{"path":"credit-analysis.html","id":"measuring-model-performance","chapter":"6 Credit analysis","heading":"6.1 Measuring model performance","text":"measure performance prediction, use confusion\nMatrix. , need transform prediction factor.confusion Matrix categorizes predictions according whether\nmatch actual value. One table’s dimensions indicates \npossible categories predicted values, shows \nreal (reference) values.measures “confusionMatrix” functions show. \nchapter, concerned Accuracy Sensitivity.accuracy , therefore, proportion representing number true\npositives negatives divided total number predictions. \ncase, mode Accuracy 0.9485714The sensitivity model (also called true positive rate) measures\nproportion positive examples correctly classified. \nexample, end “confusionMatrix” output see \n‘Positive’ Class “Charged ”. mode Sensitivity 0.9642857.","code":"\n\npredict_factor<-factor(predicf_char)\ncaret::confusionMatrix(predict_factor,test[,\"Default\"])$table\n#>              Reference\n#> Prediction    Charged Off Fully Paid\n#>   Charged Off          27          8\n#>   Fully Paid            1        139\nconfusionMatrix(predict_factor,test[,\"Default\"])\n#> Confusion Matrix and Statistics\n#> \n#>              Reference\n#> Prediction    Charged Off Fully Paid\n#>   Charged Off          27          8\n#>   Fully Paid            1        139\n#>                                           \n#>                Accuracy : 0.9486          \n#>                  95% CI : (0.9046, 0.9762)\n#>     No Information Rate : 0.84            \n#>     P-Value [Acc > NIR] : 8.743e-06       \n#>                                           \n#>                   Kappa : 0.8263          \n#>                                           \n#>  Mcnemar's Test P-Value : 0.0455          \n#>                                           \n#>             Sensitivity : 0.9643          \n#>             Specificity : 0.9456          \n#>          Pos Pred Value : 0.7714          \n#>          Neg Pred Value : 0.9929          \n#>              Prevalence : 0.1600          \n#>          Detection Rate : 0.1543          \n#>    Detection Prevalence : 0.2000          \n#>       Balanced Accuracy : 0.9549          \n#>                                           \n#>        'Positive' Class : Charged Off     \n#> "},{"path":"credit-analysis.html","id":"prediction-with-linear-discriminant-analysis-lda","chapter":"6 Credit analysis","heading":"6.2 Prediction with Linear Discriminant Analysis (LDA)","text":"need another method already logistic model?\nseveral reasons (James et al. 2017):• classes well-separated, parameter estimates \nlogistic regression model surprisingly unstable. linear\nDiscriminant Analysis method suffer problem.• number observations small distribution \nindependent variables approximately normal class, linear\ndiscriminant model stable logistic regression\nmodel.chapter, use LDA model compare Accuracy\nLogit model.next code estimates LDA model makes prediction:object “pred_lda” R-list, contains prediction, \nalso many statistics, apply “confusionMatrix” need\nget prediction results:Accuracy LDA model 0.9542857, Sensitivity 0.8571429.case, LDA Accuracy higher logit model; \nsensitivity opposite. Depending interested , \nmodel better predicts performance. “credit allocation,” \nusually concerned ‘Positive’ cases, case, “Charged\n,” default risk.","code":"\nmodel_lda<-MASS::lda(Default~.,data=train)\npred_lda<-predict(model_lda, newdata = test)\nconfusionMatrix(pred_lda[[\"class\"]],test[,\"Default\"])\n#> Confusion Matrix and Statistics\n#> \n#>              Reference\n#> Prediction    Charged Off Fully Paid\n#>   Charged Off          24          4\n#>   Fully Paid            4        143\n#>                                           \n#>                Accuracy : 0.9543          \n#>                  95% CI : (0.9119, 0.9801)\n#>     No Information Rate : 0.84            \n#>     P-Value [Acc > NIR] : 2.372e-06       \n#>                                           \n#>                   Kappa : 0.8299          \n#>                                           \n#>  Mcnemar's Test P-Value : 1               \n#>                                           \n#>             Sensitivity : 0.8571          \n#>             Specificity : 0.9728          \n#>          Pos Pred Value : 0.8571          \n#>          Neg Pred Value : 0.9728          \n#>              Prevalence : 0.1600          \n#>          Detection Rate : 0.1371          \n#>    Detection Prevalence : 0.1600          \n#>       Balanced Accuracy : 0.9150          \n#>                                           \n#>        'Positive' Class : Charged Off     \n#> "},{"path":"credit-analysis.html","id":"cross-validation.","chapter":"6 Credit analysis","heading":"6.2.1 3 Cross validation.","text":"Cross-validation “resampling” method. involves repeatedly\ndrawing samples training set refitting model interest sample obtain additional information model. approach may allow us get information available fitting model using original training sample.Instead dividing sample , approach involves\nrandomly k-fold CV splits set observations k groups, \nfolds, approximately equal size.words, procedure validate Accuracy/sensitivity stable split sample training test several times. Caret function “train” can optimize Accuracy.start applying logit model:LDA:results consistent previous result; Accuracy \nLDA higher Logit one.improve accuracy, also apply variable selection methods,\nglmStepAIC.Warning: following code takes 20 minutes run, depending\nprocessor.accuracy 0.9599 higher Logit model, includes variables 0.9470.get final model final variables, apply following:want use variables improve model:example, run LDA model variables :got higher accuracy.","code":"\n# similar to the previous models\ngbmFit1 <- train(Default ~ ., data = train,\n                  \n                 method = \"glm\",\n                 \n# in here we have the tuning parameters, in this case we use the \"cv\" method for cross, which is the number of times the model split the sample.                   \n                            trControl = trainControl(method = \"cv\", number = 10),\n                        trace=0,   metric=\"Accuracy\")\ngbmFit1\n#> Generalized Linear Model \n#> \n#> 698 samples\n#>  70 predictor\n#>   2 classes: 'Charged Off', 'Fully Paid' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 627, 628, 628, 629, 628, 629, ... \n#> Resampling results:\n#> \n#>   Accuracy   Kappa    \n#>   0.9470376  0.8144761\n# similar to the previous models\ngbmFit1 <- train(Default ~ ., data = train,\n                  \n                 method = \"lda\",\n                 \n# in here we have the tuning parameters, in this case we use the \"cv\" method for cross, which is the number of times the model split the sample.                   \n                            trControl = trainControl(method = \"cv\", number = 10),\n                        trace=0,   metric=\"Accuracy\")\ngbmFit1\n#> Linear Discriminant Analysis \n#> \n#> 698 samples\n#>  70 predictor\n#>   2 classes: 'Charged Off', 'Fully Paid' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 628, 629, 629, 628, 628, 628, ... \n#> Resampling results:\n#> \n#>   Accuracy   Kappa    \n#>   0.9527329  0.8275475\ngbmFit1 <- train(Default ~ ., data = train, method = \"glmStepAIC\",\n              \n                 trControl = trainControl(method = \"cv\", number = 10),\n                        trace=0,   metric=\"Accuracy\")\n    gbmFit1## Generalized Linear Model with Stepwise Feature Selection\n##\n## 698 samples \n## 70 predictor \n## 2 classes: 'Charged Off', 'Fully Paid'\n##\n## No pre-processing Resampling: Cross-Validated (10 fold)\n## Summary of sample sizes: 628, 629, 628, 628, 628, 628, ... \n## Resampling results:\n##\n## Accuracy   Kappa\n## 0.9599149 0.8585923\nstep_var<-rownames(data.frame(gbmFit1$finalModel$coefficients))[-1] \nstep_var#>  [1] \"term\"                  \"purpose\"               \"revol_bal\"            \n#>  [4] \"total_rec_int\"         \"recoveries\"            \"last_pymnt_amnt\"      \n#>  [7] \"last_fico_range_high\"  \"open_act_il\"           \"total_cu_tl\"          \n#> [10] \"num_accts_ever_120_pd\" \"num_sats\"              \"num_tl_op_past_12m\"   \n#> [13] \"total_bc_limit\"\ntrain_step<-cbind(train[,\"Default\"],train[,step_var])\ncolnames(train_step)[1]<-\"Default\"\ntest_step<-cbind(test[,\"Default\"],test[,step_var])\ncolnames(test_step)[1]<-\"Default\"\ngbmFit1 <- train(Default ~ ., data = train_step,\n                  \n                 method = \"lda\",\n                 \n# in here we have the tuning parameters, in this case we use the \"cv\" method for cross, which is the number of times the model split the sample.                   \n                            trControl = trainControl(method = \"cv\", number = 10),\n                        trace=0,   metric=\"Accuracy\")\ngbmFit1\n#> Linear Discriminant Analysis \n#> \n#> 698 samples\n#>  13 predictor\n#>   2 classes: 'Charged Off', 'Fully Paid' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 628, 628, 627, 629, 628, 628, ... \n#> Resampling results:\n#> \n#>   Accuracy   Kappa    \n#>   0.9612612  0.8568619"},{"path":"rational-agent-and-behavioral-finance-in-investment.html","id":"rational-agent-and-behavioral-finance-in-investment","chapter":"7 Rational agent and behavioral finance in investment","heading":"7 Rational agent and behavioral finance in investment","text":"chapter use following libraries:Around 1970, economists argued efficient market instantaneously reflect available information particular financial security, called Efficient Market Hypothesis EMH (Fama 1970). , arbitrage opportunities difficult exist, used argue markets needed predictable. Academics reasonably content EMH in1987 stock market behavior 1987 bizarre. year began Dow Jones Industrial Average’s historic collapse. interesting 1987 trading folklore activities leading academic economists fit behavioral finance point view, EMH point view. Economists actively discussing acting financial markets seemed believe markets predictable, key principle modern behavioral finance (Burton Shah 2013).designing systematic trading platforms, traders trading systems aim generate trading signals consistently produce positive outcomes many trades. Usually, trades test successfully trading systems large amounts past historical data. scientific method analyzing particular financial security may lie determining whether security price changes random . price changes random, probability detecting consistently profitable trading opportunity particular security small. hand, price changes nonrandom, financial security persistent predictability analyzed . , possible measure relative availability trading opportunities market inefficiency tests (Aldridge 2010). summary, tests detect new information takes slowly asset prices, arbitrage opportunities exist, market inefficient.chapter, apply test proposed (Wooldridge 2020) identify arbitrage opportunities find inefficient markets.","code":"\nlibrary(quantmod)\nlibrary(lmtest)\nlibrary(openxlsx)\nlibrary(dplyr)"},{"path":"rational-agent-and-behavioral-finance-in-investment.html","id":"emh-test-on-historical-returns-for-one-asset","chapter":"7 Rational agent and behavioral finance in investment","heading":"7.1 EMH test on historical returns for one asset","text":"Suppose \\(y_{t}\\) daily price S&P500. strict form efficient markets hypothesis establishes historical information index day t help predict index. use past information \\(y_{t}\\), market efficient following true:\\[y_t= \\beta_0 +\\beta_1\\ y_{t-1} + \\beta_2\\ y_{t-2}+u_t\\]term right expected value \\(y_{t}\\), given historical information index \\(y_{t-1} ,y_{t-2},....\\). words, expected value depend historical information. However, previous equation false, implies use information predict current price. previous equation false, use information past predict current price.Suppose want make EMH test returns S&P 500 index. next code, download S&P 500 index “2019-10-20” “2022-10-20”. Remember yahoo finance, ticker name “^GSPC”.take close price index, apply function Delt estimate returns.\\[y_t= \\beta_0 +\\beta_1\\ y_{t-1} + \\beta_2\\ y_{t-2}+u_t\\]run previous model, need create variables store data frame. apply function lag create lagged variables-convenience, change column names.apply OLS regression:Remember, significant beta1 coefficient reject EMH.example, use past information price moving average forecast Biance price.\\[bnb_{t}=\\alpha\\ +\\beta1\\ bnb_{t-1}+\\beta2\\ bnb_{t-2} +\\beta3\\ sma +\\ e\\]","code":"\nticker<-\"^GSPC\"\ngetSymbols(ticker, from=\"2019-10-20\", to=\"2022-10-20\")\n#> [1] \"^GSPC\"\nclose<-GSPC[,4]\nret<-Delt(close)\nlag1<-stats::lag(ret[,1],1)\nlag2<-stats::lag(ret[,1],2)\nret<-cbind(ret, lag1,lag2)\ncolnames(ret)<-c(\"SP500\",\"SP500_lag1\",\"SP500_lag2\")\nmodel<-lm(SP500 ~.,data =ret)\nsummary(model)\n#> \n#> Call:\n#> lm(formula = SP500 ~ ., data = ret)\n#> \n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -0.111441 -0.005786  0.000537  0.007425  0.092978 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.0004309  0.0005521   0.780 0.435371    \n#> SP500_lag1  -0.1990963  0.0362312  -5.495 5.35e-08 ***\n#> SP500_lag2   0.1248616  0.0362416   3.445 0.000602 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.01514 on 750 degrees of freedom\n#>   (3 observations deleted due to missingness)\n#> Multiple R-squared:  0.06649,    Adjusted R-squared:  0.06401 \n#> F-statistic: 26.71 on 2 and 750 DF,  p-value: 6.219e-12"},{"path":"rational-agent-and-behavioral-finance-in-investment.html","id":"emh-test-for-variance-for-one-asset","chapter":"7 Rational agent and behavioral finance in investment","heading":"7.2 EMH test for variance for one asset","text":"financial time series, stock returns, expected returns depend past returns (Market efficient), variance returns . example, model:\\[r_t= \\beta_0 +\\beta_1\\ r_{t-1}+u_t\\]apply test verify variance returns effect o returns:\\[u^2_{t}= \\delta_0 +\\delta_1\\ r_{t-1}+e_t\\]previous model heteroskedasticity test, applya Breusch-Pagan test heteroskedasticity, using function bptest(model)Suppose test significant former result (p-value <10%). EMH, use variance standard deviation make prediction.example, use past information standard deviation moving average, \\(sdma\\), forecast Biance price.\\[bnb_{t}=\\alpha\\ +\\beta1\\ bnb_{t-1}+\\beta2\\ bnb_{t-2} +\\beta3\\ sma +\\beta4\\ sdma +\\ e\\]See chapter 5 book example. “Big data machine learning”\nhttps://www.arturo-bernal.com/book/AFP/big-data--machine-learning.htmlWe show another example next section.","code":"\nlmtest::bptest(model)\n#> \n#>  studentized Breusch-Pagan test\n#> \n#> data:  model\n#> BP = 27.216, df = 2, p-value = 1.23e-06"},{"path":"rational-agent-and-behavioral-finance-in-investment.html","id":"emh-test-for-variance-for-many-asset-to-build-a-portfolio","chapter":"7 Rational agent and behavioral finance in investment","heading":"7.3 EMH test for variance for many asset to build a portfolio","text":"next session, cover subject market anomalies. cover momentum anomalies. However, strategy constructing portfolio stocks. However, first, need filter stocks can make prediction. filter variance test efficient markets hypothesis.First read data, convenience, transform data frame xts.estimated returns, Delt.\napply(data, 2, func)Xts : becasue loos xts property:following code test made one stock previous second section chapter:next chunk shows “Loop ” show similar procedure previous chunk. knowledge requires level course’s contents. expect develop process final exam following procedure previous chunk, building R function together “apply” function, creating “Loop .”following code helps us count number tickers use make prediction, applying “ifelse” function combing object containing EMH test.Finally, filter get tickers category predict.Also take historical information filtered stocks","code":"\n\ndf<-read.xlsx(\"data/df_dates.xlsx\", detectDates = T)\ndate<-df[,1]\ndatax<- xts(df[,-1],\n         order.by = as.Date(date))\nreturn_all<-apply(datax, 2, Delt)\nreturn_all_xts<-xts(return_all,\n         order.by = as.Date(date))\nhead(return_all_xts[,1:4]) \n#>              AAPL.Close   MSFT.Close    GOOG.Close  GOOGL.Close\n#> 2020-01-02           NA           NA            NA           NA\n#> 2020-01-03 -0.009722044 -0.012451750 -0.0049072022 -0.005231342\n#> 2020-01-06  0.007968248  0.002584819  0.0246570974  0.026654062\n#> 2020-01-07 -0.004703042 -0.009117758 -0.0006240057 -0.001931646\n#> 2020-01-08  0.016086289  0.015928379  0.0078803309  0.007117757\n#> 2020-01-09  0.021240806  0.012492973  0.0110444988  0.010497921\ni<-1\n\n#For the next chunk:\n\n#---------------copy from here------- \ndata<-return_all_xts[, i]\nlag1 <- stats::lag(data,1)\nlag2 <- stats::lag(data,2)\nret <- data.frame(cbind(data, lag1,lag2))\ncolnames(ret)<-c(\"SP500\",\"SP500_lag1\",\"SP500_lag2\")\n\n#-------------to here ------------\n\nmodel <- lm(SP500 ~.,data =ret)\nbp <- bptest(model)\nbp <- bp[[\"p.value\"]]\nbp\n#>         BP \n#> 0.00134487\nbp_all_Loop<-data.frame()\ndim<-dim(return_all_xts)[2] \nfor (i in 1:dim){\n \n#----copy here -----\n\n  data<-return_all_xts[, i]\nlag1 <- stats::lag(data,1)\nlag2 <- stats::lag(data,2)\nret <- data.frame(cbind(data, lag1,lag2))\ncolnames(ret)<-c(\"SP500\",\"SP500_lag1\",\"SP500_lag2\")\n\n# ----to Here----------------\n\nmodel <- lm(SP500 ~.,data =ret)\n  bp <- bptest(model)\n  bp <- bp[[\"p.value\"]]\n  bp_all_Loop[i,1]<-colnames(return_all_xts)[i]\n  bp_all_Loop[i,2]<-bp\n}\ncolnames(bp_all_Loop)<-c(\"Ticker\",\"p-value\")\nhead(bp_all_Loop)\n#>        Ticker     p-value\n#> 1  AAPL.Close 0.001344870\n#> 2  MSFT.Close 0.002359318\n#> 3  GOOG.Close 0.007415325\n#> 4 GOOGL.Close 0.009713821\n#> 5  AMZN.Close 0.542408713\n#> 6  TSLA.Close 0.086492404\ntail(bp_all_Loop)\n#>          Ticker      p-value\n#> 95    TXN.Close 0.0002357208\n#> 96    CRM.Close 0.8711756478\n#> 97    BMY.Close 0.1610277136\n#> 98    UPS.Close 0.9590241181\n#> 99  RLLCF.Close 0.8083656314\n#> 100  QCOM.Close 0.0539758548\npred<-ifelse(bp_all_Loop[,\"p-value\"]<0.1,\"Predict\",\"No Predict\" )\n\nbp_all_Loop_1<- cbind(bp_all_Loop,pred)\n\nhead(bp_all_Loop_1)\n#>        Ticker     p-value       pred\n#> 1  AAPL.Close 0.001344870    Predict\n#> 2  MSFT.Close 0.002359318    Predict\n#> 3  GOOG.Close 0.007415325    Predict\n#> 4 GOOGL.Close 0.009713821    Predict\n#> 5  AMZN.Close 0.542408713 No Predict\n#> 6  TSLA.Close 0.086492404    Predict\ntail(bp_all_Loop_1)\n#>          Ticker      p-value       pred\n#> 95    TXN.Close 0.0002357208    Predict\n#> 96    CRM.Close 0.8711756478 No Predict\n#> 97    BMY.Close 0.1610277136 No Predict\n#> 98    UPS.Close 0.9590241181 No Predict\n#> 99  RLLCF.Close 0.8083656314 No Predict\n#> 100  QCOM.Close 0.0539758548    Predict\nbp_all_Loop_f<- bp_all_Loop_1 %>%\n  dplyr::filter(pred == \"Predict\") \nhead(bp_all_Loop_f)\n#>        Ticker      p-value    pred\n#> 1  AAPL.Close 1.344870e-03 Predict\n#> 2  MSFT.Close 2.359318e-03 Predict\n#> 3  GOOG.Close 7.415325e-03 Predict\n#> 4 GOOGL.Close 9.713821e-03 Predict\n#> 5  TSLA.Close 8.649240e-02 Predict\n#> 6 BRK.A.Close 3.950538e-06 Predict\ntail(bp_all_Loop_f)  \n#>        Ticker      p-value    pred\n#> 71  WFC.Close 6.487655e-07 Predict\n#> 72 C.PJ.Close 3.292293e-07 Predict\n#> 73   PM.Close 2.047688e-07 Predict\n#> 74  LIN.Close 3.799114e-02 Predict\n#> 75  TXN.Close 2.357208e-04 Predict\n#> 76 QCOM.Close 5.397585e-02 Predict\nticker<-bp_all_Loop_f[,\"Ticker\"]\nemh<-return_all_xts[,ticker]\nhead(emh[,1:4])\n#>              AAPL.Close   MSFT.Close    GOOG.Close  GOOGL.Close\n#> 2020-01-02           NA           NA            NA           NA\n#> 2020-01-03 -0.009722044 -0.012451750 -0.0049072022 -0.005231342\n#> 2020-01-06  0.007968248  0.002584819  0.0246570974  0.026654062\n#> 2020-01-07 -0.004703042 -0.009117758 -0.0006240057 -0.001931646\n#> 2020-01-08  0.016086289  0.015928379  0.0078803309  0.007117757\n#> 2020-01-09  0.021240806  0.012492973  0.0110444988  0.010497921\ntail(emh[,1:4])\n#>              AAPL.Close   MSFT.Close    GOOG.Close  GOOGL.Close\n#> 2022-05-19 -0.024641392 -0.003699634 -0.0147285646 -0.013543429\n#> 2022-05-20  0.001747288 -0.002291226 -0.0129350191 -0.013371513\n#> 2022-05-23  0.040119232  0.032031977  0.0215299497  0.023689766\n#> 2022-05-24 -0.019215988 -0.003951656 -0.0514075636 -0.049494164\n#> 2022-05-25  0.001139947  0.011170149 -0.0008165988 -0.001556952\n#> 2022-05-26  0.023199508  0.012875229  0.0232096155  0.018784556"},{"path":"momentum-or-directional-trading.html","id":"momentum-or-directional-trading","chapter":"8 Momentum or directional trading","heading":"8 Momentum or directional trading","text":"previous chapter, covered “Long samples test efficient markets hypothesis (EMH) applied portfolio”. remember, filter get stocks EMH fails, test suggests inefficient market, can make prediction based past information.final goal (expect cover final session) create portfolio filtered stocks submit trading platform, interactive brokers.chapter, make another filter get stocks high expected performance. apply trading strategy called Momentum. strategy consists buying stocks instrument trending selling . idea historical winners expected winners historical losers expected lose short run. momentum effect considered market anomaly (see Cervantes, M., Montoya, M. Á., & Bernal, L.. (2016))Note. also apply machine learning technique predict stocks make filter get ones better risk-reward expected performance, however, probably covered topic UF Algorithms Data Analysis, 2nd period.create signal buy sell, based variables created past information (reason made EMH test). Finally, cover make back-testing using historical data.code methodology based Based : Jeet Vats Learning (2017). code authorship.First read dataTo back-test momentum strategy, divide data set two smaller data sets called -sample -sample data sets. Something similar training test machine learning.define four dates. in_sd defines date -sample data starts, in_ed -sample end date. Similarly, out_sd out_ed defined -sample start end dates. dates defined order data time series format interested building model historical data used real-time data, , data set dates later historical data:sample 1-3 years sample 1 year.back-test momentum strategy, divide data set two: -sample -sample. Something like training test machine learning. back-testing proving strategy performance implementing , without wait days, months see works. Also, helps control human bias towards parameter estimation. use -sample data back-test strategy, estimate optimal set parameters, evaluate performance.-sample data estimating optimal set parameters, evaluate performance. optimal set parameters must applied -sample data understand generalization capacity rules parameters. performance -sample data pretty similar -sample data, assume parameters rule set good generalization power can used live trading.create subsample, price return, datax retx.subset(data,\n+index(data)>= initial date &\n+index(data)<= final date)use moving average convergence divergence (MACD) Bollinger band indicators generate automated trading signals.MACD Bollinger band indicators calculated using following two lines code. used parameter values functions; however, can use parameters think best dataset. output variable macd contains MACD indicator signal value; however, output variable bb contains lower band, average, upper band, percentage Bollinger band:use 26 days lags ArchTest function. generating MACD 26 lags, implying use information past 26 days predict signal buying selling stock.going star single stock, one first column, understand . apply procedure stocks.apply first stock price, in_sample[,1]\nMACD(data, nFast = , nSlow = , nSig = ,maType=“SMA”)usually nfast 12 nSlow 26 nsig 9.BBands(data, n = , maType=“SMA”, sd = )\nusually n=20 se=2Now create variable signal initializes NULL. second line, generated buy signal (1) dji upper Bollinger band macd value macd-signal value; sell signal (-1) dji lower Bollinger band macd less macd-signal value; market signal 0:signal <- ifelse(in_data[,col]> bb[,‘’] &macd[,‘macd’] >macd[,‘signal’],1,ifelse(in_sample[,col]< bb[,‘dn’] &macd[,‘macd’] <macd[,‘signal’],-1,0))\n“col” column numberWe can modify signal generation mechanism use criterion. haven’t included transaction cost slippage cost calculate performance none strategies directly trading.estimate strategy return, use return previous day signal. like assuming , buying signal, buy today sell tomorrow, making one day profit.in_return[,col]*(stats::lag(signal[,col]))\n“col” column numberWe use package PerformanceAnalytics calculate strategy performance annual return.\nEstimating ReturnsAnnualized geometric return\\[ geometric\\ return =\\prod_{=1}^{n} (1+HPR)^{scale/n}\\]\n\\(\\prod_{=1}^{n} (1+HPR)\\) product (1+HPR). Also, n number observations, scale number periods year (daily scale = 252, monthly scale = 12, quarterly scale = 4) HPR Holding Period Return:\\[HPR=  \\frac{Price_{t} - Price_{t-1}}{Price_{t-1}}\\]Return.annualized(data,geometric = T,scale= )Annualized SD\\[ Std\\ Dev.annualized  = (variance(HPR)*252)^{0.5}\\]StdDev.annualized(x,scale=)Assuming risk free rate zero, estimate Sharp Ratio.","code":"\nlibrary(openxlsx)\nlibrary(quantmod)\n\ndata<-read.xlsx(\"data/dfx_2.xlsx\")\n\n# The following codes are to transform the data frame date into xts (as we did last session)\ndate<-data[,1]\n\n# liminating the fits column\ndata<-data[,-1]\n\n#Applying the xts function\ndatax<- xts(data,\n         order.by = as.Date(date))\n\n# Eliminating rows with missing values\ndatax<-na.omit(datax)\n\n#Also we estimate the returns for each stock, as we did in last session\nret<-apply(datax,2,Delt)\n\n# we lost the xts \nretx<- xts(ret,\n         order.by = as.Date(date))\n# Eliminating rows with missing values\nretx<-na.omit(retx)\nin_sd<- \"2018-05-26\"\nin_ed<- \"2021-05-26\" \n\nout_sd<- \"2021-05-27\"\nout_ed<- \"2022-05-27\"\nin_sd<- \"2018-05-26\"  # están en la 59\nin_ed<- \"2021-05-26\" \n# in_sample of the price of the stocks\nin_datax<- subset(datax,\n  +index(datax)>= in_sd &\n  +index(datax)<= in_ed)\n# in_sample of the returns\nin_ret<- subset(retx,\n  +index(retx)>= in_sd &\n  +index(retx)<= in_ed)\n\nout_datax<- subset(datax,\n  +index(datax)>= out_sd &\n  +index(datax)<= out_ed)\n# in_sample of the returns\nout_ret<- subset(retx,\n  +index(retx)>= out_sd &\n  +index(retx)<= out_ed)\nmacd<-MACD(in_datax[,3] , nFast =12 , nSlow =26  , nSig =9 ,maType=\"SMA\")\n\nbb<-BBands(in_datax[,3], n = 20, maType=\"SMA\", sd = 2)\nsignal <- ifelse(in_datax[,1]> bb[,'up'] & macd[,'macd'] >macd[,'signal'],1,ifelse(in_datax[,1]< bb[,'dn'] &macd[,'macd'] <macd[,'signal'],-1,0))\nplot(signal[,1])\nstrat_ret<-in_ret[,1]*(stats::lag(signal[,1]))\nlibrary(PerformanceAnalytics)\n# The strategy return signal \nret_annual<-Return.annualized(strat_ret,geometric = T,scale= 252)\nret_annual\n#>                   AAPL.Close\n#> Annualized Return -0.3863758\n\nsd<-StdDev.annualized(strat_ret,scale=252)\n\n# Sharpe ratio\nret_annual/sd\n#>                   AAPL.Close\n#> Annualized Return  -1.077952"},{"path":"momentum-or-directional-trading.html","id":"bibliography-2","chapter":"8 Momentum or directional trading","heading":"8.1 Bibliography","text":"Cervantes, M., Montoya, M. Á., & Bernal, L.. (2016). Effect Business Cycle Investment Strategies: Evidence Mexico. Revista Mexicana de Economía y Finanzas, 11(2).Jeet, P VatsLearning, P (2017). Quantitative Finance R. Packt Publishing.","code":""},{"path":"portfolio-management-algorithms.html","id":"portfolio-management-algorithms","chapter":"9 Portfolio management algorithms","heading":"9 Portfolio management algorithms","text":"chapter, take filtered stocks “Rational agents theory behavioral finance theories”. remember, previous chapter applied momentum strategy.file df_merge.xlsx find estimations in_sample out_sample.mention previous chapter, performance -sample data pretty like -sample data, assume parameters rule set good generalization power can used live trading. session, filter stocks similar -sample -sample data. purpose, took difference Sharpe ratios in_sample out_sample. also need define threshold tolerance difference. example, take stocks difference less 20% absolute value.\ndf %>%\nfilter(Sharpe_diff < n & Sharpe_diff > -n)\nn thresholdThe momentum strategy consists buying stocks instrument trending selling . case, order sample in_Sharpe, split sample 3 tranches. first stocks taking long positions 3rd one shorts positions.df %>% arrange(desc(col))next code make split winnersThe next code make split losersFinally, combine tranches 1 3 one single objectWe generate thousands simulations portfolio weights, need generate aleatory numbers weights. long position, weights must positive short position, weight must negative. , first trancheThe function runif create random numbers apply function first tranche, runif(n, 0, 1), n number simulations want. need generate number random weights rend_win object.Regarding set.seed(42), runif generate aleatory numbers. useful take # set.seed(42) get result. everyone gets results, insert # .simplicity, generate one portfolio weights simulation, late generate .\nrunif.short position weights must negative. , 3rd tranche:Finally, combine weights.portfolio standard deviation result covariance multiplied portfolio weights.estimate covariance matrix, tickers tranches 1 3. covariance need returs tickers .filtered stocks, get returns stocks, taking returns estimated last session, :Also, filter get filtered stocks, co_allportfolio covariance\ncov(df,use=“complete.obs”)portfolio_std =cov%% weigths\n%% para multiplicar matricesBut need annualized portfolio_std\ntwe<-t(weigths)\nportfolio_std_1=(twe%%portfolio_std252)^.5The following code annualized returns in_sample data, momentum portfolio.","code":"\ndf_merge<-read.xlsx(\"data/df_merge.xlsx\",rowNames=T)\ntreh<-0.2\n\n  df_merge2<- df_merge   %>%\n  filter(Sharpe_diff < treh & Sharpe_diff > -treh)\ndf_merge2\n#>                 in_return     in_sd in_Sharpe   out_return     out_sd\n#> AAPL.Close   -0.054771548 0.2747365   -0.1994 -0.082373325 0.23299812\n#> AMZN.Close   -0.016509915 0.2788159   -0.0592 -0.035770361 0.29049652\n#> TSLA.Close    0.564098218 0.5666686    0.9955  0.532664333 0.45951725\n#> TSM.Close    -0.135791198 0.3021663   -0.4494 -0.121677232 0.22938977\n#> BAC.Close     0.027169757 0.3355819    0.0810 -0.011570730 0.20220638\n#> NSRGF.Close  -0.223354678 0.1760606   -1.2686 -0.210133138 0.15186225\n#> LVMUY.Close  -0.108547037 0.2736889   -0.3966 -0.101687079 0.24380399\n#> BAC.PK.Close  0.017871194 0.1272181    0.1405  0.001189315 0.06644728\n#> RHHBF.Close  -0.667692583 0.3264243   -2.0455 -0.660057680 0.33908716\n#> KO.Close     -0.038464295 0.2080766   -0.1849 -0.023579715 0.12021278\n#> TM.Close     -0.091717493 0.2144692   -0.4276 -0.118928664 0.21121348\n#> RYDAF.Close   0.017644987 0.3660373    0.0482  0.001445751 0.20941036\n#> SHEL.Close   -0.010895257 0.3851020   -0.0283 -0.003778604 0.17708269\n#> BHP.Close    -0.028648261 0.3246614   -0.0882 -0.029134810 0.25596044\n#> VZ.Close     -0.119001044 0.1588481   -0.7491 -0.102977092 0.12531984\n#> AZN.Close    -0.207450467 0.2140546   -0.9691 -0.193951418 0.17725594\n#> AZNCF.Close  -0.372264173 0.2180492   -1.7072 -0.346987471 0.21398241\n#> ADBE.Close   -0.044939517 0.2979994   -0.1508 -0.049712045 0.24744804\n#> NVSEF.Close  -0.095764445 0.2272564   -0.4214 -0.034737903 0.11387070\n#> DIS.Close    -0.045685926 0.2775004   -0.1646 -0.079449603 0.21852916\n#> ORCL.Close   -0.121939597 0.2788517   -0.4373 -0.095171013 0.25138906\n#> WFC.PR.Close -0.002421536 0.1492000   -0.0162  0.002284474 0.05670749\n#> TMUS.Close   -0.216877202 0.2408982   -0.9003 -0.195021363 0.18670398\n#>              out_Sharpe Sharpe_diff\n#> AAPL.Close      -0.3535      0.1541\n#> AMZN.Close      -0.1231      0.0639\n#> TSLA.Close       1.1592     -0.1637\n#> TSM.Close       -0.5304      0.0810\n#> BAC.Close       -0.0572      0.1382\n#> NSRGF.Close     -1.3837      0.1151\n#> LVMUY.Close     -0.4171      0.0205\n#> BAC.PK.Close     0.0179      0.1226\n#> RHHBF.Close     -1.9466     -0.0989\n#> KO.Close        -0.1961      0.0112\n#> TM.Close        -0.5631      0.1355\n#> RYDAF.Close      0.0069      0.0413\n#> SHEL.Close      -0.0213     -0.0070\n#> BHP.Close       -0.1138      0.0256\n#> VZ.Close        -0.8217      0.0726\n#> AZN.Close       -1.0942      0.1251\n#> AZNCF.Close     -1.6216     -0.0856\n#> ADBE.Close      -0.2009      0.0501\n#> NVSEF.Close     -0.3051     -0.1163\n#> DIS.Close       -0.3636      0.1990\n#> ORCL.Close      -0.3786     -0.0587\n#> WFC.PR.Close     0.0403     -0.0565\n#> TMUS.Close      -1.0445      0.1442\ndf_filtered<- df_merge2 %>% arrange(desc(in_Sharpe))\n# to gt the names of those stocks\nco<-rownames(df_filtered)\nle<-length(co)\nn<-round(le/3,0)  \nwin<-co[1:n] # long positions\nn\n#> [1] 8\nloss<-co[(le-n):le]\nloss # short positions\n#> [1] \"TM.Close\"    \"ORCL.Close\"  \"TSM.Close\"   \"VZ.Close\"    \"TMUS.Close\" \n#> [6] \"AZN.Close\"   \"NSRGF.Close\" \"AZNCF.Close\" \"RHHBF.Close\"\nco_all<-c(win,loss)\nco_all\n#>  [1] \"TSLA.Close\"   \"BAC.PK.Close\" \"BAC.Close\"    \"RYDAF.Close\"  \"WFC.PR.Close\"\n#>  [6] \"SHEL.Close\"   \"AMZN.Close\"   \"BHP.Close\"    \"TM.Close\"     \"ORCL.Close\"  \n#> [11] \"TSM.Close\"    \"VZ.Close\"     \"TMUS.Close\"   \"AZN.Close\"    \"NSRGF.Close\" \n#> [16] \"AZNCF.Close\"  \"RHHBF.Close\"\nw<- 1.2 # long position weight\nw_short<- 1-w \nset.seed(42)\n#runif \nru<-runif(n , 0, 1)\n# weigths sum\nsu<-sum(ru)\n# runif/sum and trasnsforming into data frame\nwe_win<-data.frame(ru*w/su)\n#colnanmes weigth\ncolnames(we_win)<-\"we\"\n# row names from win\nrownames(we_win)<-win\nru<-runif(length(loss), 0, 1)\nset.seed(42)\nsu<-sum(ru)\n# runif/sum and trasnsforming into data frame\nwe_loss<-data.frame(ru*w_short/su) \n#colnanmes weigth\ncolnames(we_loss)<-\"we\"\n# row names from loss\nrownames(we_loss)<-loss\nsum(we_loss) # set.seed(42)\n#> [1] -0.2\nwe_loss\n#>                      we\n#> TM.Close    -0.02150707\n#> ORCL.Close  -0.02308076\n#> TSM.Close   -0.01498448\n#> VZ.Close    -0.02354061\n#> TMUS.Close  -0.03059711\n#> AZN.Close   -0.00836163\n#> NSRGF.Close -0.01513346\n#> AZNCF.Close -0.03077199\n#> RHHBF.Close -0.03202288\nwe_all<-rbind(we_win,we_loss)\nwe_all\n#>                       we\n#> TSLA.Close    0.21952864\n#> BAC.PK.Close  0.22487269\n#> BAC.Close     0.06866573\n#> RYDAF.Close   0.19928491\n#> WFC.PR.Close  0.15400152\n#> SHEL.Close    0.12456895\n#> AMZN.Close    0.17676122\n#> BHP.Close     0.03231633\n#> TM.Close     -0.02150707\n#> ORCL.Close   -0.02308076\n#> TSM.Close    -0.01498448\n#> VZ.Close     -0.02354061\n#> TMUS.Close   -0.03059711\n#> AZN.Close    -0.00836163\n#> NSRGF.Close  -0.01513346\n#> AZNCF.Close  -0.03077199\n#> RHHBF.Close  -0.03202288\ndata<-read.xlsx(\"data/dfx_2.xlsx\")\ndate<-data[,1]\ndata<-data[,-1]\ndatax<- xts(data,\n         order.by = as.Date(date))\ndatax<-na.omit(datax)\nret<-apply(datax,2,Delt)\nretx<- xts(ret,\n         order.by = as.Date(date))\nretx<-na.omit(retx)\nhead(retx[,1:5])\n#>              AAPL.Close   MSFT.Close    GOOG.Close  GOOGL.Close   AMZN.Close\n#> 2020-01-03 -0.009722044 -0.012451750 -0.0049072022 -0.005231342 -0.012139050\n#> 2020-01-06  0.007968248  0.002584819  0.0246570974  0.026654062  0.014885590\n#> 2020-01-07 -0.004703042 -0.009117758 -0.0006240057 -0.001931646  0.002091556\n#> 2020-01-08  0.016086289  0.015928379  0.0078803309  0.007117757 -0.007808656\n#> 2020-01-09  0.021240806  0.012492973  0.0110444988  0.010497921  0.004799272\n#> 2020-01-10  0.002260711 -0.004627059  0.0069726829  0.006458647 -0.009410597\nretx_all<-retx[,co_all]\ncovar<-cov(retx_all,use=\"complete.obs\")\nportfolio_std =covar %*% we_all[,1]\nportfolio_std\n#>                      [,1]\n#> TSLA.Close   7.294191e-04\n#> BAC.PK.Close 1.071041e-04\n#> BAC.Close    3.251108e-04\n#> RYDAF.Close  4.231321e-04\n#> WFC.PR.Close 1.213717e-04\n#> SHEL.Close   4.451261e-04\n#> AMZN.Close   2.318799e-04\n#> BHP.Close    3.398317e-04\n#> TM.Close     1.541582e-04\n#> ORCL.Close   1.497664e-04\n#> TSM.Close    2.662127e-04\n#> VZ.Close     6.751342e-05\n#> TMUS.Close   2.029555e-04\n#> AZN.Close    1.142982e-04\n#> NSRGF.Close  9.421821e-05\n#> AZNCF.Close  7.306948e-05\n#> RHHBF.Close  8.447179e-05\ntwe<-t(we_all[,1])\nportfolio_std_1=(twe%*%portfolio_std*252)^.5\nret_a<-df_merge2[co_all,1]\n\nret_a_f<-twe %*%ret_a \nret_a_f\n#>           [,1]\n#> [1,] 0.1818727"},{"path":"references.html","id":"references","chapter":"10 References","heading":"10 References","text":"","code":""}]
