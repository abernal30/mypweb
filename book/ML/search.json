[{"path":"index.html","id":"machine-learning-introductory-guide","chapter":"Machine learning introductory guide","heading":"Machine learning introductory guide","text":"book Machine learning introductory guide!work Aturo Bernal\nVisit GitHub repository site.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"text examples aim generate basic guide Machine Learning (ML) methodology. writing book inspired students AI concentration Tecnologico de Monterrey spring 2022.document follows practical example: ) continuous variables California Housing Prices data set presented (Géron 2023); ii) categorical variables, follow Lending Club fintech data set Kaggle analyze credit default, financial institution grants personal loans.following steps may differ books, data scientists, experts many ways deal whit machine learning. precisely richness area expertise. existed one way apply machine learning, everybody use !!R files stored GitHub repository site.","code":""},{"path":"preface.html","id":"outline","chapter":"Preface","heading":"Outline","text":"","code":""},{"path":"ml-in-the-bussines-lascape-and-data-collection.html","id":"ml-in-the-bussines-lascape-and-data-collection","chapter":"1 ML in the bussines lascape and data collection","heading":"1 ML in the bussines lascape and data collection","text":"","code":""},{"path":"ml-in-the-bussines-lascape-and-data-collection.html","id":"machine-learning-ml","chapter":"1 ML in the bussines lascape and data collection","heading":"1.1 Machine learning (ML)","text":"short, machine learning problem generally relates prediction data available. Machine Learning science (art) programming computers can learn data (Géron 2023). extracting knowledge data, research field intersection statistics, artificial intelligence, computer science. also known predictive analytics statistical learning (Muller?).\nMachine learning programming, programming problems require ML. detect problem facing ML problem, need objective, benefit company (client) apply business (Burton Shah 2013).paragraph, describe examples. describing examples, use terminology may sound unfamiliar , cover chapters.Understanding goal allows us determine kind data expect handle models apply. Suppose facing problem housing market, business objective detect investment opportunities buying sub-valuated (price) houses predicting housing prices. example, expect housing price data, housing location latitude, longitude, median age z, total rooms, etc. case, may apply supervised models linear regression evaluate model performance RMSE. Another example financial sector predicting new bank customer default loan (repay loan ). case, classification problem, use models logit LDA measure performance Confusion Matrix.\nhand, housing prices example, already information mentioned last paragraph, want know crime affects price areas. solve linear regression, wúltnd ML problem, causality one. ML problem want predict house prices certain areas crime increased. Even ML problem, wouldn´t necessarily benefit client us. example, client housing builder, help decide build. Still, client unaffected relationship crime-house prices, ML problem, benefiting client us.conclusion, handling data running algorithms, suggest establishing business goal, detecting ML problem, benefit company (client).","code":""},{"path":"ml-in-the-bussines-lascape-and-data-collection.html","id":"diferrence-between-machine-leagning-and-causality-approach","chapter":"1 ML in the bussines lascape and data collection","heading":"1.2 Diferrence between Machine Leagning and causality approach","text":"","code":""},{"path":"ml-in-the-bussines-lascape-and-data-collection.html","id":"business-objectives-and-data-sources","chapter":"1 ML in the bussines lascape and data collection","heading":"1.3 Business objectives and data sources","text":"book, use cross-sectional data set consisting sample houses bank clients taken given time: ) house pricing; ii) credit analysis. time series objectives consist observations variable several variables time, see chapters four five book (Bernal 2023).","code":""},{"path":"ml-in-the-bussines-lascape-and-data-collection.html","id":"variables-terminology-and-notation","chapter":"1 ML in the bussines lascape and data collection","heading":"1.4 Variables terminology and notation","text":"Many models cover book kind:\\[y=\\alpha_{0}\\ +\\beta_{1}\\ x_{1}+\\beta_{2}\\ x_{2}+...+\\beta_{n}\\ x_{n}+e\\]\n\\(y\\) called dependent variable, also materials , called explained, output variable response variable. hand, \\(x\\) called independent variables input variables, predictors features. \\(\\beta´s\\) parameters estimated, \\(e\\) error term.regression, idea estimate parameters \\(\\beta_{1}, \\beta_{2},...,\\beta_{n}\\), predict value \\(y\\). happens, represent predicted values estimated parameters :\\[\\hat{y}=\\beta_{0}\\ +\\hat{\\beta_{1}}\\ x_{1}+\\hat{\\beta_{1}}\\ x_{2}+...+\\ \\hat{\\beta_{1}}\\ x_{n}\\]\nAlso, compare \\(y_{}\\) predicted value, call residual, usually denoted \\(\\hat{e}\\). defined :\\[\\hat{e_{}}=y_{}-\\hat{y_{}}=y_{}-\\beta_{0}\\ -\\hat{\\beta_{1}}\\ x_{1i}-\\hat{\\beta_{1}}\\ x_{2i}-,...,-\\ \\hat{\\beta_{1}}\\ x_{ni}\\]\nwords, \\(\\hat{e_{}}\\) residual observation. example, data set \\(n\\) variables \\(m\\) observations, \\(m\\) residuals.","code":""},{"path":"ml-in-the-bussines-lascape-and-data-collection.html","id":"house-pricing","chapter":"1 ML in the bussines lascape and data collection","heading":"1.4.1 House pricing","text":"case, facing problem housing market, business objective detect investment opportunities, buying sub-valuated (price) houses predicting median housing price.can get data GitHub:housing data house media prices, houses location latitude x, longitude y, housing median age z, total rooms, etc. apply following model.\\[median\\_house\\_value=\\beta_{0}\\ +\\beta_{1}\\ longitude+\\beta_{2}\\ latitude+...+\\beta_{n}\\ variable_{n}+e\\]","code":"\nhouse<-read.csv(\"https://raw.githubusercontent.com/abernal30/ml_book/main/housing.csv\")\nstr(house)\n#> 'data.frame':    584 obs. of  52 variables:\n#>  $ Id           : int  1 2 3 4 5 6 7 8 9 10 ...\n#>  $ SalePrice    : int  181500 223500 140000 250000 307000 129900 118000 345000 279500 325300 ...\n#>  $ MSSubClass   : int  20 60 70 60 20 50 190 60 20 60 ...\n#>  $ MSZoning     : int  4 4 4 4 4 5 4 4 4 4 ...\n#>  $ LotFrontage  : int  80 68 60 84 75 51 50 85 91 101 ...\n#>  $ LotArea      : int  9600 11250 9550 14260 10084 6120 7420 11924 10652 14215 ...\n#>  $ LotShape     : int  4 1 1 1 4 4 4 1 1 1 ...\n#>  $ LotConfig    : int  3 5 1 3 5 5 1 5 5 1 ...\n#>  $ Neighborhood : int  25 6 7 14 21 18 4 16 6 16 ...\n#>  $ Condition1   : int  2 3 3 3 3 1 1 3 3 3 ...\n#>  $ BldgType     : int  1 1 1 1 1 1 2 1 1 1 ...\n#>  $ HouseStyle   : int  3 6 6 6 3 1 2 6 3 6 ...\n#>  $ OverallQual  : int  6 7 7 8 8 7 5 9 7 8 ...\n#>  $ OverallCond  : int  8 5 5 5 5 5 6 5 5 5 ...\n#>  $ YearRemodAdd : int  1976 2002 1970 2000 2005 1950 1950 2006 2007 2006 ...\n#>  $ RoofStyle    : int  2 2 2 2 2 2 2 4 2 2 ...\n#>  $ Exterior1st  : int  9 13 14 13 13 4 9 15 13 13 ...\n#>  $ MasVnrType   : int  3 2 3 2 4 3 3 4 4 2 ...\n#>  $ MasVnrArea   : int  0 162 0 350 186 0 0 286 306 380 ...\n#>  $ ExterQual    : int  4 3 4 3 3 4 4 1 3 3 ...\n#>  $ ExterCond    : int  5 5 5 5 5 5 5 5 5 5 ...\n#>  $ Foundation   : int  2 3 1 3 3 1 1 3 3 3 ...\n#>  $ BsmtQual     : int  3 3 4 3 1 4 4 1 3 1 ...\n#>  $ BsmtExposure : int  2 3 4 1 1 4 4 4 1 1 ...\n#>  $ BsmtFinType1 : int  1 3 1 3 3 6 3 3 6 6 ...\n#>  $ BsmtFinSF1   : int  978 486 216 655 1369 0 851 998 0 0 ...\n#>  $ BsmtUnfSF    : int  284 434 540 490 317 952 140 177 1494 1158 ...\n#>  $ HeatingQC    : int  1 1 3 1 1 3 1 1 1 1 ...\n#>  $ CentralAir   : int  2 2 2 2 2 2 2 2 2 2 ...\n#>  $ Electrical   : int  5 5 5 5 5 2 5 5 5 5 ...\n#>  $ X1stFlrSF    : int  1262 920 961 1145 1694 1022 1077 1182 1494 1158 ...\n#>  $ X2ndFlrSF    : int  0 866 756 1053 0 752 0 1142 0 1218 ...\n#>  $ BsmtFullBath : int  0 1 1 1 1 0 1 1 0 0 ...\n#>  $ BsmtHalfBath : int  1 0 0 0 0 0 0 0 0 0 ...\n#>  $ FullBath     : int  2 2 1 2 2 2 1 3 2 3 ...\n#>  $ HalfBath     : int  0 1 0 1 0 0 0 0 0 1 ...\n#>  $ BedroomAbvGr : int  3 3 3 4 3 2 2 4 3 4 ...\n#>  $ KitchenQual  : int  4 3 3 3 3 4 4 1 3 3 ...\n#>  $ TotRmsAbvGrd : int  6 6 7 9 7 8 5 11 7 9 ...\n#>  $ Fireplaces   : int  1 1 1 1 1 2 2 2 1 1 ...\n#>  $ FireplaceQu  : int  5 5 3 5 3 5 5 3 3 3 ...\n#>  $ GarageType   : int  2 2 6 2 2 6 2 4 2 4 ...\n#>  $ GarageYrBlt  : int  1976 2001 1998 2000 2004 1931 1939 2005 2006 2005 ...\n#>  $ GarageFinish : int  2 2 3 2 2 3 2 1 2 2 ...\n#>  $ GarageArea   : int  460 608 642 836 636 468 205 736 840 853 ...\n#>  $ PavedDrive   : int  3 3 3 3 3 3 3 3 3 3 ...\n#>  $ WoodDeckSF   : int  298 0 0 192 255 90 0 147 160 240 ...\n#>  $ OpenPorchSF  : int  0 42 35 84 57 0 4 21 33 154 ...\n#>  $ MoSold       : int  5 9 2 12 8 4 1 7 8 11 ...\n#>  $ YrSold       : int  2007 2008 2006 2008 2007 2008 2008 2006 2007 2006 ...\n#>  $ SaleType     : int  9 9 9 9 9 9 9 7 7 7 ...\n#>  $ SaleCondition: int  5 5 1 5 5 1 5 6 6 6 ..."},{"path":"ml-in-the-bussines-lascape-and-data-collection.html","id":"the-credit-analysis","chapter":"1 ML in the bussines lascape and data collection","heading":"1.4.2 The credit analysis","text":"credit analysis case, interested predicting new bank customer default loan (repay loan ). classification problem, use models logit LDA. can get data also GitHub.database historical information Lendingclub, https://www.lendingclub.com/ fintech marketplace bank scale. original data set least 2 million observations 150 variables. find 873 observations (rows) 71 columns. row represents Lendingclub client. previously made data cleaning (missing values, correlated variables, Zero- Near Zero-Variance Predictors).case, model .\\[Default=\\beta_{0}\\ +\\beta_{1}\\ term_{1}+\\beta_{2}\\ grade_{2}+...+\\beta_{n}\\ variable_{n}+e\\]variable “Default” winch originally name “loan_status”; two labels:“Charge ” means credit grantor wrote account receivables loss closed future charges. account displays status “charge ,” closed future use, although customer still owns debt. example, consider Charged equivalent Default Fully Paid default.previous output, show “Default” variable class “character,” function apply accept numeric factor variables. transform variable “factor.”","code":"\ncredit<-read.csv(\"https://raw.githubusercontent.com/abernal30/ml_book/main/credit.csv\")\nstr(credit[,1:5])\n#> 'data.frame':    873 obs. of  5 variables:\n#>  $ Default    : chr  \"Fully Paid\" \"Fully Paid\" \"Fully Paid\" \"Fully Paid\" ...\n#>  $ term       : int  1 1 2 2 1 1 1 1 1 1 ...\n#>  $ installment: num  123 820 433 290 405 ...\n#>  $ grade      : int  3 3 2 6 3 2 2 1 2 3 ...\n#>  $ emp_title  : num  299 209 623 126 633 636 481 540 631 314 ...\ntable(credit[,\"Default\"])\n#> \n#> Charged Off  Fully Paid \n#>         145         728\ncredit[,\"Default\"]<-factor(credit[,\"Default\"])"},{"path":"ml-in-the-bussines-lascape-and-data-collection.html","id":"take-a-quick-look-at-the-data-structure","chapter":"1 ML in the bussines lascape and data collection","heading":"1.5 Take a Quick Look at the Data Structure","text":"ML literature suggests looking data structure see issues, numerical categorical variables, missing values, etc. several books cover , cover book. suggest chapter two book (Bernal 2023). However, include section book? expect book introductory guide ML. , book’s structure steps develop ML analysis without redundant materials.","code":""},{"path":"training-and-evaluating-regression-models.html","id":"training-and-evaluating-regression-models","chapter":"2 Training and evaluating regression models","heading":"2 Training and evaluating regression models","text":"","code":""},{"path":"training-and-evaluating-regression-models.html","id":"model-training","chapter":"2 Training and evaluating regression models","heading":"2.1 Model training","text":"goal machine learning models regression predict dependent variable, \\(y\\). example, house pricing data set described chapter 1, defined dependent variable “SalePrice”, independent variables MSSubClass, MSZoning others data set.\\[SalePrice=\\beta_{0}+\\beta_{1}\\ MSSubClass + \\beta_{2}\\ MSZoning+ ....+x_{n}+ e\\]\\(e\\) error term. regression model aims estimate parameters \\(\\beta_{1}, \\beta_{2},...,\\beta_{n}\\), predict SalePrice. following predicted model. call training.\\[\\hat{SalePrice}=\\hat{\\beta_{0}}+\\hat{\\beta_{1}}MSSubClass+\\hat{\\beta_{2}}MSZoning+....+\\hat{\\beta_{n}}x_{n} \\]hat parameters indicates estimated parameters.\nNote: learning ML, convenient understand models step step. familiar estimating regression prediction, can skip following steps Training test set (Back testing).exemplify ML regression models works, first estimate two independent variables: MSSubClass MSZoning.\\[SalePrice=\\beta_{0}+\\beta_{1}\\ MSSubClass + \\beta_{2}\\ MSZoning + \\epsilon \\]R, use “lm” function run regression model Ordinary Least squaresAs result, get following estimated model:\\[`\\hat{r con[2]`}= 354837.1+-233.4\\ MSSubClass + -29665.2\\ MSZoning\\]Machine Learning (ML), concerned coefficient significance; chapter one, explain differences Machine Learning models causality approach ones explain . ML models, predict dependent variable, case, SalePrice, given certain features independent variables.Suppose want predict SalePrice values variables MSSubClass MSZoning 20 4, respectively. following formula predicts SalePrice taking parameters OLS regression:SalePrice=354837.1+354837.1 * 20+-29665.2*4 = 231508.1The result 231508.1 predicts SalePrice variables MSSubClass MSZoning take values 20 4, respectively.Remember previous result exposition proposes. use “predict” function, gives us result. must add arguments OLS model object data frame features want predict.still determining previous prediction good. moment, can compare observation “house” data set, example, first one:prediction far SalePrice observation “house” data set. course, need add independent variables procedures, cover subsequent sections.","code":"\nhouse<-read.csv(\"data/house_clean.csv\")\n#house<-read.csv(\"https://raw.githubusercontent.com/abernal30/ml_book/main/housing.csv\")\n\nhouse_model<-lm(SalePrice~MSSubClass+MSZoning,data=house)\nsummary(house_model)\n#> \n#> Call:\n#> lm(formula = SalePrice ~ MSSubClass + MSZoning, data = house)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -169125  -58658  -26555   43751  532828 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 354837.1    30654.8  11.575  < 2e-16 ***\n#> MSSubClass    -233.4      102.0  -2.289   0.0224 *  \n#> MSZoning    -29665.2     7528.4  -3.940 9.12e-05 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 90790 on 581 degrees of freedom\n#> Multiple R-squared:  0.03625,    Adjusted R-squared:  0.03294 \n#> F-statistic: 10.93 on 2 and 581 DF,  p-value: 2.194e-05\npredict(house_model,house_test)\n#>        1 \n#> 231508.1\nhead(house[1,2:4])\n#>   SalePrice MSSubClass MSZoning\n#> 1    181500         20        4"},{"path":"training-and-evaluating-regression-models.html","id":"training-and-test-set-back-testing","chapter":"2 Training and evaluating regression models","heading":"2.1.1 Training and test set (Back testing)","text":"previous section, made one prediction compared given observation. machine learning literature common apply testing procedure several observations. book, call back-testing, divides data set training testing, often called in_sample out_sample. previous example, house_test must data frame. However, instead one observation two independent variables, contain many observations independent variables.answer need back-testing, think want validate prediction performance, least two alternatives:Alternative 1: Estimate ML model, make prediction wait time, example, 30 days, verify prediction good ; forecast good (close real value), train test , let’s say 30 days .Alternative 2 (one apply): Take aside observations, assuming observations don’t know store data frame called “test.” Train test ML model. making good prediction, train test model get good prediction performance.common practice divide data set training (80% observations) testing (20%). However, authors suggest splitting Training Set, Validation Set Test Set (Lantz 2019). latter case, proposal train model training set (example, 60% data), cross-validation (procedure cover ) validation set (example, 20% data), model good prediction performance, test test set (20% data).book, use methods. start chapter first one, cross-validation chapter, cover second one.housing prices data set, split randomly 80%, applying function sample.“set.seed” function helps us control results getting aleatory partition data; otherwise, get different result every time run R chunk. spirit book compare various methods methodologies without concern possible differences models processes result random partition.compare original house data set previous output, “rownames” (ID) different order selected randomly. Another important distinction observations house_test data set complement house_train. words, observations (IDs) training set aren’t test, vice versa.","code":"\nset.seed (26)\ndim<-dim(house)\ntrain_sample<-sample(dim[1],dim[1]*.8)\nhouse_train <- house[train_sample, ]\nhouse_test  <- house[-train_sample, ]\nhead(house_train[,1:5])\n#>      Id SalePrice MSSubClass MSZoning LotFrontage\n#> 64   64    163000         20        4          80\n#> 540 540    170000         80        4         102\n#> 200 200    155000         70        5          50\n#> 171 171    142000         20        4          65\n#> 41   41    250000         60        2          75\n#> 268 268    155000         60        4          70\nhead(house_test[,1:5])\n#>    Id SalePrice MSSubClass MSZoning LotFrontage\n#> 17 17    165500         20        4          70\n#> 20 20    153000         20        4          74\n#> 26 26    385000         20        4          68\n#> 31 31    317000         60        4          76\n#> 42 42    190000         20        4         105\n#> 43 43    383970         60        4          77"},{"path":"training-and-evaluating-regression-models.html","id":"performance-measure","chapter":"2 Training and evaluating regression models","heading":"2.1.2 Performance Measure","text":"","code":""},{"path":"training-and-evaluating-regression-models.html","id":"rmse","chapter":"2 Training and evaluating regression models","heading":"2.1.3 RMSE","text":"previous section, discussed whether prediction good (prediction performance). section formally defines metrics prediction performance.first metric Root Mean Square Error (RMSE). mathematical formula compute RMSE :\\[RMSE =\\sqrt{\\frac{1}{n}\\ \\sum_{=1}^{n} (y_{}-\\hat{y_{}})^{2}} \\]\\(\\hat{y_{}}\\) prediction ith observation, \\(y_{}\\) ith observation independent variable store test set, n number observations.\\[\\hat{y_{}}=\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n}\\]can estimate RMSE using training data set. generally, care well model works training set. Rather, interested model performance tested unseen data; , try RMSE test data set validation set cross-validation. lower test RMSE, better prediction.estimate RMSE housing example. time use variables data set. train model, use training data set. “lm” function requires adding dot symbol “~.”make prediction, use test data set.previous output, numbers prediction (17,20,26,31, etc.) row number test data frame model predicted. calling RMSE test test test data set. code estimate RMSE :ML’s main idea minimize RMSE test possible. subsequent sections, show accomplish .","code":"\nhouse_model<-lm(SalePrice~.,data=house_train)\nsummary(house_model)\n#> \n#> Call:\n#> lm(formula = SalePrice ~ ., data = house_train)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -311484  -19921   -1159   18474  219630 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    1.535e+06  3.384e+06   0.454  0.65040    \n#> Id            -1.208e+01  1.268e+01  -0.953  0.34119    \n#> MSSubClass    -2.860e+02  1.367e+02  -2.092  0.03706 *  \n#> MSZoning      -1.286e+03  4.747e+03  -0.271  0.78667    \n#> LotFrontage   -3.126e+02  1.021e+02  -3.061  0.00235 ** \n#> LotArea        5.531e-01  2.113e-01   2.618  0.00918 ** \n#> LotShape      -2.770e+03  1.680e+03  -1.649  0.09984 .  \n#> LotConfig     -1.144e+03  1.422e+03  -0.805  0.42134    \n#> Neighborhood   3.287e+02  4.110e+02   0.800  0.42439    \n#> Condition1    -1.679e+03  2.579e+03  -0.651  0.51547    \n#> BldgType       1.202e+03  4.281e+03   0.281  0.77909    \n#> HouseStyle    -2.336e+03  1.830e+03  -1.276  0.20267    \n#> OverallQual    1.797e+04  3.096e+03   5.806 1.28e-08 ***\n#> OverallCond    9.543e+03  2.960e+03   3.224  0.00136 ** \n#> YearRemodAdd  -1.525e+01  2.074e+02  -0.074  0.94142    \n#> RoofStyle      3.244e+03  2.784e+03   1.165  0.24465    \n#> Exterior1st   -1.142e+03  7.714e+02  -1.481  0.13937    \n#> MasVnrType     1.049e+03  3.500e+03   0.300  0.76450    \n#> MasVnrArea     5.071e+01  1.210e+01   4.189 3.42e-05 ***\n#> ExterQual     -8.256e+03  4.918e+03  -1.679  0.09400 .  \n#> ExterCond      6.708e+02  3.758e+03   0.178  0.85842    \n#> Foundation     7.901e+03  5.232e+03   1.510  0.13179    \n#> BsmtQual      -1.073e+04  3.331e+03  -3.222  0.00137 ** \n#> BsmtExposure  -5.919e+03  2.114e+03  -2.801  0.00534 ** \n#> BsmtFinType1  -3.214e+03  1.588e+03  -2.024  0.04361 *  \n#> BsmtFinSF1    -2.507e+01  9.435e+00  -2.657  0.00819 ** \n#> BsmtUnfSF     -1.827e+01  9.992e+00  -1.828  0.06825 .  \n#> HeatingQC     -6.688e+02  1.636e+03  -0.409  0.68287    \n#> CentralAir     4.803e+03  2.020e+04   0.238  0.81217    \n#> Electrical     7.323e+02  3.113e+03   0.235  0.81414    \n#> X1stFlrSF      3.447e+01  1.291e+01   2.671  0.00786 ** \n#> X2ndFlrSF      2.999e+01  1.101e+01   2.724  0.00672 ** \n#> BsmtFullBath   1.070e+04  6.443e+03   1.661  0.09749 .  \n#> BsmtHalfBath   4.813e+03  8.795e+03   0.547  0.58453    \n#> FullBath       1.772e+04  6.958e+03   2.547  0.01123 *  \n#> HalfBath       1.030e+04  6.666e+03   1.545  0.12303    \n#> BedroomAbvGr  -5.016e+03  4.631e+03  -1.083  0.27937    \n#> KitchenQual   -7.330e+03  3.546e+03  -2.067  0.03936 *  \n#> TotRmsAbvGrd   5.196e+03  2.754e+03   1.887  0.05987 .  \n#> Fireplaces     1.291e+04  6.274e+03   2.057  0.04029 *  \n#> FireplaceQu   -3.753e+03  2.213e+03  -1.696  0.09072 .  \n#> GarageType    -4.360e+02  1.880e+03  -0.232  0.81674    \n#> GarageYrBlt   -1.237e+02  1.902e+02  -0.650  0.51576    \n#> GarageFinish  -2.065e+03  3.550e+03  -0.582  0.56114    \n#> GarageArea     4.217e+01  1.718e+01   2.454  0.01453 *  \n#> PavedDrive     3.140e+03  8.320e+03   0.377  0.70602    \n#> WoodDeckSF     1.127e+01  1.993e+01   0.566  0.57191    \n#> OpenPorchSF   -1.189e+01  3.404e+01  -0.349  0.72699    \n#> MoSold        -4.744e+02  7.750e+02  -0.612  0.54080    \n#> YrSold        -6.036e+02  1.682e+03  -0.359  0.71984    \n#> SaleType      -1.475e+03  1.799e+03  -0.820  0.41280    \n#> SaleCondition  4.838e+03  2.319e+03   2.087  0.03752 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 44510 on 415 degrees of freedom\n#> Multiple R-squared:  0.7822, Adjusted R-squared:  0.7554 \n#> F-statistic: 29.22 on 51 and 415 DF,  p-value: < 2.2e-16\nhouse_predict<-predict(house_model,house_test)\nhead(house_predict)\n#>       17       20       26       31       42       43 \n#> 152651.6 135846.9 321903.0 312590.4 219878.9 331208.6\nchp3_swr<-sqrt(mean((house_test[,\"SalePrice\"]-house_predict)^2 ,na.rm = T))\nchp3_swr\n#> [1] 47889.11"},{"path":"training-and-evaluating-regression-models.html","id":"mean-absolute-error-mae","chapter":"2 Training and evaluating regression models","heading":"2.1.4 Mean absolute error (MAE)","text":"RMSE generally preferred performance measure regression models. However, measuring model performance MAE useful data outliers.\\[MAE =\\frac{1}{n}\\ \\sum_{=1}^{n} |y_{}-\\hat{y_{}}|\\]\nexample, Mean absolute error test (MAE) :","code":"\nmean(abs(house_test[,\"SalePrice\"]-house_predict),na.rm = T)\n#> [1] 28038.65"},{"path":"training-and-evaluating-regression-models.html","id":"r-squared","chapter":"2 Training and evaluating regression models","heading":"2.1.5 R-squared","text":"called Goodness--Fit measure. define R-squared follows:\n\\[R^2=SSE/SST\\]interpret fraction sample variation dependent variable \\(y\\), explained independent variable \\(X\\). \\(SST\\) total sum squares,\\[SST=\\frac{1}{n}\\ \\sum_{=1}^{n}(y_{}-\\overline{y})^{2},\\]\\(\\overline{y}\\) sample average \\(y_{}\\). indicator SST measures total sample variation \\(y_{}\\). measure SSE explained sum squares,\\[SSE=\\frac{1}{n}\\ \\sum_{=1}^{n}(\\hat{y_{}}-\\overline{y})^{2}.\\]\nSSE measures sample variation \\(\\hat{y_{}}\\) (Wooldridge 2020).","code":""},{"path":"training-and-evaluating-regression-models.html","id":"model-selection","chapter":"2 Training and evaluating regression models","heading":"2.1.6 Model Selection","text":"can improve RMSE several ways. first one applying ML methodologies. section starts subset selection, Regularization dimension Reduction.subset selection consists independent variables selection among available independent variables, helps improve model’s predictability performance. housing example 51 independent variables, including ID. must careful choosing among variables. see define train_RMSE:\\[train\\_RMSE =\\sqrt{\\frac{1}{n}\\ \\sum_{=1}^{n} (y_{}-\\hat{y_{}})^{2}}=\\sqrt{\\frac{1}{n}\\ train\\_RSS}\\]:\\[train\\_RSS = \\sum_{=1}^{n} (y_{}-\\hat{y_{}})^{2}=e_{1}^2+e_{2}^2+,..+e_{n}^2 \\]\n\\(train\\_RSS\\) explained sum squares residual sum squares training data set. consequence, RMSE decrease \\(train\\_RSS\\) decrease. According (James et al. 2017), \\(train\\_RSS\\) decrease number independent variables included models increases, even variables unrelated independent variable (significant). Therefore, use statistics select best model training data set, always end model involving variables. problem low \\(RSS\\) indicates model low training error, whereas wish choose model low test error (James et al. 2017). problem overfitting data, term explain (Suzuky 2022).solve , indicators looks minimize RSS penalize inclusion independent variables, example, Akaike Information Criterion AIC:solve problem, use indicators aim minimize RSS, penalizing inclusion independent variables, example, Akaike Information Criterion AIC:\\[AIC=\\frac{1}{n \\hat{\\sigma^2}}(RSS+2d\\hat{\\sigma^2})\\]\\(\\hat{\\sigma^2}\\) estimate variance error term, \\(d\\) number predictors, \\(n\\) number observations. AIC adds penalty \\(2d\\hat{\\sigma^2}\\) training RSS adjust training error tends underestimate test error. penalty increases number predictors model increases; intended adjust corresponding decrease training RSS (statistical_learning?). decision criteria model lower AIC.use function step, chooses variables according AIC.trace=F argument printing models algorithm evaluates. direction=“,” selection methods. Forward selection begins model containing independent variables adds predictors model one time predictors model; finally selects model (combination variables) lowest AIC. important notice algorithm selected among possible combinations. example, case 51 vaiiravles, imply \\(2^{51}=2.2518e+15\\). instead, evaluate \\(RSS = \\sum_{k=o}^{p-1} (p-k)=1+p(p+1)/2=1+50(50+1)/2=1327\\) models (statistical_learning?).hand, backward begins full least squares model containing \\(p\\) predictors interactively removes least useful predictor, one time. “” argument implies applies procedures, forward backward.exposition purposes, print AIC criterion two models, one 51 variables one variable selection based AIC.expected, model using step function lowest AIC. Also, estimate RMSE test data set step model, get:lower one full variables model.","code":"\nstep_house<-step(house_model,house_train,trace = F,direction=\"both\")\nsummary(step_house)\n#> \n#> Call:\n#> lm(formula = SalePrice ~ MSSubClass + LotFrontage + LotArea + \n#>     LotShape + HouseStyle + OverallQual + OverallCond + Exterior1st + \n#>     MasVnrArea + ExterQual + Foundation + BsmtQual + BsmtExposure + \n#>     BsmtFinType1 + BsmtFinSF1 + BsmtUnfSF + X1stFlrSF + X2ndFlrSF + \n#>     BsmtFullBath + FullBath + HalfBath + KitchenQual + TotRmsAbvGrd + \n#>     Fireplaces + FireplaceQu + GarageArea + SaleCondition, data = house_train)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -320194  -19377   -1434   17833  220366 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    4.120e+04  4.318e+04   0.954 0.340604    \n#> MSSubClass    -2.299e+02  6.787e+01  -3.388 0.000768 ***\n#> LotFrontage   -3.216e+02  9.392e+01  -3.424 0.000674 ***\n#> LotArea        5.828e-01  2.036e-01   2.862 0.004411 ** \n#> LotShape      -2.812e+03  1.537e+03  -1.829 0.068033 .  \n#> HouseStyle    -2.889e+03  1.553e+03  -1.860 0.063500 .  \n#> OverallQual    1.893e+04  2.922e+03   6.478 2.49e-10 ***\n#> OverallCond    9.620e+03  2.386e+03   4.032 6.53e-05 ***\n#> Exterior1st   -1.302e+03  7.056e+02  -1.845 0.065677 .  \n#> MasVnrArea     5.605e+01  1.050e+01   5.338 1.51e-07 ***\n#> ExterQual     -8.628e+03  4.554e+03  -1.894 0.058823 .  \n#> Foundation     8.200e+03  4.347e+03   1.886 0.059902 .  \n#> BsmtQual      -1.178e+04  3.121e+03  -3.774 0.000182 ***\n#> BsmtExposure  -5.687e+03  2.000e+03  -2.844 0.004665 ** \n#> BsmtFinType1  -3.146e+03  1.507e+03  -2.088 0.037393 *  \n#> BsmtFinSF1    -2.498e+01  8.965e+00  -2.786 0.005571 ** \n#> BsmtUnfSF     -2.111e+01  9.549e+00  -2.211 0.027582 *  \n#> X1stFlrSF      3.817e+01  1.159e+01   3.292 0.001074 ** \n#> X2ndFlrSF      2.565e+01  9.388e+00   2.732 0.006540 ** \n#> BsmtFullBath   9.998e+03  5.832e+03   1.714 0.087191 .  \n#> FullBath       1.689e+04  5.966e+03   2.831 0.004847 ** \n#> HalfBath       1.139e+04  6.012e+03   1.895 0.058711 .  \n#> KitchenQual   -7.131e+03  3.345e+03  -2.132 0.033563 *  \n#> TotRmsAbvGrd   4.149e+03  2.422e+03   1.713 0.087404 .  \n#> Fireplaces     1.359e+04  6.017e+03   2.259 0.024362 *  \n#> FireplaceQu   -4.135e+03  2.073e+03  -1.995 0.046680 *  \n#> GarageArea     4.338e+01  1.484e+01   2.923 0.003649 ** \n#> SaleCondition  4.348e+03  2.211e+03   1.967 0.049854 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 43750 on 439 degrees of freedom\n#> Multiple R-squared:  0.7774, Adjusted R-squared:  0.7637 \n#> F-statistic: 56.79 on 27 and 439 DF,  p-value: < 2.2e-16\nprint(extractAIC(step_house)[2])\n#> [1] 10008.02\nprint(extractAIC(house_model)[2])\n#> [1] 10045.9\nstep_house_predict<-predict(step_house,house_test)\nmean(abs(house_test[,\"SalePrice\"]-step_house_predict),na.rm = T)\n#> [1] 28117.64\nsqrt(mean((house_test[,\"SalePrice\"]-house_predict)^2 ,na.rm = T))\n#> [1] 47889.11"},{"path":"training-and-evaluating-regression-models.html","id":"overfittingunderfitting","chapter":"2 Training and evaluating regression models","heading":"2.1.7 Overfitting/Underfitting","text":"Overfitting occurs ML model trained learn noise (.e., non-representative data result chance) rather patterns trends data. following text (James et al. 2017):“general rule, use flexible methods, variance increase bias decrease. case, increase flexibility, bias tends initially decrease faster variance increases.Consequently, expected test MSE declines. However, point increasing flexibility little impact bias starts significantly increase variance.happens test MSE increases” .TTo explain better, use MSE decomposition. expected MSE decomposed following.\\[E(MSE) =E(\\frac{1}{n}\\ \\sum_{=1}^{n} (y-\\hat{y})^2)= E(\\epsilon^2)+E ((y-\\hat{y})^2)=Var(e)+Var(\\hat{y})+[Bias(\\hat{y})]^2\\]\n\\[Bias(\\hat{y})=y-E(\\hat{y})\\]\\[Var(\\hat{y})=E((y-E(\\hat{y}))^2)\\]See proof Appendix chapter.idea evaluating methodologies based notion Bias-Variance Trade-:\\(Var[e]\\) variance error term, call irreducible part.\\(E (y_{0}-\\hat{y})^{2}\\) average MSE test—————esto ———Models frequently overfit small number training samples relative flexibility complexity model. model considered high variance low bias.supervised model overfit typically perform well data model trained , perform poorly data model seen (Krishnan 2020).Underfitting occurs machine learning model capture variations data – variations data caused noise. model considered high bias, low variance.\nsupervised model underfit typically perform poorly data model trained , data model seen . Examples overfitting, underfitting, good balanced model (Krishnan 2020).","code":"\nset.seed (26)\ndim<-dim(house)\ntrain_sample<-sample(dim[1],dim[1]*.8)\nhouse_train <- house[train_sample, ]\nhouse_test  <- house[-train_sample, ]\n#indes<-\"OverallQual\"\n#indes<-\"YearRemodAdd\"\n#indes<-\"X2ndFlrSF\"\nindes<-\"GarageArea\"\nform<-as.formula(paste(\"SalePrice\", \"~\", indes))\nmod<-lm(form, data = house_train)\n\nmod2<-lm(form, data = house_test)\nplot(form, data = ,house_test,ylim=c(91000,350000),xlim=c(200,900))\nabline(mod)\nabline(mod2)\n\n#plot(form, data = ,house_train)\nmod<-lm(form, data = house_train)\n#abline(mod)\nmod2<-lm(form, data = house_test)\nplot(form, data = ,house_test)\nabline(mod)\nabline(mod2)\n#indes<-\"OverallQual\"\n#indes<-\"YearRemodAdd\"\nindes<-\"X2ndFlrSF\"\n#indes<-\"GarageArea\"\nplot(house_train[,\"SalePrice\"],house_train[,indes])\nabline(a = 500, b = 1)\nplot(house_test[,\"SalePrice\"],house_test[,indes])\nh_vars<-c(\"SalePrice\",colnames(house_train[45:49]))\n\npanel.cor <- function(x, y, digits = 2, prefix = \"\", cex.cor, ...)\n{\n    usr <- par(\"usr\"); on.exit(par(usr))\n    par(usr = c(0, 1, 0, 1))\n    r <- abs(cor(x, y,use=\"pairwise.complete.obs\"))\n    txt <- format(c(r, 0.123456789), digits = digits)[1]\n    txt <- paste0(prefix, txt)\n    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)\n    text(0.5, 0.5, txt, cex = cex.cor * r)\n}\npairs(na.omit(house_train[,h_vars]),upper.panel=panel.cor)\n#knitr::include_graphics('images/overfit.jpg')"},{"path":"training-and-evaluating-regression-models.html","id":"regularization-models","chapter":"2 Training and evaluating regression models","heading":"2.1.8 Regularization models","text":"Regularization method balance overfitting underfitting model training. overfitting\nunderfitting problems ultimately cause poor predictions new data.Regularization technique adjust closely model trained fit historical data. One way apply\nregularization adding parameter penalizes loss function tuned model overfit.allows use regularization parameter affects closely model trained fit historical\ndata. regularization prevents overfitting, less regularization prevents underfitting. Balancing \nregularization parameter helps find good tradeoff bias variance.","code":"\nsummary(house[,\"SalePrice\"])\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   62383  159988  193690  222864  266625  755000"},{"path":"training-and-evaluating-regression-models.html","id":"appendix","chapter":"2 Training and evaluating regression models","heading":"2.2 Appendix","text":"","code":""},{"path":"training-and-evaluating-regression-models.html","id":"proof-of-emse-varepsilonvarhatybiashaty2","chapter":"2 Training and evaluating regression models","heading":"2.2.1 Proof of \\(E(MSE) =Var(\\epsilon)+Var(\\hat{y})+[Bias(\\hat{y})]^2\\)","text":"\\[E ((y-\\hat{y})^2)=E((y-E(\\hat{y})+E(\\hat{y})-\\hat{y})^2)=(y-E(\\hat{y}))^2+E((E(\\hat{y})-y)^2)+2E(y-E(\\hat{y}))\\ E(E(\\hat{y})-\\hat{y})\\]\\[2E(y-E(\\hat{y}))\\ E(E(\\hat{y})-\\hat{y})=2E(y-E(\\hat{y}))\\ (E(\\hat{y})-E(\\hat{y}))=0\\]\\[2E(y-E(\\hat{y}))\\ E(E(\\hat{y})-\\hat{y})=2E(y-E(\\hat{y}))\\ (E(\\hat{y})-E(\\hat{y}))=0\\]","code":""},{"path":"training-and-evaluating-classification-models.html","id":"training-and-evaluating-classification-models","chapter":"3 Training and evaluating classification models","heading":"3 Training and evaluating classification models","text":"linear regression, OLS, dependent variable continuous, SalePrice variable, sense house price $62,383 $755000. However, situations dependent variable two categories: women men, finance, signal buy sell, etc. case, call variables categorical. One popular classification models logistic regression linear discriminant analysis.","code":""},{"path":"training-and-evaluating-classification-models.html","id":"logit-model","chapter":"3 Training and evaluating classification models","heading":"3.1 Logit model","text":"section work credit data set. Remember describe credit data set chapter one, define model:\\[ Default=\\beta_{0}+\\beta_{1}\\ x_{1} + \\beta_{2}\\ x_{2}+ ....+\\beta_{n} x_{n}+\\epsilon \\]algorithm procedures runs binary response models, logit, first transforms categories, case “Charged ” “Fully Paid”, numerical values, zero one. assigns probability \\(y\\) takes value one \\(P(y=1)= \\pi\\) takes value zero \\(P(y=0)= 1-\\pi\\). statistical analysis aims investigate relationship probability \\(\\pi(X)\\) independent variables \\(X=x_{1}, x_{2}...x_{n}\\) . convenient construct model capable describing effect \\(pi\\) changes \\(X=x_{1}, x_{2}...x_{n}\\), form function \\(g(\\pi)\\) (McCullagh FRS 1989).logic model function \\(g(\\pi)=\\log{(\\pi/(1-\\pi))}\\)\\[g(\\pi)=\\log{(\\pi/(1-\\pi))}=beta_{0}+\\beta_{1}x_{1}+,..,+\\beta_{n}x_{n} \\]Taking exponential sides, get:\\[\\pi/(1-\\pi)= \\exp{(beta_{0}+\\beta_{1}x_{1}+,..,+\\beta_{n}x_{n})}\\]get probability, need transformation like :\\[\\pi= \\frac{\\exp{(beta_{0}+\\beta_{1}x_{1}+,..,+\\beta_{n}x_{n})}}{1+\\exp{(beta_{0}+\\beta_{1}x_{1}+,..,+\\beta_{n}x_{n})}}\\]variable default categorical variable becasue takes folowing two values:function run logistic model R doesn´t accept character values, numeric factor. transform variable “factor.”split training test data sets, housing example.function run logistic model “glm”.paragraph , explain warning red: “glm.fit: fitted probabilities numerically 0 1 occurred”.use predict function make prediction.parameters estimate “glm” function, adding family=binomial() type= “response”, parameters equation.\\[\\pi= \\frac{\\exp{(beta_{0}+\\beta_{1}x_{1}+,..,+\\beta_{n}x_{n})}}{1+\\exp{(beta_{0}+\\beta_{1}x_{1}+,..,+\\beta_{n}x_{n})}}\\]Consequently, make prediction, get probability.However, expected prediction either Charged Fully Paid, prediction numbers cero one.Finally, must still resolve whether prediction Charged Fully Paid. transform following code.transform forecast category, establish threshold 0.5; prediction higher 0.5, convert “Fully Paid” otherwise “Charged .” “ok” wonder threshold 0.5. estimating prediction accuracy model, change threshold improve prediction performance.suggest verifying comparing resulting structure original data structure. case, “Charged ” 82 % observations (144 / (144+31)).complete data set (train + test), percentage 83%, consistent result. inconsistent result 87% prediction.","code":"\ncredit<-read.csv(\"https://raw.githubusercontent.com/abernal30/ml_book/main/credit.csv\")\n\nc2<-ifelse(credit[,\"Default\"]==\"Charged Off\" ,\"No_default\",\"default\") \ncredit[,\"Default\"]<-c2\ntable(credit[,\"Default\"])\n#> \n#>    default No_default \n#>        728        145\ncredit[,\"Default\"]<-factor(credit[,\"Default\"])\n\nset.seed (43)\ndim<-dim(credit)\ntrain_sample<-sample(dim[1],dim[1]*.8)\ncredit_train <- credit[train_sample, ]\ncredit_test  <- credit[-train_sample, ]\ncredit_model<-glm(Default ~ .,data= credit_train ,family=binomial())\nsummary(credit_model)\n#> \n#> Call:\n#> glm(formula = Default ~ ., family = binomial(), data = credit_train)\n#> \n#> Deviance Residuals: \n#>        Min          1Q      Median          3Q         Max  \n#> -8.169e-05  -2.100e-08  -2.100e-08  -2.100e-08   6.626e-05  \n#> \n#> Coefficients:\n#>                         Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)            2.897e+02  2.242e+06   0.000    1.000\n#> term                   8.976e+01  6.971e+04   0.001    0.999\n#> installment            1.223e-02  2.300e+02   0.000    1.000\n#> grade                 -1.242e+01  3.695e+04   0.000    1.000\n#> emp_title             -1.979e-02  2.551e+02   0.000    1.000\n#> emp_length            -8.469e-01  2.485e+04   0.000    1.000\n#> home_ownership        -9.860e+00  2.016e+04   0.000    1.000\n#> annual_inc             9.569e-05  3.390e+00   0.000    1.000\n#> verification_status   -4.514e+00  3.047e+04   0.000    1.000\n#> purpose                4.467e+00  2.071e+04   0.000    1.000\n#> title                 -1.085e+00  2.569e+04   0.000    1.000\n#> zip_code               2.512e-02  8.173e+02   0.000    1.000\n#> addr_state            -2.288e-01  3.921e+03   0.000    1.000\n#> dti                    1.426e+00  8.380e+03   0.000    1.000\n#> delinq_2yrs            3.129e+00  2.009e+04   0.000    1.000\n#> earliest_cr_line      -3.055e-02  3.341e+02   0.000    1.000\n#> fico_range_high       -5.329e-02  3.646e+03   0.000    1.000\n#> inq_last_6mths         2.822e+00  4.734e+04   0.000    1.000\n#> pub_rec                6.300e-01  1.310e+05   0.000    1.000\n#> revol_bal             -1.205e-03  4.719e+00   0.000    1.000\n#> revol_util            -8.785e-02  8.591e+03   0.000    1.000\n#> total_acc              1.151e+01  5.819e+04   0.000    1.000\n#> total_rec_int         -4.211e-03  1.308e+01   0.000    1.000\n#> recoveries             1.110e-01  2.909e+02   0.000    1.000\n#> last_pymnt_d           7.074e-01  2.275e+03   0.000    1.000\n#> last_pymnt_amnt       -6.044e-03  1.338e+01   0.000    1.000\n#> last_credit_pull_d     7.841e-01  4.390e+03   0.000    1.000\n#> last_fico_range_high  -6.891e-01  4.561e+02  -0.002    0.999\n#> last_fico_range_low    4.923e-03  2.849e+02   0.000    1.000\n#> tot_coll_amt           1.224e-03  2.466e+00   0.000    1.000\n#> tot_cur_bal            2.562e-05  3.973e-01   0.000    1.000\n#> open_acc_6m            1.177e+01  6.376e+04   0.000    1.000\n#> open_act_il            6.880e+00  3.555e+04   0.000    1.000\n#> open_il_12m           -9.614e+00  1.184e+05   0.000    1.000\n#> open_il_24m            6.398e+00  4.427e+04   0.000    1.000\n#> mths_since_rcnt_il     3.689e-01  1.095e+03   0.000    1.000\n#> total_bal_il           1.374e-04  2.479e+00   0.000    1.000\n#> il_util                3.744e-01  3.611e+03   0.000    1.000\n#> open_rv_12m           -1.147e+01  7.695e+04   0.000    1.000\n#> open_rv_24m            1.252e+01  4.016e+04   0.000    1.000\n#> max_bal_bc             5.577e-04  1.057e+01   0.000    1.000\n#> all_util               3.889e-01  4.471e+03   0.000    1.000\n#> total_rev_hi_lim       3.081e-04  3.123e+00   0.000    1.000\n#> inq_fi                -1.414e-02  1.968e+04   0.000    1.000\n#> total_cu_tl            3.728e+00  9.716e+03   0.000    1.000\n#> inq_last_12m           8.275e-01  3.789e+04   0.000    1.000\n#> acc_open_past_24mths  -1.146e+01  4.153e+04   0.000    1.000\n#> avg_cur_bal           -1.149e-04  1.849e+00   0.000    1.000\n#> bc_open_to_buy        -5.062e-04  1.554e+01   0.000    1.000\n#> bc_util               -5.402e-02  1.391e+03   0.000    1.000\n#> mo_sin_old_il_acct    -5.365e-02  4.233e+02   0.000    1.000\n#> mo_sin_old_rev_tl_op   6.397e-03  8.812e+02   0.000    1.000\n#> mo_sin_rcnt_rev_tl_op  2.013e-01  3.696e+03   0.000    1.000\n#> mo_sin_rcnt_tl         3.205e-02  2.013e+03   0.000    1.000\n#> mort_acc              -1.140e+01  7.505e+04   0.000    1.000\n#> mths_since_recent_bc   8.572e-03  3.642e+03   0.000    1.000\n#> mths_since_recent_inq -1.035e+00  4.052e+03   0.000    1.000\n#> num_accts_ever_120_pd -4.406e+00  2.104e+04   0.000    1.000\n#> num_actv_bc_tl         2.636e+00  7.399e+04   0.000    1.000\n#> num_bc_sats           -4.123e+00  8.698e+04   0.000    1.000\n#> num_bc_tl              3.302e+00  4.186e+04   0.000    1.000\n#> num_il_tl             -1.397e+01  6.543e+04   0.000    1.000\n#> num_op_rev_tl          6.232e+00  6.346e+04   0.000    1.000\n#> num_rev_accts         -1.211e+01  4.019e+04   0.000    1.000\n#> num_rev_tl_bal_gt_0    8.575e-01  6.424e+04   0.000    1.000\n#> num_sats              -5.882e+00  3.720e+04   0.000    1.000\n#> num_tl_op_past_12m     5.371e+00  9.234e+04   0.000    1.000\n#> pct_tl_nvr_dlq         1.125e-01  3.589e+03   0.000    1.000\n#> percent_bc_gt_75      -4.595e-02  3.134e+03   0.000    1.000\n#> pub_rec_bankruptcies  -7.226e+00  1.451e+05   0.000    1.000\n#> total_bc_limit         1.527e-04  1.590e+01   0.000    1.000\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 6.3112e+02  on 697  degrees of freedom\n#> Residual deviance: 8.7225e-08  on 627  degrees of freedom\n#> AIC: 142\n#> \n#> Number of Fisher Scoring iterations: 25\ncredit_predict<-predict(credit_model, newdata=credit_test, type=\"response\")\nhead(credit_predict)\n#>           15           19           24           26           32           34 \n#> 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16\nsummary(credit_predict)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.0000  0.0000  0.0000  0.1754  0.0000  1.0000\ntable(credit[,\"Default\"])\n#> \n#>    default No_default \n#>        728        145\ncredit_predict_char<-ifelse(credit_predict>.5,\"No_default\",\"default\")\ntable(credit_predict_char)\n#> credit_predict_char\n#>    default No_default \n#>        144         31"},{"path":"training-and-evaluating-classification-models.html","id":"performance-measure-in-clasification","chapter":"3 Training and evaluating classification models","heading":"3.2 Performance Measure in clasification","text":"Now measure prediction accuracy, applying confusion Matrix:Confusion matrixIt table categorizes predictions according whether match actual value. One table’s dimensions indicates possible categories predicted values, shows actual values. Although seen 2 x 2 confusion matrices far, matrix can created models predict number class values. following figure shows generic confusion matrix.\nFigure 3.1: Confusion matrix.\nTrue Positive (TP): Correctly classified class interest. True Negative (TN) Correctly classified class interest. False Positive (FP) Incorrectly classified class interest. False Negative (FN): Incorrectly classified class interest.use confusionMatrix function library “caret” credit analysis example. first argument prediction, second argument reference, case, variable “Default” credit_test data set; variables must factors, true variable inside credit_test.review consistency data structure. case, consistent. important keep consistency levels; case, write first “Charged ” reference data set starts label first:Finally, apply function. case, assuming positive class, class interest, “Charged ”:several indicators output, “confusing.” analyze parts. start table:matrix’s left-superior/right-inferior numbers true positive/true negative number observations prediction equals reference. left-inferior/ right-superior numbers matrix true positive/ true negative number observations prediction “” equal reference.numbers get following metrics.accuracy :\\[ accuracy =\\frac{TP+TN}{TP+TN+FP+FN}\\]","code":"\nlibrary(\"caret\")\nDefaultf<-factor(credit_predict_char,levels=c(\"No_default\",\"default\"))\ntable(Defaultf)\n#> Defaultf\n#> No_default    default \n#>         31        144\nhead(credit_test[,\"Default\"])\n#> [1] default default default default default default\n#> Levels: default No_default\nconfu<-confusionMatrix(Defaultf,credit_test[,\"Default\"],positive=\"default\")\nconfu\n#> Confusion Matrix and Statistics\n#> \n#>             Reference\n#> Prediction   default No_default\n#>   default        141          3\n#>   No_default       6         25\n#>                                           \n#>                Accuracy : 0.9486          \n#>                  95% CI : (0.9046, 0.9762)\n#>     No Information Rate : 0.84            \n#>     P-Value [Acc > NIR] : 8.743e-06       \n#>                                           \n#>                   Kappa : 0.8166          \n#>                                           \n#>  Mcnemar's Test P-Value : 0.505           \n#>                                           \n#>             Sensitivity : 0.9592          \n#>             Specificity : 0.8929          \n#>          Pos Pred Value : 0.9792          \n#>          Neg Pred Value : 0.8065          \n#>              Prevalence : 0.8400          \n#>          Detection Rate : 0.8057          \n#>    Detection Prevalence : 0.8229          \n#>       Balanced Accuracy : 0.9260          \n#>                                           \n#>        'Positive' Class : default         \n#> \nconfu$table\n#>             Reference\n#> Prediction   default No_default\n#>   default        141          3\n#>   No_default       6         25\nconfu$table[1]+confu$table[2]\n#> [1] 147\nconfu$overall[1]\n#>  Accuracy \n#> 0.9485714"},{"path":"training-and-evaluating-classification-models.html","id":"the-accuracy","chapter":"3 Training and evaluating classification models","heading":"3.2.1 The accuracy:","text":"proportion true positives true negatives, divided total number predictions. beginning tempting tuning model increase Accuracy, increase true predictions default default, fin. However, machine larning clasficiation theory suggest considering measures.therms confusion matrix, sum green squares toal number observations.\nFigure 3.2: Confusion matrix.\nexplain may need measures, example, imagine scenario company concern default rates, increasing. first thought correct , tuning model predict almost always new customer going pay, “Charge ”. consequence, model discriminating bad new customers, probably ´t pay loan, also good ones. problem bank, ot may end business, bank ´t make loans, winch negative effect profits.contrary, tuning model predict almost always new customer going pay, cause granting loans customers probably sould pay, also negative profits .","code":""},{"path":"training-and-evaluating-classification-models.html","id":"the-sensitivity-or-recall-also-called-the-true-positive-rate","chapter":"3 Training and evaluating classification models","heading":"3.2.2 The sensitivity or Recall (also called the true positive rate)","text":"Measures well predicting variable interest regarding total positive observations reference:\\[ Sensitivity\\ \\ Recall =\\frac{TP}{TP+FN}\\]\nFigure 3.3: Confusion matrix.\nimportance measure relative important analysis. example, imagine scenario company concern default rates, increasing. Mabe first tougth correct , restrictive model toIn case, may problematic predicting someone going pay credit ( “Fully Paid”) reality going pay (“Charged ”) predicting someone going pay credit (“Charged ”) reality pay ( “Fully Paid”). case, sensitivity good indicator.sensitivity 95.92% may high expected, example, follow 95% higher rule. hand, scenario company low default rate expecting expand business. problematic predict someone going pay credit (“Charged ”) reality, pay ( “Fully Paid”).case, class interest may “Fully Paid.”Sensitivity ´s good measure, wee need complement ones, issues, even high, 100%, ´n necessarily mean good. example suppose unlikely case predictions positive ones, every person going pay loan, let´s call noisy model:sensitivity 100%, something wrong, among things rejecting credit application, whic problem ending business negative effect profits. prevent , something similar, metrics Precision.","code":"\nconfu$byClass[1]\n#> Sensitivity \n#>   0.9591837\npre_pos<-rep(\"default\",length(Defaultf))\npre_pos_f<-factor(pre_pos,levels=c(\"default\",\"No_default\"))\n\nnoisy<-confusionMatrix(pre_pos_f,credit_test[,\"Default\"],positive=\"default\")\nnoisy\n#> Confusion Matrix and Statistics\n#> \n#>             Reference\n#> Prediction   default No_default\n#>   default        147         28\n#>   No_default       0          0\n#>                                          \n#>                Accuracy : 0.84           \n#>                  95% CI : (0.7771, 0.891)\n#>     No Information Rate : 0.84           \n#>     P-Value [Acc > NIR] : 0.5502         \n#>                                          \n#>                   Kappa : 0              \n#>                                          \n#>  Mcnemar's Test P-Value : 3.352e-07      \n#>                                          \n#>             Sensitivity : 1.00           \n#>             Specificity : 0.00           \n#>          Pos Pred Value : 0.84           \n#>          Neg Pred Value :  NaN           \n#>              Prevalence : 0.84           \n#>          Detection Rate : 0.84           \n#>    Detection Prevalence : 1.00           \n#>       Balanced Accuracy : 0.50           \n#>                                          \n#>        'Positive' Class : default        \n#> "},{"path":"training-and-evaluating-classification-models.html","id":"precision-or-positive-predicted-values","chapter":"3 Training and evaluating classification models","heading":"3.2.3 Precision or Positive Predicted Values","text":"Measures well predicting variable interest regarding total positive observations prediction:\\[ Precision =\\frac{TP}{TP+FP}\\]proportion true positive predictions positive predictions.Sensitivity Precision similar measure proportion true positives, , former total positive values reference latter total positive values prediction.example, precision penalize model like noisy one, 0.84. intended indicate interesting relevant model’s results , whether predictions diluted meaningless noise (Lantz 2019).Precision original model :higher Precision model also something good . example, model predicted positive predictions, like 2, chance two well classified, TP 2, FP zero, Precision 100%. reference, 28 positive observations total, like example, mean short number positive observations predicting. term bank-business, imply model able identify large portion customers default credit, side problem negative profits.need balance sensitivity (Recall) precision.\ngood classification model may achieve 100% precision 100% sensitivity, reality unlikely happens. Models usually trade precision recall; typically higher precision, lower recall, vicevers. noisy model precision 0.84% sensitivity 95.92% (Krishnan 2020).","code":"\npre<-round(confu$byClass[3],4)\npre\n#> Pos Pred Value \n#>         0.9792#> [1] 0.9792"},{"path":"training-and-evaluating-classification-models.html","id":"the-f1-score","chapter":"3 Training and evaluating classification models","heading":"3.2.4 The F1 Score","text":"metric measure balanced relationship sensitivity (Recall) precision. formula called harmonic mean.\nFigure 3.4: Confusion matrix.\nmodel high F1 score precision recall high. However, model low F1 score one factor low, even 100 percent (Krishnan 2020). example, F1 Score :","code":"\nround(2*(sen*pre_v)/(sen+pre_v),4)\n#> [1] 1.9386"},{"path":"training-and-evaluating-classification-models.html","id":"the-specificity-true-negative-rate-measures-the-proportion-of-negatives-that-were-correctly-classified","chapter":"3 Training and evaluating classification models","heading":"3.2.5 The specificity (true negative rate) measures the proportion of negatives that were correctly classified:","text":"\\[ specificity =\\frac{TN}{TN+FP}\\]latter implies 95.92% time, predicting well class “Fully Paid.","code":"\nconfu$byClass[2]\n#> Specificity \n#>   0.8928571"},{"path":"training-and-evaluating-classification-models.html","id":"kappa","chapter":"3 Training and evaluating classification models","heading":"3.2.6 kappa","text":"Adjusts accuracy taking account class imbalance, refers trouble associated data large majority records belonging single class. kappa statistic adjust data sets severe class imbalance classifier can obtain high accuracy simply always guessing frequent class (Lantz 2019). call agreement actual proportions classes.\\[\\kappa=\\frac{Pr()-Pr(e)}{1-Pr(e)} \\]\nPr() proportion actual agreement Pr(e) refers expected agreement classifier true values, assumption chosen random","code":"Poor agreement = less than 0.20\nFair agreement = 0.20 to 0.40\nModerate agreement = 0.40 to 0.60\nGood agreement = 0.60 to 0.80\nVery good agreement = 0.80 to 1.00"},{"path":"training-and-evaluating-classification-models.html","id":"the-roc-curve","chapter":"3 Training and evaluating classification models","heading":"3.2.7 The ROC Curve","text":"receiver operating characteristic (ROC) looks similar balance, Precision/Recall, Recall false positive rate (FPR). focus total\\[false\\_positive\\_rate (FPR)=(1-Specificity)= \\frac{FP}{FP+TN}\\]therms confusion matrix, blue vector grey one. proportion TP, well positive predicted, positive total reference proportion FP, bad positive predicted, negative total reference.\nFigure 3.5: Confusion matrix.\nuse function “roc” “pROC” library (Robin et al. 2011).\nfirst argument reference (“response”) second one prediction(“predictor”), probablity.Also need add levels:apply function:reults, message direction comparing group median higher take direction accordingly. used generates Sensitivities specificity’s given probabilities (Thresholds). Remember previous chunk trasnform prediction, porbability, labels c(“Fully Paid”,“Charged ”), used code like :ifelse(credit_predict>.5,“Fully Paid”,“Charged ”). example, arbitrarily selected Thresholds 0.5. function generates several thresholds considering means two consecutive values observed data. example generated 17 thresholds, estimate 17 sensitibities `specificity’s.print.thres.pattern=“thresholds: %.3f : %.3f : %.3f”)Best = “T” argument controls optimal threshold determined.“youden” Youden’s J statistic (Youden, 1950) employed. optimal cut-threshold maximizes distance identity (diagonal) line. Can shortened “y”. optimality criterion :\\[max(sensitivities + specificities)\\]“closest.topleft” optimal threshold point closest top-left part plot perfect sensitivity specificity. Can shortened “c” “t”.\noptimality criterion (Robin et al. 2011):\\[min((1-sensitivities)^2 + (1-specificities)^2)\\]","code":"\nlibrary(pROC)\nref<- credit_test[,\"Default\"]\npredict_glm<-predict(credit_model, newdata=credit_test, type=\"response\")\nhead(predict_glm)\n#>           15           19           24           26           32           34 \n#> 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16\nlevels(ref)\n#> [1] \"default\"    \"No_default\"\nroc0<-roc(ref, predict_glm, levels = rev(levels(ref)),ret=\"coords\") \nroc0\n#> \n#> Call:\n#> roc.default(response = ref, predictor = predict_glm, levels = rev(levels(ref)),     ret = \"coords\")\n#> \n#> Data: predict_glm in 28 controls (ref No_default) > 147 cases (ref default).\n#> Area under the curve: 0.982\n# This function prints the ROC Curve. \n# roc0  is a object type \"roc\"\n# p is a vector of probabilities \n# best is a method to estimate the best model (see an expination below)\nMy_roc<-function(roc0,best=c(\"FALSE\",\"youden\",\"closest.topleft\"))\n  {\n  yl<-\"Recall or sensitivity\"\n  xl<-\"False pos or (1-specificity)\"\n  pat<-\"thresholds: %.3f \\n\\nSp: %.3f \\nSens: %.3f\"\n  if (best==F){\n    plot(roc0,print.auc=TRUE, ylab=yl, xlab=xl,auc.polygon=TRUE, \n         \n         legacy.axes=T,\n         auc.polygon.col=\"lightblue\",\n         print.thres = c(.5),\n         print.thres.pattern=pat,\n         max.auc.polygon=TRUE,\n         print.thres.pch=16)\n     \n\n  } else if (best==\"youden\") {  \n    plot(roc0,  ylab=yl, xlab=xl, print.thres=\"best\", print.thres.best.method=\"youden\", \n         legacy.axes=TRUE, \n         print.auc=TRUE, auc.polygon=TRUE,auc.polygon.col=\"lightblue\",\n         print.thres.cex = .9 ,print.thres.pattern=pat,max.auc.polygon=TRUE,print.thres.pch=16)\n  } else if (best==\"closest.topleft\")  {\n    plot(roc0,  ylab=yl, xlab=xl, print.thres=\"best\", \n         legacy.axes=TRUE, \n         print.thres.best.method=\"closest.topleft\",print.auc=TRUE,\n         auc.polygon=TRUE,auc.polygon.col=\"lightblue\",\n         print.thres.cex = .9,\n         print.thres.pattern=pat,max.auc.polygon=TRUE,print.thres.pch=16)\n}\n}\n\nMy_roc(roc0,best=FALSE)\nMy_roc(roc0,best=\"youden\")\nMy_roc(roc0,best=\"closest.topleft\")"},{"path":"cross-validation.html","id":"cross-validation","chapter":"4 Cross Validation","heading":"4 Cross Validation","text":"","code":""},{"path":"cross-validation.html","id":"regression-cross-validation","chapter":"4 Cross Validation","heading":"4.1 Regression Cross Validation","text":"previous chapter, split sample training testing. partition different samples? Remember last chapter, got RMSE house_test data set 4.7889106^{4}. used another seed partition got lower, worse, higher RMSE? Don´t think better make many partitions estimate time RMSE verify consistent RMSE ? cross-validation. Instead dividing sample , cross-validation involves splitting sample several times estimating RMSE, MAE R-squared . several cross validations methods. common Leave-One-Cross-Validation (LOOCV) k-Fold Cross-Validation (k-fold).","code":""},{"path":"cross-validation.html","id":"leave-one-out-cross-validation-loocv","chapter":"4 Cross Validation","heading":"4.1.1 Leave-One-Out Cross-Validation (LOOCV)","text":"Understanding question. “id” previous output order (.e., 1,2,3,…,n)?Continuing housing example, split sample “n” parts. “n” number rows data set. Cross-validation training dataset; case, \\(n\\) 584.first fold partition taking first observation. see previous output, 1rst observation one id=77.first fold partition taking first observation. see previous output, 1rst observation one id=77.second fold taking second, one id=480And , get \\(n\\) folds, case, 467. One folds, usually first, validation set (like test data set). \\(n−1\\) model must trained tested, making prediction fold1 estimating performance metric, RMSE. example:processes repeat n-2 folds, training fold3 testing fold1. end, get n-1 performance measures. average RMSEs LOOCV.\\[LOOCV =\\frac{1}{n}\\ \\sum_{=1}^{n} RMSE_{} \\]last formula, use RMSE, metric, MAE Rsquared.Fortunately, function “train” library “caret” formerly described processes (Kuhn 2019).train function similar structure “lm” function.train(SalePrice ~ ., data = house_train,\nmethod = “lm”,\ntrControl = )first argument equation, second database. cross-validation, use train data set need specify model, case, “lm”. function allows 238 writing book, “lm” one . name says, argument “trControl” control cross-validation parameters.convenience, library developers suggest separating “trControl” argument argument ruled another function, “trainControl”, many parameters. start creating object function, argument method, case, “LOOCV”.use function train, specifying model “lm”, trControl use object fitControl.previous result, got RMSE 5.4486353^{4}, average n-1 RMSE. see implication previous result, remember previous chapter, got RMSE house_test data set 4.7889106^{4}, result one partition. last result, cross-validation, tells us, case, taking RMSE 4.7889106^{4} -estimating (thinking good model ) (thinking good model ) model performance.arguments “trainControl” function, preprocessing, indicate want process data, example, center scale.“preprocessing” message previous output indicates didn´t select option. preprocess option improve model performance. example, adding “nzv”, Zero- Near Zero-Variance Predictors, BoxCox transforming Predictors preprocess. explanation preprocess alternatives, see library Caret (Kuhn 2019).Another alternative preProcess=c(“center”, “scale”,“YeoJohnson”)\n“center”, “scale” centering scaling methods. “YeoJohnson” another transforming Predictors preprocess option.see previous output, “Preprocessing” indicates four variables removed, suggests improve model performance removing four variables. get remaining variables, use code:Regarding re-sampling: cross-validation name method. summary sample sizes always \\(n−1\\), case case 466. RMSE 466 average \\(n−1\\) test predictions. Finally, message “tuning parameter ‘intercept’ …”, “trControl” argument “train” function accepts arguments parameter tuning process, cover next chapter.Finally, message “Tuning parameter ‘intercept’ held constant value TRUE” argument changed tuning parameters, cover next chapter.","code":"#>      Id SalePrice MSSubClass MSZoning\n#> 64   64    163000         20        4\n#> 540 540    170000         80        4\n#> 200 200    155000         70        5\n#> 171 171    142000         20        4\n#> 41   41    250000         60        2\n#> 268 268    155000         60        4\n#>      Id SalePrice MSSubClass MSZoning\n#> 77   77    274900         20        4\n#> 480 480    186700         20        4\n#> 452 452    135000         50        4\n#> 580 580    240000         60        4\n#> 139 139    207000        120        5\n#> 162 162    245500        120        4\nfold1<- house_train[2:dim[1],] \nhead(fold1[,1:4])\n#>      Id SalePrice MSSubClass MSZoning\n#> 540 540    170000         80        4\n#> 200 200    155000         70        5\n#> 171 171    142000         20        4\n#> 41   41    250000         60        2\n#> 268 268    155000         60        4\n#> 566 566    175900        120        4\ntail(fold1[,1:4])\n#>      Id SalePrice MSSubClass MSZoning\n#> 77   77    274900         20        4\n#> 480 480    186700         20        4\n#> 452 452    135000         50        4\n#> 580 580    240000         60        4\n#> 139 139    207000        120        5\n#> 162 162    245500        120        4\nfold2<- house_train[c(1,3:dim[1]),] \nhead(fold2[,1:4])\n#>      Id SalePrice MSSubClass MSZoning\n#> 64   64    163000         20        4\n#> 200 200    155000         70        5\n#> 171 171    142000         20        4\n#> 41   41    250000         60        2\n#> 268 268    155000         60        4\n#> 566 566    175900        120        4\ntail(fold2[,1:4])\n#>      Id SalePrice MSSubClass MSZoning\n#> 77   77    274900         20        4\n#> 480 480    186700         20        4\n#> 452 452    135000         50        4\n#> 580 580    240000         60        4\n#> 139 139    207000        120        5\n#> 162 162    245500        120        4\nhouse_model<-lm(SalePrice~.,data=fold2)\nhouse_predict<-predict(house_model,fold1)\nsqrt(mean((fold1[,\"SalePrice\"]-house_predict)^2 ,na.rm = T))\n#> [1] 41984.83\nlibrary(caret)\n\nfitControl <- trainControl(method = \"LOOCV\")                   \ngbmFit1 <- train(SalePrice ~ ., data = house_train, \n                 method = \"lm\", \n                 trControl = fitControl)\ngbmFit1\n#> Linear Regression \n#> \n#> 467 samples\n#>  51 predictor\n#> \n#> No pre-processing\n#> Resampling: Leave-One-Out Cross-Validation \n#> Summary of sample sizes: 466, 466, 466, 466, 466, 466, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   54486.35  0.6468026  31103.23\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\n\ngbmFit1 <- train(SalePrice ~ ., data = house_train, \n                 method = \"lm\", \n                 trControl = fitControl, preProcess= c(\"BoxCox\",\"nzv\"))\n\ngbmFit1\n#> Linear Regression \n#> \n#> 467 samples\n#>  51 predictor\n#> \n#> Pre-processing: Box-Cox transformation (36), remove (4) \n#> Resampling: Leave-One-Out Cross-Validation \n#> Summary of sample sizes: 466, 466, 466, 466, 466, 466, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   50277.51  0.6939262  29454.77\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\nhead(gbmFit1$finalModel$xNames)\n#> [1] \"Id\"          \"MSSubClass\"  \"MSZoning\"    \"LotFrontage\" \"LotArea\"    \n#> [6] \"LotShape\""},{"path":"cross-validation.html","id":"k-fold-cross-validation-k-foldcv","chapter":"4 Cross Validation","heading":"4.1.2 k-fold Cross Validation (k-FoldCV)","text":"k-FoldCV similar LOOCV, except instead making \\(n\\) partitions, just requires \\(k\\). difference LOOCV usually, \\(k\\) less \\(n\\); example, \\(k=10\\). Another difference \\(k\\) partitions randomly selected. example, first partition called Fold1. takes database, example, house_train, randomly splits 80% training 20% validation data set. model estimated training, prediction validated validation set estimating metrics. process repeated \\(k\\) times.see , “train” function rule determine % training validation sets. following formula summarizes procedure RMSE.\\[k-FoldCV =\\frac{1}{k}\\ \\sum_{=1}^{k} RMSE_{} \\]use “train” function example LOOCV method. apply ten folds \\(k=10\\)case, k-FoldCV method “cv.” argument “number” indicates number folds \\(k\\)arguments “train” function similar LOOCV method.previous output, “Summary sample sizes” shows, fold, number observations training set, case 90% sample size, case 467. Resampling results, RMSE, Rsquared MAE average 10 folds. next plot, show 10 results.“train” function programmed different partitions number k-folds. example, k=5 partition 80. \\(k\\), % partition.Authors (James et al. 2017), consider k-FoldCV advantage less computationally intensive method like LOOCV. Also, exemplifies results k-Fol=10 LOOV similar regarding bias-variance trade-.“train” function allows repetition fold. example, want repeat fold two times, need add arguments, method = “repeatedcv”, number repeats, case, 2. result, end 20 results.algorithm “train” function developed, get different results use k=20 k=10 repeated 2 times.example, following plots, upper side, show different examples “repeatedcv”. bottom, show different examples similar, number, “cv” méthod. cases seed.results cv method concentrated bottom. Meanwhile, repeated-cv dispersion higher. result, RMSE average lower “cv”.valid question , better method? need evaluate model test data set (validation) answer . now, answer results tell us , depending resampling method, getting quite different results, implies probably need something else get stable results methods.example, can make cross-validation variable selection.Remember RMSE variables 5.3829898^{4}. get variables final model:","code":"\nk<-10\nfitControl <- trainControl(method = \"cv\",\n                           number = k)\nset.seed (26)\n\nlmFit1 <- train(SalePrice ~ ., data = house_train, \n                 method = \"lm\", \n                 trControl = fitControl)\nlmFit1\n#> Linear Regression \n#> \n#> 467 samples\n#>  51 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 420, 421, 421, 419, 420, 421, ... \n#> Resampling results:\n#> \n#>   RMSE     Rsquared   MAE     \n#>   53829.9  0.7130432  32074.47\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE#> [1] \"420 467 90\"\ny<-as.data.frame(lmFit1$resample)[,\"RMSE\"] # this data frame has the RMSE results \np1<-lmFit1$results[,\"RMSE\"] # this object has the average RMSE \n\nplot(y, ylab=\"RMSE\", xlab=\"k-Folds\", pch = 16, main= paste(\"k-FoldsCV results for k=\",k))\nabline(p1,0,col=\"black\",lwd=2,lty = 2)\nlegend(x= \"topright\", legend = c(paste(\"average RMSE\",round(p1,0))),lty = 2,lwd=2,col=c(\"black\"))\ntext(x =3, y = p1*1.1, labels = round(p1,0),pos = 3)#> Linear Regression \n#> \n#> 467 samples\n#>  51 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold, repeated 2 times) \n#> Summary of sample sizes: 420, 421, 421, 419, 420, 421, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   52572.29  0.7112406  31826.41\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\nk2<-10\nset.seed (26)\nfitControl3 <- trainControl(method = \"cv\",\n                           number = k2)\n\nstep <- train(SalePrice ~ ., data = house_train, \n                 method = \"lmStepAIC\", \n                 trControl = fitControl3, trace=F)\nstep\n#> Linear Regression with Stepwise Selection \n#> \n#> 467 samples\n#>  51 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 420, 421, 421, 419, 420, 421, ... \n#> Resampling results:\n#> \n#>   RMSE     Rsquared   MAE     \n#>   53183.1  0.7218324  31441.74\nmo<-colnames(step$finalModel$model)[-1]\nmo\n#>  [1] \"MSSubClass\"    \"LotFrontage\"   \"LotArea\"       \"LotShape\"     \n#>  [5] \"HouseStyle\"    \"OverallQual\"   \"OverallCond\"   \"Exterior1st\"  \n#>  [9] \"MasVnrArea\"    \"ExterQual\"     \"Foundation\"    \"BsmtQual\"     \n#> [13] \"BsmtExposure\"  \"BsmtFinType1\"  \"BsmtFinSF1\"    \"BsmtUnfSF\"    \n#> [17] \"X1stFlrSF\"     \"X2ndFlrSF\"     \"BsmtFullBath\"  \"FullBath\"     \n#> [21] \"HalfBath\"      \"KitchenQual\"   \"TotRmsAbvGrd\"  \"Fireplaces\"   \n#> [25] \"FireplaceQu\"   \"GarageArea\"    \"SaleCondition\""},{"path":"cross-validation.html","id":"stochastic-gradient-boosting","chapter":"4 Cross Validation","heading":"4.1.3 Stochastic Gradient Boosting","text":"https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdfesto —————\n##> Linear Regression Stepwise Selection\n##>\n##> 467 samples\n##> 51 predictor\n##>\n##> pre-processing\n##> Resampling: Cross-Validated (10 fold, repeated 10 times)\n##> Summary sample sizes: 420, 421, 421, 419, 420, 421, …\n##> Resampling results:\n##>\n##> RMSE Rsquared MAE\n##> 50981 0.7143878 30917.71","code":"\nset.seed (26)\nfitControl <- trainControl(method = \"cv\",\n                           number = 10)\n\ngbmFit1 <- train(SalePrice ~ ., data = house_train, \n                 method = \"gbm\", \n                 trControl = fitControl,\n                 ## This last option is actually one\n                 ## for gbm() that passes through\n                 verbose = FALSE)\ngbmFit1\n#> Stochastic Gradient Boosting \n#> \n#> 467 samples\n#>  51 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 420, 421, 421, 419, 420, 421, ... \n#> Resampling results across tuning parameters:\n#> \n#>   interaction.depth  n.trees  RMSE      Rsquared   MAE     \n#>   1                   50      47239.17  0.7444321  30600.93\n#>   1                  100      46147.22  0.7537532  29225.96\n#>   1                  150      45741.85  0.7555015  28969.01\n#>   2                   50      45232.87  0.7570658  28665.82\n#>   2                  100      44866.17  0.7601488  27908.81\n#>   2                  150      45748.88  0.7554831  27734.49\n#>   3                   50      44032.44  0.7708265  27661.85\n#>   3                  100      43706.40  0.7775313  26762.67\n#>   3                  150      44620.73  0.7733739  27046.71\n#> \n#> Tuning parameter 'shrinkage' was held constant at a value of 0.1\n#> \n#> Tuning parameter 'n.minobsinnode' was held constant at a value of 10\n#> RMSE was used to select the optimal model using the smallest value.\n#> The final values used for the model were n.trees = 100, interaction.depth =\n#>  3, shrinkage = 0.1 and n.minobsinnode = 10.\nmin(gbmFit1$results[,\"RMSE\"])\n#> [1] 43706.4##> Call:\n##> lm(formula = .outcome ~ MSSubClass + LotFrontage + LotArea + \n##>    LotShape + HouseStyle + OverallQual + OverallCond + Exterior1st + \n##>    MasVnrArea + ExterQual + Foundation + BsmtQual + BsmtExposure + \n##>    BsmtFinType1 + BsmtFinSF1 + BsmtUnfSF + X1stFlrSF + X2ndFlrSF + \n##>    BsmtFullBath + FullBath + HalfBath + KitchenQual + TotRmsAbvGrd + \n##>    Fireplaces + FireplaceQu + GarageArea + SaleCondition, data = dat)\n##>\n##> Coefficients:\n##>  (Intercept)     MSSubClass    LotFrontage        LotArea       LotShape  \n##> HouseStyle  \n##>    4.120e+04     -2.299e+02     -3.216e+02      5.828e-01     -2.812e+03     \n##> -2.889e+03  \n##>  OverallQual    OverallCond    Exterior1st     MasVnrArea      ExterQual     \n##> Foundation  \n##>    1.893e+04      9.620e+03     -1.302e+03      5.605e+01     -8.628e+03      \n##> 8.200e+03  \n##>     BsmtQual   BsmtExposure   BsmtFinType1     BsmtFinSF1      BsmtUnfSF      \n##> X1stFlrSF  \n##>   -1.178e+04     -5.687e+03     -3.146e+03     -2.498e+01     -2.111e+01      \n##> 3.817e+01  \n##>    X2ndFlrSF   BsmtFullBath       FullBath       HalfBath    KitchenQual   \n##> TotRmsAbvGrd  \n##>    2.565e+01      9.998e+03      1.689e+04      1.139e+04     -7.131e+03      \n##> 4.149e+03  \n##>   Fireplaces    FireplaceQu     GarageArea  SaleCondition  \n##>    1.359e+04     -4.135e+03      4.338e+01      4.348e+03  "},{"path":"cross-validation.html","id":"classification-cross-validation","chapter":"4 Cross Validation","heading":"4.2 Classification Cross-Validation","text":"Finally use functions trainControl train cross validation.start aplying logit model. default, metric verofy predictive power Accuracy.Also specify ROC, sensitivity specificity metrics.can use models gbm.LDA","code":"\ncredit<-read.csv(\"https://raw.githubusercontent.com/abernal30/ml_book/main/credit.csv\")\n\nc2<-ifelse(credit[,\"Default\"]==\"Charged Off\" ,\"No_default\",\"default\") \ncredit[,\"Default\"]<-c2\ncredit[,\"Default\"]<-factor(credit[,\"Default\"])\n\nset.seed (43)\ndim<-dim(credit)\ntrain_sample<-sample(dim[1],dim[1]*.8)\ncredit_train <- credit[train_sample, ]\ncredit_test  <- credit[-train_sample, ]\nfitControl <- trainControl(method = \"cv\",\n                           number = 10, \n                           classProbs = T ) \n\nglmFit <- train(Default~ ., data = credit_train, \n                 method = \"glm\", \n                 trControl = fitControl,\n                family=binomial())\nglmFit        \n#> Generalized Linear Model \n#> \n#> 698 samples\n#>  70 predictor\n#>   2 classes: 'default', 'No_default' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 628, 628, 629, 628, 627, 628, ... \n#> Resampling results:\n#> \n#>   Accuracy   Kappa    \n#>   0.9399764  0.7908423\n\nfitControl_Roc <- trainControl(method = \"cv\",\n                           number = 10, \n                           classProbs = T,\n                           summaryFunction = twoClassSummary ) \n\nglmFit <- train(Default~ ., data = credit_train, \n                 method = \"glm\", \n                 trControl = fitControl_Roc,\n                 metric = \"ROC\",\n                family=binomial())\nglmFit        \n#> Generalized Linear Model \n#> \n#> 698 samples\n#>  70 predictor\n#>   2 classes: 'default', 'No_default' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 628, 628, 628, 628, 628, 628, ... \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.9591776  0.9638515  0.8613636\ngbmFit <- train(Default~ ., data = credit_train, \n                 method = \"gbm\", \n                 trControl = fitControl_Roc ,verbose = FALSE)\n\n#verbose = FALSE\ngbmFit \n#> Stochastic Gradient Boosting \n#> \n#> 698 samples\n#>  70 predictor\n#>   2 classes: 'default', 'No_default' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 627, 628, 628, 628, 628, 628, ... \n#> Resampling results across tuning parameters:\n#> \n#>   interaction.depth  n.trees  ROC        Sens       Spec     \n#>   1                   50      0.9944136  0.9879603  0.8560606\n#>   1                  100      0.9954822  0.9862653  0.8719697\n#>   1                  150      0.9953124  0.9879895  0.8719697\n#>   2                   50      0.9969223  0.9896844  0.9234848\n#>   2                  100      0.9960390  0.9879603  0.9143939\n#>   2                  150      0.9964807  0.9931327  0.9143939\n#>   3                   50      0.9958871  0.9914085  0.9159091\n#>   3                  100      0.9960366  0.9879603  0.9068182\n#>   3                  150      0.9967525  0.9914085  0.9068182\n#> \n#> Tuning parameter 'shrinkage' was held constant at a value of 0.1\n#> \n#> Tuning parameter 'n.minobsinnode' was held constant at a value of 10\n#> ROC was used to select the optimal model using the largest value.\n#> The final values used for the model were n.trees = 50, interaction.depth =\n#>  2, shrinkage = 0.1 and n.minobsinnode = 10.\nset.seed(825)\nldaFit <- train(Default~ ., data = credit_train, \n                 method = \"lda\", \n                 trControl = fitControl_Roc ,\n                  metric = \"ROC\")\nldaFit\n#> Linear Discriminant Analysis \n#> \n#> 698 samples\n#>  70 predictor\n#>   2 classes: 'default', 'No_default' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 628, 628, 629, 628, 628, 629, ... \n#> Resampling results:\n#> \n#>   ROC        Sens       Spec     \n#>   0.9844228  0.9742256  0.8469697\nset.seed(825)\nfdaFit <- train(Default~ ., data = credit_train, \n                 method = \"fda\", \n                 trControl = fitControl_Roc ,\n                  metric = \"ROC\")\nfdaFit\n#> Flexible Discriminant Analysis \n#> \n#> 698 samples\n#>  70 predictor\n#>   2 classes: 'default', 'No_default' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 628, 628, 629, 628, 628, 629, ... \n#> Resampling results across tuning parameters:\n#> \n#>   nprune  ROC        Sens       Spec     \n#>    2      0.9037095  0.9706897  0.7454545\n#>   21      0.9978178  0.9914378  0.9151515\n#>   41      0.9971116  0.9914378  0.9227273\n#> \n#> Tuning parameter 'degree' was held constant at a value of 1\n#> ROC was used to select the optimal model using the largest value.\n#> The final values used for the model were degree = 1 and nprune = 21.\nresamps <- resamples(list(Logit = glmFit,\n                          GBM = gbmFit,\n                          LDA=ldaFit,\n                          Logit=glmFit,\n                          Fda=fdaFit))\n\ntheme1 <- trellis.par.get()\ntheme1$plot.symbol$col = rgb(.2, .2, .2, .4)\ntheme1$plot.symbol$pch = 16\ntheme1$plot.line$col = rgb(1, 0, 0, .7)\ntheme1$plot.line$lwd <- 2\ntrellis.par.set(theme1)\nbwplot(resamps, layout = c(3, 1))"},{"path":"cross-validation.html","id":"evaluate-your-system-on-the-test-set","chapter":"4 Cross Validation","heading":"4.2.1 Evaluate Your System on the Test Set","text":"","code":"\nlibrary(pROC)\nclas_test<-function(model,data,levelss,dep,pos){\n  \ncredit_predict<-predict(model, newdata=data,type = \"raw\")\nDefaultf<-factor(credit_predict,levels=levelss)\nconfu<-confusionMatrix(Defaultf,credit_test[,dep],positive=pos)\nsen<-round(as.vector(confu$byClass[1])*100,2)\n\nref<- data[,dep]\npredict_glm<-predict(model, newdata=data, type=\"prob\")\nroc0<-roc(ref, predict_glm[,1], levels = rev(levels(ref)),ret=\"coords\")\nAUC<-round(as.vector(roc0$auc),4)*100\n\n\nprint(paste(model$method,\"Sensibility=\",sen))\nprint(paste(model$method,\"AUC=\",AUC))\n\n}\nclas_test(glmFit,credit_test,c(\"default\",\"No_default\"),\"Default\",\"default\")\n#> [1] \"glm Sensibility= 95.92\"\n#> [1] \"glm AUC= 98.2\"\nclas_test(gbmFit,credit_test,c(\"default\",\"No_default\"),\"Default\",\"default\")\n#> [1] \"gbm Sensibility= 97.96\"\n#> [1] \"gbm AUC= 99.59\"\nclas_test(ldaFit,credit_test,c(\"default\",\"No_default\"),\"Default\",\"default\")\n#> [1] \"lda Sensibility= 96.6\"\n#> [1] \"lda AUC= 98.1\"\nclas_test(fdaFit,credit_test,c(\"default\",\"No_default\"),\"Default\",\"default\")\n#> [1] \"fda Sensibility= 98.64\"\n#> [1] \"fda AUC= 99.73\""},{"path":"improving-performance.html","id":"improving-performance","chapter":"5 Improving Performance","heading":"5 Improving Performance","text":"","code":""},{"path":"improving-performance.html","id":"parameter-tuning","chapter":"5 Improving Performance","heading":"5.1 PARAMETER TUNING","text":"Hyperparameters model parameters specified training model – .e., parameters different model parameters – weights AI/ML model learns model training.many machine learning problems, finding best hyperparameters iterative potentially time-intensive process called “hyperparameter optimization.Hyperparameters directly impact performance trained machine-learning model. Choosing right hyperparameters can dramatically improve prediction accuracy. However, can challenging optimize often large combination possible hyperparameter values.Tuning machine learning model iterative process. Data scientists typically run numerous experiments train evaluate models, trying different features, different loss functions, different AI/ML models, adjusting model parameters hyperparameters. Examples steps involved tuning training machine learning model include feature engineering, loss function formulation, model testing selection, regularization, selection hyperparameters Krishnan (2022).Let’s assume now shortlist promising models. help now fine-tuned . Let’s look ways can .retrieved optimum values individual model parameters, can use grid search obtain combination hyperparameter (parameters also known hyperparameters) values model can give us highest accuracy.Grid Search evaluates possible combinations parameter values.Grid Search exhaustive uses brute force evaluate accurate values. Therefore computationally intensive task.","code":"\nhouse<-read.csv(\"data/house_clean.csv\")\n#house<-read.csv(\"https://raw.githubusercontent.com/abernal30/ml_book/main/housing.csv\")\nset.seed (26)\ndim<-dim(house)\ntrain_sample<-sample(dim[1],dim[1]*.8)\nhouse_train <- house[train_sample, ]\nhouse_test  <- house[-train_sample, ]\nlibrary(caret)\nset.seed (26)\nfitControl <- trainControl(method = \"cv\",\n                           number = 10)\n\ngbmFit <- train(SalePrice ~ ., data = house_train, \n                 method = \"gbm\", \n                 trControl = fitControl, \n                 verbose = FALSE)\ngbmFit\n#> Stochastic Gradient Boosting \n#> \n#> 467 samples\n#>  51 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 420, 421, 421, 419, 420, 421, ... \n#> Resampling results across tuning parameters:\n#> \n#>   interaction.depth  n.trees  RMSE      Rsquared   MAE     \n#>   1                   50      47239.17  0.7444321  30600.93\n#>   1                  100      46147.22  0.7537532  29225.96\n#>   1                  150      45741.85  0.7555015  28969.01\n#>   2                   50      45232.87  0.7570658  28665.82\n#>   2                  100      44866.17  0.7601488  27908.81\n#>   2                  150      45748.88  0.7554831  27734.49\n#>   3                   50      44032.44  0.7708265  27661.85\n#>   3                  100      43706.40  0.7775313  26762.67\n#>   3                  150      44620.73  0.7733739  27046.71\n#> \n#> Tuning parameter 'shrinkage' was held constant at a value of 0.1\n#> \n#> Tuning parameter 'n.minobsinnode' was held constant at a value of 10\n#> RMSE was used to select the optimal model using the smallest value.\n#> The final values used for the model were n.trees = 100, interaction.depth =\n#>  3, shrinkage = 0.1 and n.minobsinnode = 10.\nmin(gbmFit$results[,\"RMSE\"])\n#> [1] 43706.4\nseq(0.08,0.2,.01)\n#>  [1] 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20\nset.seed (26)\nfitControl <- trainControl(method = \"cv\",\n                           number = 10)\n\ngbmGrid <-  expand.grid(interaction.depth = 3, \n                        n.trees = 100, \n                        shrinkage = seq(0.08,0.2,.01),\n                        n.minobsinnode = c(10,20,30))\n                        \nnrow(gbmGrid)\n#> [1] 39\n\n\ngbmFit2 <- train(SalePrice ~ ., data = house_train, \n                 method = \"gbm\", \n                 trControl = fitControl, \n                 verbose = FALSE, \n                 ## Now specify the exact models \n                 ## to evaluate:\n                 tuneGrid = gbmGrid)\ngbmFit2\n#> Stochastic Gradient Boosting \n#> \n#> 467 samples\n#>  51 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 420, 421, 421, 419, 420, 421, ... \n#> Resampling results across tuning parameters:\n#> \n#>   shrinkage  n.minobsinnode  RMSE      Rsquared   MAE     \n#>   0.08       10              43689.38  0.7757923  26414.49\n#>   0.08       20              43439.41  0.7762678  26253.74\n#>   0.08       30              43718.02  0.7740018  27043.45\n#>   0.09       10              44482.73  0.7657377  26932.51\n#>   0.09       20              44471.18  0.7694100  26930.13\n#>   0.09       30              42765.74  0.7850388  26107.17\n#>   0.10       10              45797.81  0.7565011  27752.53\n#>   0.10       20              43305.74  0.7752004  26133.99\n#>   0.10       30              44014.90  0.7733803  27155.20\n#>   0.11       10              44489.95  0.7656922  26866.05\n#>   0.11       20              44102.22  0.7772802  26763.98\n#>   0.11       30              43173.50  0.7852119  26475.20\n#>   0.12       10              45685.92  0.7560192  27278.29\n#>   0.12       20              43837.29  0.7733656  26780.05\n#>   0.12       30              43632.73  0.7731039  27097.95\n#>   0.13       10              45015.04  0.7633598  27339.32\n#>   0.13       20              44378.70  0.7733293  26922.59\n#>   0.13       30              43729.28  0.7738287  26848.09\n#>   0.14       10              45847.55  0.7616066  26985.59\n#>   0.14       20              43476.83  0.7753577  27154.57\n#>   0.14       30              44204.43  0.7733527  27236.38\n#>   0.15       10              46597.58  0.7505296  27959.85\n#>   0.15       20              45481.20  0.7606624  27931.86\n#>   0.15       30              44495.46  0.7689698  27904.48\n#>   0.16       10              46312.23  0.7551459  28093.47\n#>   0.16       20              44609.52  0.7720711  26788.57\n#>   0.16       30              44293.15  0.7709916  27231.55\n#>   0.17       10              47092.71  0.7503482  27909.55\n#>   0.17       20              45266.73  0.7597926  27838.44\n#>   0.17       30              44495.99  0.7682215  27612.17\n#>   0.18       10              46187.08  0.7560767  27455.96\n#>   0.18       20              45563.69  0.7615617  28022.82\n#>   0.18       30              44019.18  0.7697064  28476.80\n#>   0.19       10              49851.20  0.7284168  29410.44\n#>   0.19       20              46902.27  0.7505154  28841.09\n#>   0.19       30              44854.58  0.7651957  28353.89\n#>   0.20       10              48231.76  0.7329092  29188.07\n#>   0.20       20              44947.66  0.7651168  27865.51\n#>   0.20       30              44618.00  0.7586902  27987.05\n#> \n#> Tuning parameter 'n.trees' was held constant at a value of 100\n#> Tuning\n#>  parameter 'interaction.depth' was held constant at a value of 3\n#> RMSE was used to select the optimal model using the smallest value.\n#> The final values used for the model were n.trees = 100, interaction.depth =\n#>  3, shrinkage = 0.09 and n.minobsinnode = 30.\ntrellis.par.set(caretTheme())\nplot(gbmFit2) \ngbmFit2$bestTune\n#>   n.trees interaction.depth shrinkage n.minobsinnode\n#> 6     100                 3      0.09             30\nset.seed (26)\nfitControl <- trainControl(method = \"cv\",\n                           number = 10)\n\ngbmGrid <-  expand.grid(interaction.depth = 3, \n                        n.trees = 100, \n                        shrinkage = 0.09,\n                        n.minobsinnode = 30)\n                        \n\ngbmFit1 <- train(SalePrice ~ ., data = house_train, \n                 method = \"gbm\", \n                 trControl = fitControl,\n                 verbose = FALSE,tuneGrid = gbmGrid)\ngbmFit1\n#> Stochastic Gradient Boosting \n#> \n#> 467 samples\n#>  51 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 420, 421, 421, 419, 420, 421, ... \n#> Resampling results:\n#> \n#>   RMSE      Rsquared   MAE     \n#>   43538.88  0.7755887  27051.03\n#> \n#> Tuning parameter 'n.trees' was held constant at a value of 100\n#> Tuning\n#> \n#> Tuning parameter 'shrinkage' was held constant at a value of 0.09\n#> \n#> Tuning parameter 'n.minobsinnode' was held constant at a value of 30"},{"path":"improving-performance.html","id":"analyze-the-best-models-and-their-errors","chapter":"5 Improving Performance","heading":"5.2 Analyze the Best Models and Their Errors","text":"","code":"\nset.seed (26)\n\nlmFit <- train(SalePrice ~ ., data = house_train, \n                 method = \"lm\", \n                 trControl = fitControl)\nlmFit\n#> Linear Regression \n#> \n#> 467 samples\n#>  51 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 420, 421, 421, 419, 420, 421, ... \n#> Resampling results:\n#> \n#>   RMSE     Rsquared   MAE     \n#>   53829.9  0.7130432  32074.47\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\nk2<-10\nset.seed (26)\nfitControl3 <- trainControl(method = \"cv\",\n                           number = k2)\n\nstep <- train(SalePrice ~ ., data = house_train, \n                 method = \"lmStepAIC\", \n                 trControl = fitControl3, trace=F)\nstep\n#> Linear Regression with Stepwise Selection \n#> \n#> 467 samples\n#>  51 predictor\n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 420, 421, 421, 419, 420, 421, ... \n#> Resampling results:\n#> \n#>   RMSE     Rsquared   MAE     \n#>   53183.1  0.7218324  31441.74\nresamps <- resamples(list(GBM = gbmFit1,\n                          lm= lmFit,\n                          step = step))\nsummary(resamps)\n#> \n#> Call:\n#> summary.resamples(object = resamps)\n#> \n#> Models: GBM, lm, step \n#> Number of resamples: 10 \n#> \n#> MAE \n#>          Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\n#> GBM  16916.01 22683.84 26802.82 27051.03 33250.31 34700.58    0\n#> lm   24099.97 27569.17 32581.94 32074.47 33796.23 46936.58    0\n#> step 23457.60 27519.35 31131.39 31441.74 33246.95 47069.12    0\n#> \n#> RMSE \n#>          Min.  1st Qu.   Median     Mean  3rd Qu.      Max. NA's\n#> GBM  21042.28 29863.83 42580.60 43538.88 53434.67  68983.63    0\n#> lm   29902.95 38797.25 46407.32 53829.90 50676.37 142708.97    0\n#> step 29473.14 38261.54 45661.85 53183.10 49510.85 142365.64    0\n#> \n#> Rsquared \n#>           Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\n#> GBM  0.4466954 0.7499098 0.7749271 0.7755887 0.8692610 0.9222225    0\n#> lm   0.1182236 0.7380041 0.7705471 0.7130432 0.7989847 0.8440776    0\n#> step 0.1201389 0.7623466 0.7765474 0.7218324 0.7931811 0.8461111    0\ntheme1 <- trellis.par.get()\ntheme1$plot.symbol$col = rgb(.2, .2, .2, .4)\ntheme1$plot.symbol$pch = 16\ntheme1$plot.line$col = rgb(1, 0, 0, .7)\ntheme1$plot.line$lwd <- 2\ntrellis.par.set(theme1)\nbwplot(resamps, layout = c(3, 1))"},{"path":"improving-performance.html","id":"evaluate-your-system-on-the-test-set-1","chapter":"5 Improving Performance","heading":"5.3 Evaluate Your System on the Test Set","text":"","code":"\nlibrary(gbm)\ngbm1 <-gbm(SalePrice ~ ., data = house_train, n.trees = 100, shrinkage = 0.09,\ninteraction.depth = 3, distribution = \"gaussian\",bag.fraction = 0.5, train.fraction = 0.5,\nn.minobsinnode = 30, cv.folds = 10, keep.data = TRUE,\nverbose = FALSE)\nbest.iter <- gbm.perf(gbm1, method = \"cv\")\nprint(best.iter)\n#> [1] 85\nhouse_predict<-predict(gbm1, newdata = house_test, n.trees = 100, shrinkage = 0.09,interaction.depth = 3, n.minobsinnode = 30)\n\nsqrt(mean((house_test[,\"SalePrice\"]-house_predict)^2 ,na.rm = T))\n#> [1] 54768.78"},{"path":"clustering.html","id":"clustering","chapter":"6 Clustering","heading":"6 Clustering","text":"Unsupervised learning - ClusteringClustering technique aims group similar data points points group similar features groups. group similar data points called Cluster.example, suppose following data frame, hypothetical data students ages grades course:betther undestandng plot, following plot shows person similar terms age grade.One one identify person group persons similar terms age grade , equivalent say cluster, using Euclidean Distance (ED):\\[d_{euc}(p,q)= \\sqrt{ \\sum_{=1}^{n} (p_{}-q_{}})^{2}\\]\\(p_{}\\), \\(p_{}\\) two points euclidean space. example, two different persons data set. \\(n\\) number features, example two, age anf grade. example, Euclidean Distance person 1 2 :lower (higher) ED two persons, similars (different) , porbably grouped (grouped) cluster.estimate Euclidean Distance persons data set, use funciton “get_dist”, library “factoextra”:see output, result shows ED person. important notice output data frame, “dist” object:previous outpt giving us infromation people grouped clusters, following plot , using function “fviz_dist”, wich first argument “dist” object made last “chunk”:previous plot, red color squares persons higher ED blue ones lower ones.terms scatter plot made , take individual pairs ED less 2, example, get following results:see red dots kind grouped , also yellow ones. sense, say individuals red dots cluster, individuals yellow cluster. repeat procees -color individuales, moment waned explian clusters formed.","code":"\nset.seed(1100)\nx <- round(rnorm(12, 20, 3),0)\ny <- round(rnorm(12, 95, 4),0)\ny<-ifelse(y>100,100,y)\ndf<- data.frame(Age=x, Grade=y)\nrownames(df)<-paste(\"Ind\",rownames(df))\ndf\n#>        Age Grade\n#> Ind 1   21   100\n#> Ind 2   19    98\n#> Ind 3   20    95\n#> Ind 4   17    97\n#> Ind 5   17    95\n#> Ind 6   21    97\n#> Ind 7   19    96\n#> Ind 8   19    97\n#> Ind 9   16    87\n#> Ind 10  25    95\n#> Ind 11  23    97\n#> Ind 12  22   100\nplot(df[,\"Age\"], df[,\"Grade\"], col = \"blue\", pch = 1, cex = 1.5,ylab=\"Grade\",xlab=\"Age\",ylim=c(88,101),xlim=c(16,26))\ntext(df[,\"Age\"] + .3, df[,\"Grade\"] + 0.9, labels = rownames(df))\nsqrt((df[\"Ind 1\",\"Age\"]-df[\"Ind 2\",\"Age\"])^2+(df[\"Ind 1\",\"Grade\"]-df[\"Ind 2\",\"Grade\"])^2)\n#> [1] 2.828427\nlibrary(factoextra) #\ndistance<-get_dist(df, method = \"euclidean\")\ndistance\n#>            Ind 1     Ind 2     Ind 3     Ind 4     Ind 5     Ind 6     Ind 7\n#> Ind 2   2.828427                                                            \n#> Ind 3   5.099020  3.162278                                                  \n#> Ind 4   5.000000  2.236068  3.605551                                        \n#> Ind 5   6.403124  3.605551  3.000000  2.000000                              \n#> Ind 6   3.000000  2.236068  2.236068  4.000000  4.472136                    \n#> Ind 7   4.472136  2.000000  1.414214  2.236068  2.236068  2.236068          \n#> Ind 8   3.605551  1.000000  2.236068  2.000000  2.828427  2.000000  1.000000\n#> Ind 9  13.928388 11.401754  8.944272 10.049876  8.062258 11.180340  9.486833\n#> Ind 10  6.403124  6.708204  5.000000  8.246211  8.000000  4.472136  6.082763\n#> Ind 11  3.605551  4.123106  3.605551  6.000000  6.324555  2.000000  4.123106\n#> Ind 12  1.000000  3.605551  5.385165  5.830952  7.071068  3.162278  5.000000\n#>            Ind 8     Ind 9    Ind 10    Ind 11\n#> Ind 2                                         \n#> Ind 3                                         \n#> Ind 4                                         \n#> Ind 5                                         \n#> Ind 6                                         \n#> Ind 7                                         \n#> Ind 8                                         \n#> Ind 9  10.440307                              \n#> Ind 10  6.324555 12.041595                    \n#> Ind 11  4.000000 12.206556  2.828427          \n#> Ind 12  4.242641 14.317821  5.830952  3.162278\nclass(distance)\n#> [1] \"dist\"\nfviz_dist(distance,  gradient = list(low = \"#00AFBB\", mid = \"white\", high = \"#FC4E07\"))\nplot(df[,\"Age\"], df[,\"Grade\"], col = \"blue\", pch = 1, cex = 1.5,ylab=\"Grade\",xlab=\"Age\",ylim=c(86,101),xlim=c(16,26))\ntext(df[,\"Age\"] + .3, df[,\"Grade\"] + 0.6, labels = rownames(df))\npoints(df[ind[1, ],\"Age\"], df[ind[1, ],\"Grade\"], col = \"orange\", pch = 19, cex = 2)\npoints(df[ind[2, ],\"Age\"], df[ind[2, ],\"Grade\"], col = \"red\", pch = 19, cex = 2)\npoints(df[ind[3, ],\"Age\"], df[ind[3, ],\"Grade\"], col = \"red\", pch = 19, cex = 2)\npoints(df[ind[4, ],\"Age\"], df[ind[4, ],\"Grade\"], col = \"red\", pch = 19, cex = 2)\n#segments(x0 = 19, y0 = 96,x1=20,y1=95) "},{"path":"clustering.html","id":"agglomerative-hierarchical-clustering","chapter":"6 Clustering","heading":"6.1 Agglomerative hierarchical clustering","text":"prove last method, need partition define similarity beetween two individuals, th sale cluster. Hierarchical clustering algorithms doesn´t need predefined partition generate clusters.First, using particular proximity measure dissimilarity matrix constructed data points visually represented bottom dendrogram. closest sets clusters merged level dissimilarity matrix updated correspondingly. process agglomerative merging carried final maximal cluster (contains data objects single cluster) obtained. represent apex dendrogram mark completion merging process. now discuss different kinds proximity measures can used agglomerative hierarchical clustering. Subsequently, also provide complete version agglomerative hierarchical clustering algorithm inThe popular agglomerative clustering methods single link complete link clusterings. single link clustering [36, 46], similarity two clusters similarity similar (nearest neighbor) members. method intuitively gives importance regions clusters closest, neglecting overall structure cluster. Hence, method falls category local similarity-based clustering method. local behavior, single linkage capable effectively clustering nonelliptical, elongated shaped groups data objects. However, one main drawbacks method sensitivity noise outliers data.Complete link clustering [27] measures similarity two clusters similarity dissimilar members. equivalent choosing cluster pair whose merge smallest diameter. method takes cluster structure consideration nonlocal behavior generally obtains compact shaped clusters. However, similar single link clustering, method also sensitive outliers. single link complete link clustering graph-theoretic interpretations [16], clusters obtained single link clustering correspond connected components graph obtained complete link correspond maximal cliques graph.Lance Williams recurrence formula gives distance group k group (ij) formed fusion two groups (j) :\\[ d_{k(ij)}= \\alpha\\ d_{ki}+\\beta\\ d_{ij}+\\gamma\\ |d_{ki}-d_{kj}|, \\]\\(d_{ij}\\) s distance groups j. Lance Williams used formula define new ‘flexible’ scheme, parameter values αi + αj + β = 1, αi = αj, β < 1, γ = 0. allowing β vary, clustering schemes various characteristics can obtained. suggest small negative values β, −0.25, although Scheibler Schneider (1985) suggest −0.50 (Brian S. Everitt 2011).hClustering <- hclust(distance object,method)\nmethod=c(ward.D”, “ward.D2”, “single”, “complete”, “average”, “mcquitty” , “median” “centroid” )plot(hClustering object)chart can used visually inspect number clusters created selected distance threshold . number vertical lines hypothetical straight, horizontal line pass number clusters created distance threshold value. data points (leaves) branch labeled cluster horizontal line passed .members cluster\nmemb <-cutree(hClustering object, k = )k= número de clusters que se deseanh= cut number dendrogram","code":"\nhClustering <-  hclust(distance ,method=\"single\") # cuidado por que le pusimos distance también al de teens\nplot(hClustering)\nmemb <-cutree(hClustering, k = 3)\nhead(memb)\n#> Ind 1 Ind 2 Ind 3 Ind 4 Ind 5 Ind 6 \n#>     1     1     1     1     1     1\ntail(memb)\n#>  Ind 7  Ind 8  Ind 9 Ind 10 Ind 11 Ind 12 \n#>      1      1      2      3      1      1\ncent <- NULL\nfor(k in 1:10){\n  cent <- rbind(cent, colMeans(df[memb == k, , drop = FALSE]))\n}"},{"path":"clustering.html","id":"k-means-clustering","chapter":"6 Clustering","heading":"6.2 K-Means Clustering","text":"K-means clustering commonly used unsupervised machine learning algorithm partitioning given data set set k groups (.e. k clusters), k represents number groups pre-specified analyst. classifies objects multiple groups (.e., clusters), objects within cluster similar possible (.e., high intra-class similarity), whereas objects different clusters dissimilar possible (.e., low inter-class similarity). k-means clustering, cluster represented center (.e, centroid) corresponds mean points assigned cluster.Basic IdeaThe basic idea behind k-means clustering consists defining clusters total intra-cluster variation (known total within-cluster variation) minimized. several k-means algorithms available. standard algorithm Hartigan-Wong algorithm (1979), defines total within-cluster variation sum squared distances Euclidean distances items corresponding centroid:\\[ W(C_{k})=\\sum_{x_{}\\C_{k}}(x_{}- \\mu_{k})^2\\]\\(x_{}\\) data point belonging cluster Ck._{k} mean value points assigned cluster CkEach observation (xi) assigned given cluster sum squares (SS) distance observation assigned cluster centers (μk) minimized.define total within-cluster variation follows:\n\\[ tot.withiness=\\sum_{k=1}^k W(C_{k})=\\sum_{k=1}^k \\sum_{x_{}\\C_{k}}(x_{}- \\mu_{k})^2\\]total within-cluster sum square measures compactness (.e goodness) clustering want small possible.kmeans(df object, centers = )\ncenters number clustersnstart, Select randomly k objects data set initial cluster centers meansfviz_cluster(kmenas object, data =, stand=F)ylim=c(90,101),xlim=c(17,27)Elbow MethodRecall , basic idea behind cluster partitioning methods, k-means clustering, define clusters total intra-cluster variation (known total within-cluster variation total within-cluster sum square) minimized:Average Silhouette MethodIn short, average silhouette approach measures quality clustering. , determines well object lies within cluster. high average silhouette width indicates good clustering. average silhouette method computes average silhouette observations different values k. optimal number clusters k one maximizes average silhouette range possible values k.2We can use silhouette function cluster package compuate average silhouette width. following code computes approach 1-15 clusters. results show 2 clusters maximize average silhouette values 4 clusters coming second optimal number clusters.","code":"\nset.seed(1234)\n# regresamos a df con dos variables\nteens<-read.csv(\"https://raw.githubusercontent.com/abernal30/ml_book/main/teens_clean.csv\")\n\nset.seed(200)\nteens_na<-na.omit(teens)\n#dim arroja el npumero de renglones y columas de un data frame\ndim<-dim(teens_na)\n# genera números del 1 al 27,276(dim[1]) pero solo arrojame 1,000. \nsamp<-sample(dim[1],10000)\n# Del objeto teens_na, toma solo las observaciones que hay en samp\nteens_2<-teens_na[samp,]\nteens_2[,\"gender\"]<-ifelse(teens_2[,\"gender\"]==\"F\",1,0)\n\nkm<-kmeans(teens_2, centers = 3) # centers es el número de clusters\nkm\n#> K-means clustering with 3 clusters of sizes 6462, 3032, 506\n#> \n#> Cluster means:\n#>   gradyear    gender      age   friends basketball  football    soccer\n#> 1 2007.457 0.7923244 17.29125  10.92990  0.2407923 0.2541009 0.2086041\n#> 2 2007.572 0.8393799 17.16449  54.81596  0.2720976 0.2697889 0.2407652\n#> 3 2007.684 0.8833992 17.04626 139.93281  0.4150198 0.2924901 0.2766798\n#>    softball volleyball  swimming cheerleading   baseball     tennis    sports\n#> 1 0.1301455  0.1245744 0.1267409   0.09826679 0.09640978 0.08851749 0.1355617\n#> 2 0.2272427  0.1662269 0.1625989   0.10125330 0.10850923 0.09762533 0.1513852\n#> 3 0.2114625  0.2213439 0.1739130   0.18972332 0.16996047 0.13241107 0.1403162\n#>        cute       sex      sexy       hot    kissed     dance      band\n#> 1 0.2960384 0.2113897 0.1371093 0.1112659 0.1066233 0.3760446 0.3009904\n#> 2 0.3700528 0.1903034 0.1444591 0.1408311 0.1240106 0.4584433 0.3509235\n#> 3 0.4940711 0.2272727 0.1916996 0.2055336 0.1047431 0.5968379 0.3300395\n#>     marching     music      rock       god    church     jesus      bible\n#> 1 0.04008047 0.7420303 0.2429588 0.4281956 0.2233055 0.1004333 0.01980811\n#> 2 0.05178100 0.7958443 0.2500000 0.5484828 0.3182718 0.1319261 0.02638522\n#> 3 0.05533597 0.8063241 0.2984190 0.5968379 0.3181818 0.1719368 0.02766798\n#>        hair      dress     blonde      mall  shopping   clothes  hollister\n#> 1 0.3977097 0.09702878 0.07985144 0.2375426 0.3156917 0.1429898 0.05880532\n#> 2 0.4383245 0.12368074 0.09795515 0.2935356 0.4317282 0.1711741 0.08806069\n#> 3 0.5000000 0.12648221 0.11660079 0.4090909 0.5158103 0.1897233 0.13833992\n#>   abercrombie       die     death      drunk      drugs    female no_gender\n#> 1  0.04750851 0.1838440 0.1055401 0.08511297 0.06128134 0.7923244         0\n#> 2  0.05969657 0.1830475 0.1236807 0.09333773 0.05969657 0.8393799         0\n#> 3  0.08893281 0.2233202 0.1561265 0.11660079 0.03754941 0.8833992         0\n#> \n#> Clustering vector:\n#>  2431  6066  6502 13824 12117 24475 15312  7001  1708 17003 10804 11121 10807 \n#>     1     1     1     1     1     2     2     1     1     2     1     2     3 \n#> 27619 22195  4847  9538 10596  7164 14959 23056 19150 15720 27411  2195  1532 \n#>     1     2     1     2     1     1     1     1     1     2     2     2     1 \n#> 19037 12023 23498  4391 12923  6294 15949 13870 12746 13674 11424  5520 11089 \n#>     2     2     2     1     1     1     2     1     1     2     2     1     3 \n#> 24004  4878  8597 28272  4194  1074  9703 29990 11684  4429 12434 10670  5125 \n#>     1     1     1     1     1     1     1     1     1     1     1     1     1 \n#> 18648  4775 19896 12840 15785 20303  6855  6750 25616 11094 19854 28672 22484 \n#>     2     1     1     1     2     1     1     1     1     2     2     1     1 \n#>  6252 10909 10445   463 22876 17491 11160 20665 26771 15920  2233 16588  7930 \n#>     1     2     1     1     2     2     2     2     1     1     1     1     1 \n#> 26723 22185 11417 13243 13497 23942  8199  4104 13908  3511  9869   208   145 \n#>     1     1     1     3     2     1     2     1     1     1     1     1     1 \n#> 29146  5537 12425  7125 23257 27736 14147   553  8920  9618 22619 19899 18157 \n#>     1     2     1     1     2     2     1     1     1     1     1     1     2 \n#>   922  6882  7852 15438   822 11075 13034  6845  4800 21070  8818 16906 18063 \n#>     2     1     1     2     1     1     1     1     1     1     2     1     1 \n#> 16306  1334 25513  2649 23638 21569  8784 11222  1697 29265 22466 14172 28942 \n#>     1     1     1     1     3     1     2     1     1     2     1     1     2 \n#>   197   414 10749 22547  2109 15494  3906  6320  3748 11103  6229  1541 12705 \n#>     1     2     1     2     1     3     1     2     1     2     1     1     2 \n#>  6023 12008  2016 27373 21315  4850 26635  8060 23516  8164  9982 16107  6400 \n#>     2     2     2     1     1     1     1     1     1     1     1     1     2 \n#> 14948 19632  1472 19975 20862 22456  5912 14552  2853   810  7547  6472 10697 \n#>     1     1     2     3     1     2     1     2     1     1     1     1     1 \n#>  6377 16337 23018 27842  8432 24856 18155  8638  7059   921 13500 20281 14469 \n#>     1     1     3     2     1     2     1     1     1     1     1     2     1 \n#> 21128  5063 16714 10665  9715  3943 25712 21264    33 19714 25508   416 21623 \n#>     1     1     1     2     1     2     1     2     2     1     1     2     2 \n#>   633 17138  9836 24871 25636 20227 29478  6745 15497 25204 17933 12251 24385 \n#>     1     1     1     1     2     2     1     2     1     1     1     2     1 \n#> 16382 13507  7456 16286 12276 15910 20238  4559 28088 21141 21377 20013 17557 \n#>     1     2     1     1     2     1     2     1     1     2     2     1     1 \n#> 18449 12817  7624 28994  2916 15834  5875  3657 27934 11694  1721  2847 23380 \n#>     1     1     1     1     3     1     1     1     1     1     1     1     3 \n#> 25512 17224  6806 26437 17819 16398 17127 10977 11063  7330 11199  8033  7660 \n#>     1     1     1     1     2     2     1     2     1     2     2     1     1 \n#> 29727  5497 10700 27900 26282  7601 21204 18390 22577 24695 16779 22394 19966 \n#>     1     3     2     2     2     1     2     2     1     1     2     2     2 \n#> 27998 11891 27408  1938  6336 14844 10707 21381  7720 10705  7997 26026 26752 \n#>     1     1     1     1     1     1     2     1     1     1     2     1     1 \n#>  8321  3299  7944 26505  4544 19503 28642 14947 15667 23198 13692  8605   419 \n#>     1     1     2     2     1     1     1     1     2     2     2     1     1 \n#>  8229 16102   895 21345 26103 12579 13625 19065 29887 11452  6693 23211  4218 \n#>     1     1     2     1     1     1     1     2     2     1     1     1     1 \n#> 15730  9481 11202 16879 12333 25525 27369  3214  2753 18837  9718 10427 26482 \n#>     2     1     1     1     2     1     1     1     1     2     1     2     2 \n#>   468 12944  8463 27174 16776 19901 20810   894 29788  7811 26331 12955 22701 \n#>     2     1     1     1     1     2     2     2     1     1     2     1     1 \n#> 24726 17031  2859 14816 11480  7305 25305 12499 29516  2397 15677 10586 19312 \n#>     2     1     1     2     1     1     2     2     1     1     1     2     1 \n#> 11106 20574 20230 15923  7135  7909 26345 24905 28107 27384   738  3897 20500 \n#>     1     1     3     1     1     1     1     1     1     1     1     2     1 \n#>  1480  7700 27960 27462  3680 17107 16852 29032 23859 10200 27226  9873 22941 \n#>     1     1     2     1     2     1     1     2     1     2     1     1     2 \n#>   723 22626 11092 22624  2723 15251 17481 22898 22234 25501  7940 28372 25668 \n#>     2     1     1     2     1     1     2     3     1     1     1     1     1 \n#>  3863  5935 13018  5147 26150  4818 16113 12244 22147 25935 10952 22800  9197 \n#>     3     1     2     1     2     1     2     2     1     3     2     1     1 \n#> 26738  2947 25757 23775 11774  1580 10790 23682 17106 23807   783 21188  5918 \n#>     1     1     1     1     2     1     1     1     2     1     1     1     2 \n#> 15715  5828  4678 23576 10892 10530 22092  7172 20107  2698  1800 10100 16320 \n#>     1     1     1     2     1     1     2     1     2     1     1     1     3 \n#> 26104 23849 14733  8107 12154 12514 15824 13933  7680 11329 10820 26792 26648 \n#>     1     1     2     2     2     1     1     1     1     1     1     1     2 \n#> 12018 24481 25323  3439  5737 10650 16294  1979   241 12207 18612 20004  9383 \n#>     2     1     2     2     1     1     1     2     2     1     2     1     2 \n#>  3889 24790  3042  3174 20479  2255 13394 23667 21779  9911 19627  3220 10935 \n#>     1     1     1     3     2     1     2     1     2     3     2     1     1 \n#> 22740 17090 20326 13390 20923 17102  2641  2682 18040 10823 23980 24647 10486 \n#>     1     2     2     1     2     1     1     1     1     1     1     3     1 \n#> 20422 25456 20741  1061 27957 20713  6396  8271  5982  9945 26275 26891 21477 \n#>     1     1     1     1     1     1     1     1     2     1     2     1     1 \n#> 15860 26622 20287 10246 27056 17586 18581 20249  8155 19461 16824 21912 18357 \n#>     2     2     2     2     1     2     1     2     1     2     2     2     2 \n#> 14626 14779 29485 10648  2294 11158 19939 27034  8240 13139 12255 24389  4859 \n#>     1     1     2     2     2     1     1     1     2     1     1     1     2 \n#> 21887 26043 25967  5233 25369 16971  9692  3298 17000 10462  5105   587 11017 \n#>     3     1     1     1     3     1     2     1     1     2     1     1     3 \n#> 13521 21548  6299   261 28534 24933 18631 28363 12917 19490 11143 26765 24931 \n#>     1     2     1     1     1     2     1     2     1     1     2     1     1 \n#>  7990  3547 28783  3300  1308 27035 17861 14293 27244 26588 13894  8876  3449 \n#>     1     1     2     1     1     2     1     1     2     1     3     1     1 \n#> 20794 25057 20530 18629  8044  4630  9874 26569 21446 22227  6440 18409 20062 \n#>     2     1     1     2     1     1     2     2     1     1     1     2     1 \n#> 12906  7470 10677  4944 12106  9527 27151 23166  5008 19752 26189 19010 12939 \n#>     1     2     1     2     1     1     1     2     1     1     1     1     1 \n#> 27999 15976 27353 14341  3125 12298 10720 27941 20957 24951  5284  4858  9653 \n#>     1     2     1     1     1     1     1     3     2     2     1     1     2 \n#>  2235 21876  4173  4827 26203 17498 28552 12329  9554  7953 13315 20170  9033 \n#>     1     2     2     1     2     3     2     1     1     2     3     1     1 \n#> 19644  8950  8914 19401 15589 12974 16531  9458  9544  2107 24245  6575  2578 \n#>     1     2     1     2     1     2     1     1     2     1     1     1     2 \n#> 20419 25662 17163 11536 18759 13288 15623 16523 17987 10455 12576 13111 14728 \n#>     1     1     2     1     1     1     1     3     1     1     2     1     2 \n#> 21403  2062  6580  2730 10794 11821 18224 16607 16790 21237 23561   769 28480 \n#>     2     2     2     1     1     1     2     1     2     1     1     2     1 \n#> 22252 14842 29275 24151  3197 16400  3925 10866 14856 23903 16343  1594 10838 \n#>     2     2     3     1     2     1     1     1     1     1     1     1     1 \n#> 29255  5143 27469 28950  9381 11184  1879 15222 25246  6674 19829 25524 23004 \n#>     2     1     1     3     1     1     1     1     1     1     2     1     1 \n#> 19733  6793 23513 11857 28396  9942  6018  9434  1474 29746   491  7254   128 \n#>     1     1     2     1     1     1     1     1     1     1     1     2     1 \n#> 12286  6862 22111 15728 20912 16372 27367  4270  2639   349  9268 10659 12422 \n#>     1     1     1     1     1     2     1     2     1     1     2     1     1 \n#>  4497 15337  8433 27572  3444 21850 25707 19391   239 27393 10859  6819 24275 \n#>     2     2     2     2     1     1     2     2     2     3     1     1     1 \n#>  7790 27242 12501 23628 29219  5060  6951 25256 26010 20703 20245 19709  6981 \n#>     1     1     1     1     1     2     2     2     2     1     1     1     1 \n#> 21275 28973 22987 13181 24918 25296 13817 22564 20643 21905 26907 26335 23252 \n#>     1     2     1     2     1     2     2     2     2     1     1     1     2 \n#> 24346 22821 10397 10396  9349 23302 25003 29683 27573  8330   859  3145 11448 \n#>     1     1     1     1     1     1     1     2     2     1     1     1     1 \n#> 17570   806  6579 25301 29787 16869 21480  7820 20219 20190 25322  6423   601 \n#>     2     1     1     1     1     2     1     1     3     1     1     3     1 \n#> 17720 26689  4239 25385   630 13544  3501 10131 28636 16873 15190 26913  6567 \n#>     1     1     1     2     1     2     2     2     1     1     1     2     1 \n#> 11852 12088 11347 27622 22774 18107 16048  5629  6195 15499 18385 12951 20453 \n#>     1     1     1     1     1     2     1     3     1     1     1     1     1 \n#> 20617 23364  1919 15950 12123 21349  4066 13121 10900 16934 26094  9079 22669 \n#>     1     1     1     1     1     1     1     2     1     1     1     2     2 \n#>  2509  4199  8505  6498 22703 24107 10619 13784  6045  5847 26633  9625 11679 \n#>     1     1     2     2     1     1     1     3     2     3     2     1     3 \n#> 25956 26506 26685 15320 12248  8401 16315 18233  9606 16387 27097 18874  9454 \n#>     1     1     2     2     1     2     2     1     1     1     1     2     1 \n#>  2432  3987  3702  3380  3354 16569   772  7739 29236 23482  4215 26095 29118 \n#>     1     1     1     1     1     1     1     1     2     1     2     1     3 \n#>  1789 16155 11733  1385 21254 10793 12230  8441 25263  1205 27339 12678 25013 \n#>     2     1     1     1     2     1     1     1     1     2     1     1     1 \n#>   770 23200  6706  1383  1139  4262 10587  5313 22554  7507 28795 14613 22783 \n#>     1     2     1     2     1     1     3     3     1     1     2     1     1 \n#>  6817 18736   945  3836  6205 21360 21937  5411 21499  8247 23962 12727 19594 \n#>     1     2     1     1     2     1     1     1     1     2     1     2     1 \n#>  1305  3934 20580 23676  7627  8600 12272 17790 25294 20869  8446  7908 21355 \n#>     1     1     2     1     1     1     1     1     1     1     3     1     2 \n#>  7878 14485  6756 19676 18304    12  5419  1891  5512 28513  5243 17284  6285 \n#>     1     2     1     2     2     1     2     1     1     1     1     1     1 \n#>  7063 15593 26573 19725  5571 27727 11392 22024 29055 27915 19044  7352  7492 \n#>     1     1     1     1     2     3     1     2     2     1     1     1     1 \n#>  7086 22684 17293 20341 16448 29460 19194 24167 10436 20031   870  1113 29240 \n#>     1     2     1     3     1     1     3     1     2     2     1     1     1 \n#> 27525 26922 17993  9345 24669 28315 12735  2092 23858 22062 26868 18843 28594 \n#>     1     1     2     1     1     1     2     2     2     1     1     2     1 \n#> 28256 10625  1613  2017 26538  9461 15632 22599   741  9331 27712 22379  1243 \n#>     1     2     1     1     2     1     1     3     2     2     1     1     1 \n#> 10184 28181 24800 24995  1376   215 20117 27809 24442  6249 10324  6291   856 \n#>     1     2     1     1     2     2     2     1     2     2     1     1     2 \n#> 20835  9482  9993 10157 18970 13190   341 17234 10208  2224  2677 28635 26194 \n#>     2     3     1     1     3     2     2     1     2     1     2     2     1 \n#> 24954  5861 24152  9970 19595 18813  4675 24517 16858 13249 12090 21608  9013 \n#>     1     1     2     1     2     1     1     2     1     2     2     1     1 \n#>  6138 23564 10782 12924   958 11909 24242   609 22080 23293  9521 18899 11552 \n#>     2     1     2     2     1     2     1     1     1     1     2     1     1 \n#> 14047 12263 12850  8046 11980 26647 15913  9591 12332   729 18210 11945  4705 \n#>     1     3     1     2     1     1     2     1     2     1     3     1     2 \n#>  8642 23749 20378 29270 29711 25254 26578  9276  2900  6917 23025  7163 23494 \n#>     1     2     1     1     1     2     1     2     2     1     1     2     1 \n#>  7270  7211 25776 24308 16027 11855  4801   326 26110  1694 10515 26974  2775 \n#>     2     2     2     3     1     2     1     1     2     1     1     1     1 \n#> 18610 15261 15831 20146  8714  9735 10970  4629 26180  1013  1915 11084 16052 \n#>     1     2     1     2     2     1     1     2     1     1     2     1     1 \n#> 11078    63   717 17027 26328 29411  4346 18215 27861  4281 19976 21037 11491 \n#>     2     1     1     1     2     1     1     1     2     1     1     2     1 \n#>   106 15237 23416  3217  6667  2177  7979  4489  7397 17840  1873    27 25165 \n#>     1     1     2     1     1     2     2     2     1     1     2     1     1 \n#>  5963 11673 10321 14340 24586 24205 12844  8280 12967  4529 17394 12910 27277 \n#>     2     1     3     1     1     2     2     1     1     1     1     1     1 \n#>  8324 25868 17171 21219  4969 26540 18262 21307 16530   759  2211 24176  8298 \n#>     2     2     1     2     1     1     1     1     2     1     1     1     1 \n#>  8367 17059 27782  2803 23523 20256 20972 21191 11682 15065  2193 13635  1159 \n#>     1     1     1     1     1     2     2     1     1     1     1     2     1 \n#>  7343  8482 20887  2818   929 13626 20687 18725  5871 28453  6452  8087 11316 \n#>     1     1     3     1     1     1     1     1     1     1     1     1     1 \n#>  3276 14250 12754 17033  3175 22403   955 11474 29899 13573  5296 17485 12634 \n#>     1     1     2     2     1     1     1     1     2     1     3     1     1 \n#> 26533 13912  7042 13296  3572 17730 27454 12048 27602 18536   162 11784 24860 \n#>     1     2     1     2     2     1     1     2     1     1     2     2     1 \n#> 25219 21420 10779  5282 27503  1835  3673 28869 20919 16210 13185 29168 24030 \n#>     2     1     1     2     1     1     1     2     2     1     1     1     1 \n#> 20609 29657 10913 12890 28953  3065 23986 20570 12726  9800 23437 10981 21862 \n#>     1     1     1     1     1     1     2     1     1     2     1     1     1 \n#> 19615 19792  3163 21883 16366 11535 12391 25068 11800 13356 18670 12737 15521 \n#>     2     1     1     1     3     2     2     1     1     2     1     2     2 \n#>  2740 13490 26188 14279 28748  9599  6405 12282 29623 25269 11836 28680 13891 \n#>     1     1     2     1     1     1     1     1     1     1     1     2     2 \n#>  2896 23040 14328 21331   497 22428 26446  8454 29001  4487 13319 25255 18451 \n#>     1     1     1     1     2     1     2     1     1     1     1     3     1 \n#> 14663  5958  9271 27690 21202 23420 16727 20795 15815 20856  8901  8552 28024 \n#>     2     2     1     1     2     3     1     2     1     1     1     3     1 \n#>  9894  7222  5170 10966 28483  2180 14197 26247 15435 24093 15198 24674 27894 \n#>     1     1     2     1     1     1     2     1     2     1     2     2     1 \n#> 29125 17531   290 24867 19717  7524 17203 22541 28611  1709 18415 22381 13514 \n#>     2     1     2     1     1     1     1     1     1     2     2     2     2 \n#> 23879 27773 11163 29743 21178 12829 17245 13608 15500 22455  5665 15101 15112 \n#>     1     2     1     2     1     1     1     1     1     1     1     1     1 \n#> 11203 28920  5793  1806 20685  1140  2749  8242  1081 15899 27489 22445 14037 \n#>     1     1     1     1     2     2     1     1     1     2     2     2     1 \n#>   679 23850 26438 12854  9500  5801 27776  9255 24462 12209 20738  8527 21493 \n#>     1     1     1     1     2     1     1     1     1     1     1     2     2 \n#> 25186   828 15209 15930 27863 23270 19285  8934 22336 24132  6612  1258 13737 \n#>     1     1     1     1     1     3     2     1     1     1     1     1     2 \n#>  2237 18687  6261  3014  9120  2827 24453 13864 10372 14934  5252 25193 16983 \n#>     1     1     1     2     1     2     1     2     1     1     2     1     1 \n#> 27047 20735  4989 24460 26572  3590 12960  2270 28727   965 29213 14825 28965 \n#>     1     1     2     1     1     1     1     1     1     2     2     1     1 \n#>  4913 21979   805 23171  6391 12469 12443 16882 27312 26674 14843  9470  8416 \n#>     1     1     1     2     2     1     1     1     1     2     2     1     2 \n#> 16023   460 19320 12509  9092 16031 10284 12704 11273 15331 11438 24640 16860 \n#>     2     2     2     1     1     1     2     1     1     2     1     3     2 \n#>  4337  8261  7390 24948 25044 15394 12374 13035 25596  3785 13585  2253  5975 \n#>     1     2     2     1     1     1     2     1     1     1     1     1     2 \n#> 19884 28859 17600 13244 27308  5931  9021  7463 21422  8322  4368 19403 18009 \n#>     1     2     3     2     2     1     1     1     2     1     1     1     1 \n#>  5762  9686 28645 18710  5301  7327 15086 13247 22414 29805 11921 11753  8484 \n#>     2     1     3     1     1     1     1     2     1     2     2     2     1 \n#> 11946 13670 18130 18934  8054 12182 11657 25511 29878 20017 23457  2633 12530 \n#>     1     1     2     1     1     1     2     1     1     2     2     1     1 \n#>  9329  3922  2462 19354 18669  8163 28246 14682  3935 24268 12114 22523  5779 \n#>     2     1     1     1     1     1     1     3     1     2     1     1     2 \n#> 18980 14343 23631  5675  5767 19187 10400 11819 22488 13688 15678 28696 10243 \n#>     1     1     2     1     1     3     2     2     1     1     2     1     2 \n#>  1902 26304  2118  7401  6352 24897  9093 27996 11542  7648  9212  5615 11966 \n#>     1     1     2     1     1     1     1     2     1     1     2     1     3 \n#>   494  5714  3307 21573 18329 11835 17456  3811  2209  6663  6428 18657  3618 \n#>     1     2     1     1     1     2     1     3     1     2     2     2     1 \n#> 15258 23987 19887  1778  5892   233 14941 13733  8718  5449 17856  1804 13108 \n#>     2     2     1     2     1     1     2     1     1     1     1     1     1 \n#> 16671  9801  8450 27456 19040 16674  5477 25239  2580  4985 22679 26236  6275 \n#>     1     1     2     1     1     1     1     2     1     1     1     3     1 \n#>  5374 27399 21524  1582  3569 15392 29750 16505 18800 27555  2523 20867  9140 \n#>     2     1     1     1     2     1     2     1     2     2     2     1     2 \n#> 23247 20996 11805 15002 28425  8940  3010   112 27790 12575 28579  8459 27777 \n#>     1     1     1     1     1     2     1     1     1     1     2     2     1 \n#>  2502 15553 28422  8143 18436   787  7602 13008 18542  3105  7647 27588 20988 \n#>     2     1     1     2     2     1     2     1     1     3     3     1     2 \n#>  5215 25423 19148 25117 14676  7591 27216 23058 11259 13612  4646  9196  9674 \n#>     1     1     1     1     1     1     3     2     1     1     1     1     1 \n#> 27963  3856 28943 18293 27434  9831  6713 29051  6274 26599 13547  3417 19914 \n#>     2     1     2     1     1     3     1     1     1     1     2     2     1 \n#> 12822  5029 17359  5942  5515 17605 18522 12442  9638 25688 26831 12523 29237 \n#>     1     1     1     1     2     2     1     2     1     1     1     1     1 \n#> 15054 26687  8913 19246 25846  8371 18719 19198 22596   188  2592 19472  8766 \n#>     3     1     1     1     1     1     1     1     3     1     1     2     2 \n#>  9900 12412 17415 27500   890 26368  4784  1174   424 17900 14479 25670 27362 \n#>     1     1     1     3     1     1     2     1     2     1     2     2     1 \n#> 19583  2989 27698 17676 28384 20162 22593  9886  1514  7710  5334 14365 15900 \n#>     1     1     2     2     3     1     1     2     2     3     2     1     2 \n#> 18463 10635 20619 22995 22530  3438 22122  7576 14275 13647  4459  1315  6798 \n#>     1     1     1     1     1     3     1     2     1     1     2     1     1 \n#>  4561  3462  3810  4309  5424 18367 10500   887 20288 27496 10673 11572 23808 \n#>     2     2     1     1     1     1     1     1     1     2     1     3     2 \n#> 14493 28737 24519 23852 27601  3245  9681 11904  1198 17741 23487  2352 22608 \n#>     1     2     1     3     1     1     1     1     1     1     1     2     1 \n#>  2031 23191 15469 14061 19242 15287 18091 11860  6392  5934  7755  7686 27058 \n#>     2     2     1     1     1     2     1     1     1     1     1     2     1 \n#> 24545  2408  3496 16473 25361 11678  9822  9016 27873 26862 18411 19410  7501 \n#>     2     2     1     2     1     2     1     3     3     1     1     1     2 \n#> 21735 17431 29763  6475 20271  2752 10244 13808   648 26680 25387 19267 28955 \n#>     1     1     1     1     2     1     1     1     1     1     1     1     1 \n#>  4072 26185  6137 22587 11853 21981 16868  1836  6737  3896  3433 26019 18595 \n#>     2     2     2     1     1     1     2     1     1     1     1     1     2 \n#> 16896 15141 18549  6358  5800  7900  1248  4250  9811 29349 27488  4033 12455 \n#>     1     1     1     1     3     1     1     1     1     1     1     1     1 \n#> 24270 21818 20656 29758  6592 25789 17058  8486  7692  6482  3334  3746 29079 \n#>     2     1     1     1     2     1     1     1     1     1     3     1     1 \n#>  6527 14357 18330 10287 21852 20338 10266  4927  7233 17330 27735 20948  4607 \n#>     1     1     1     2     1     2     1     1     2     2     1     1     2 \n#> 21028 17816 19497   152 24229 11520  7931  7262 28527  7518 18399 24907 29003 \n#>     2     2     1     1     1     1     2     1     1     1     1     3     1 \n#> 25444 15176 26827 28144 17403 20589  5974  3204  1513  6803 21738 10446  8726 \n#>     1     1     1     1     1     2     1     2     1     1     3     1     1 \n#>  6544 25540  3827 13054  4355  5738 21666  4321   363 14149  7350 27591   136 \n#>     1     1     1     1     2     1     3     1     1     1     1     2     1 \n#> 13089  4090  5380 10948 24361 21107 27211   766 11425  7219  3720 13058  5628 \n#>     1     1     1     2     1     1     2     1     2     1     1     1     1 \n#> 29993 28553 22750  1864  6259  8051 18911  9243  7255  2004 20120  5495 15734 \n#>     1     1     1     1     1     2     2     1     1     1     2     1     1 \n#> 25225  8725  5418  8620 20025  9028  3166 27129 13628 13566 10638 11306 27967 \n#>     1     2     3     3     1     2     1     2     2     1     2     1     2 \n#>  7709 19166  5322 23225 15166 24347 11125 26276 27502 18651  5134   127   562 \n#>     2     1     1     1     1     1     1     1     1     2     1     1     3 \n#>  5697 18730  5678 15516 26519 13208 10485 25224  1336  6559 13657 24532 11147 \n#>     2     2     1     1     1     1     1     1     2     1     1     2     1 \n#>  2600 24662 11845   910 15791 15757 10828 27223 23560 25393  5152  2174 19484 \n#>     1     2     3     1     1     1     3     1     1     1     1     1     1 \n#> 22015 26916 10273 13331 25278 16517 17360  9842 12376 16661 27933  2484 26390 \n#>     1     2     1     3     2     1     1     3     2     2     1     1     1 \n#> 19801 17197 15295 16369 21218  9412  2934 12894 24491 13506 21807 25424  3541 \n#>     1     1     1     1     2     2     1     2     2     1     1     1     1 \n#>  9414 23648  1119 24928 11404 22476  2568  5533 29318 29065    47 11991  9172 \n#>     1     1     1     1     1     2     1     1     3     2     1     2     1 \n#> 17175 15224 25962 22576   199 14595  6147 20766 12660 10250  4286   181 24712 \n#>     1     1     1     2     1     1     1     1     2     1     1     1     1 \n#> 20019  4186  6576  2481 14155  1632 28151  3686 14617 27961 16341 15038 20676 \n#>     1     1     2     2     1     1     1     1     1     1     1     1     1 \n#>  3258 13977 12900 12742  6185 27512  6624 28234 14809  7026 19311 26951 21990 \n#>     1     2     1     1     2     2     1     2     3     2     1     1     2 \n#>  3073  1273 24454  1414  8938  4116  6879  8048  4301  8962  4921  8857 16486 \n#>     1     2     2     1     1     2     1     1     1     1     1     2     1 \n#> 11653 14057  4679 26112 17839    18  6015 16392  3390 26587  9623 10468  8437 \n#>     3     1     2     1     3     1     1     1     1     2     2     1     3 \n#> 14738  8615 21013 29414 10367 24924   672  4531 10392 15074  9462 10378 27593 \n#>     1     2     2     1     2     1     1     1     2     1     2     1     2 \n#> 10980  3136 18774 29961 18230 21729  7273  4251 12294  3052 25253 19797 10224 \n#>     1     1     1     1     1     1     1     1     1     1     1     1     1 \n#> 20728  7426 12121 16447  6330 12615 12313 21995 16942 28019 16700  6894 26313 \n#>     1     1     1     1     2     1     2     2     1     1     1     1     2 \n#>  5539  5263 13501  3732 17467 10976  5165 25172 23336  8819 16985 17972  1551 \n#>     1     1     1     2     1     2     1     1     1     2     1     2     1 \n#> 15813 12681 25139 16745 19207  9526 29370  9732  7708 19208   352 21412  9540 \n#>     2     1     2     2     2     2     1     1     1     1     1     2     2 \n#>  3760 25258 12761 15889  1786 27965 19381 23170 17731 18179   559 22954 29088 \n#>     1     1     2     1     1     1     1     2     2     1     1     1     2 \n#>    45  9018 21947  5353 22861 15670 18423  5054 15133 26334 17042  3321 27179 \n#>     1     2     2     2     2     2     2     2     1     2     1     1     2 \n#> 26797  4267 11058 20343 13731  1017 29453 26594 21709 17357  8387 11119 29986 \n#>     3     2     1     2     1     2     2     1     1     1     1     1     1 \n#> 10503  1809 16846 13140 14383  1340 15703  6090 23865  7958 28196 28889 28430 \n#>     1     1     1     1     1     1     2     1     1     1     1     1     1 \n#> 17647 27505 13005 10870  6679 17348   589 22387  9479  4037  7428  3003   360 \n#>     1     1     1     3     1     2     1     1     1     1     1     1     2 \n#> 27117 22749 12188  8727  6807 15997  7117 28124  2379 15574  5455 20922  3434 \n#>     1     1     1     1     3     2     2     1     2     1     1     3     1 \n#> 20495 24298 23179  9350 15965 22467  2033 10023  7147 17063 12157 29361   611 \n#>     1     1     2     1     2     1     2     1     2     3     2     1     2 \n#> 13064   449 14004  2812 27518 19694 15051 15160 28039 18526  3587  3350 15036 \n#>     2     1     1     1     1     1     1     1     1     1     1     1     1 \n#> 28661 15083 18061 14580 10787 16766 25171 20193 22235 22929  6814  3060 25166 \n#>     2     1     1     2     1     1     1     1     1     2     2     1     1 \n#>  4053  3712 13006 11706  2451  9011 12857 17111  1860  7787 14769 10604 12773 \n#>     1     1     1     1     1     2     1     1     3     1     2     1     2 \n#> 25340 23203 22803  9551 13207 16175 23992  3397  6136 24613 12625 21374 17721 \n#>     1     1     1     1     1     2     2     1     2     1     1     1     2 \n#> 16427 18848  1882 14576 28659 20755 11605 17644 22826 19730 23423 22986  3737 \n#>     1     1     1     3     2     1     1     1     1     2     1     1     2 \n#> 27115 15540  7599 15729 20426 13277 15556  2835 26906 25823 15170 10115 19345 \n#>     2     3     2     2     2     1     2     2     3     3     2     2     1 \n#> 29633   568 16069  5395 24063 18852 21231 13444 15161  2937  9400  5606 16057 \n#>     1     1     1     2     1     1     1     2     1     1     1     1     3 \n#>  8748 21495 23929 13509 13609  1209 19321   984  1617 12833 19522 14895 16591 \n#>     1     1     2     2     2     1     1     1     1     2     2     1     3 \n#> 25871 26683  1212 12223 10093 15102 16450 24604 18271  5042 27450 10191  7928 \n#>     1     1     2     1     2     1     3     1     1     2     2     2     2 \n#> 25570 28056 19520  5685 22388 27862  7760  9755 15753 11277 20540 21452 14544 \n#>     1     2     2     2     1     1     1     3     2     1     1     1     3 \n#> 14794 20783 19618 24896 25072 26748  6554 13916  6972  5670 21580  9225  9838 \n#>     1     1     1     3     2     2     1     1     2     1     1     1     1 \n#> 28585  2376  3148 13723 20840  9672 26338  3362 10932 23290  6578 19685 25969 \n#>     2     1     2     1     1     1     1     1     1     1     1     2     1 \n#>  7353 29732 28966 11216 19856 15805 16596 22210 27529  1442 15787  4326 11914 \n#>     1     2     1     1     3     3     1     2     2     1     1     2     2 \n#> 22893  6859 13238  3972 13227 26629 21142 23818 19175  1097  2935   799 19014 \n#>     1     1     1     2     1     1     1     2     1     2     2     1     1 \n#> 23313 14696 18012 17323  7445 14639 26291   667 16014  1795 13312  6042 13632 \n#>     1     2     1     2     1     1     1     1     2     1     1     2     1 \n#> 16202 17590   440 13373  6868 28010 19665 11447  8730  7641  4626  6820 19782 \n#>     2     2     1     1     2     2     2     1     1     1     1     1     1 \n#>  3219  9325 28574 10836 22347 26884 27916  6101 19088  8921 20011 14358  5508 \n#>     1     1     2     1     1     1     1     1     2     1     1     1     1 \n#> 17183 18517   382 21000 22163  4415  7657  5064 21344 11584 16277  5703  5745 \n#>     1     1     1     2     2     2     1     2     1     2     1     2     1 \n#>  3668 18895 25228 10808 25314  3339 19344 23069 19237 22776 10654 22130 15622 \n#>     1     2     1     1     1     1     1     1     3     1     1     2     1 \n#> 18746 24084 20549  2103 13943 10598  1021 22434 25131  5473 15779 27598 22889 \n#>     1     1     2     1     1     1     2     2     2     1     1     1     1 \n#> 27891  3617 21720  7506 14655  3094 29121 17212 29057 25405 27876 13666  9292 \n#>     2     1     2     2     2     1     1     3     1     2     1     1     1 \n#> 16759 21932  1918  9744 14752 23255 19558 20979 10265 29949  4857  5228   947 \n#>     1     2     1     1     1     1     1     2     1     1     1     2     2 \n#>  6772 29651 25192  9159 13576 14570 14308  4293 26118 29269 13548 27247 19055 \n#>     2     1     1     1     1     1     1     1     1     2     1     1     1 \n#> 15460 25273  6003  9558 16722  2567   517  6076 23601  7821 28079  8461 12580 \n#>     1     1     3     1     1     1     1     2     1     1     1     2     1 \n#> 19436 29512 24745 23090  9957  1394  8581 15482 13061  6515 22330 18319 18916 \n#>     1     2     1     1     2     1     1     1     2     2     1     2     2 \n#> 14276 20773 21222 21194 23373 21858 10411 20429 20666 11301  1872 20528  1854 \n#>     2     2     1     1     1     1     1     2     1     1     3     1     2 \n#> 23261 26980  9915  6786  3127 22027  5548    85 19701 15379 17105 12536 28464 \n#>     2     1     1     2     2     1     3     2     1     1     1     1     1 \n#> 21551 21125 13570  2354  1232 21956 27486 10202  7198 26486  9072  2991 16808 \n#>     1     1     3     1     2     1     1     2     1     1     1     1     2 \n#> 18545  1080  1166  1525 29945 29338  7582 10295  7549 22633 21205 13400  4670 \n#>     2     1     2     2     1     2     1     2     1     1     3     1     2 \n#> 26080 11988  9743 26764 15852 16597 24430  6910 18718  2538 16731 16818 22714 \n#>     1     1     1     1     1     2     1     1     2     1     2     2     2 \n#> 15545 17581  3892 29112  8705 13342 11716  1965 21098 29047 27546 17682  8378 \n#>     2     2     1     1     1     2     2     1     2     1     2     1     1 \n#> 23571 13071 17656 20469 22218 29777 29131 20934 26579 14528  1379 14912 16168 \n#>     2     3     1     3     1     1     2     1     2     1     2     1     2 \n#> 13389 20886 12723  5366   784 12702   245 25112 21166 10376  6533 15538 24609 \n#>     1     2     2     2     1     1     2     2     1     1     1     1     1 \n#> 24814 28941  1609 15833 21639 13890 24042 17893 22785   656 14521 29765  8031 \n#>     1     1     1     1     1     2     1     1     1     1     1     1     1 \n#> 19180 23773 28153 27826 14023  1628 20040 26805 12564  8022  3059 22869 17263 \n#>     1     2     2     1     1     2     1     1     1     1     2     1     1 \n#> 18021  8334  8200 29700 15303 28711 26857 17722 26888 10907 19985 20454 11969 \n#>     3     1     1     2     2     1     1     2     1     2     1     1     2 \n#>  8989 12300 18539  1807 22441 25537 25761 21033  3642 11193 19514  8724 13396 \n#>     2     1     2     1     1     1     1     1     2     1     1     2     2 \n#> 11761   597 13734 28185 13845 29227 10032 13876 20888 18527 26217 22868 23133 \n#>     3     2     3     1     1     1     2     2     1     1     2     2     1 \n#> 18438 25760   223 21461 25888 17630  5227 23767 20615 11321 23592 16883 19956 \n#>     1     1     1     2     2     1     1     1     1     2     1     2     1 \n#> 29004  6692 27506 11686 26251 21392 21026  9676 14887 26009  4255 18822 23829 \n#>     1     2     2     1     1     2     1     2     2     1     1     1     1 \n#> 26241   621 19365 10264 29230 26149  8941   743 10692 16831 23121 22814 11979 \n#>     1     1     1     1     3     1     1     1     2     1     1     3     2 \n#> 13062  4165   946 29831 25076  3266  1463   285  8254 12280 29935 16209 15216 \n#>     2     3     1     1     2     1     1     1     1     2     1     1     1 \n#> 25497 10833 19586 18904  7427  2467 23274 17445  7082 19963  4010 24379 15332 \n#>     2     1     1     1     1     2     2     1     1     1     1     2     2 \n#>  9741 20970 20101 29246  4323 14487 11292 17080 20285 28622 20987 15347  2190 \n#>     1     1     1     2     1     1     1     1     1     2     1     1     1 \n#> 19259 15851 29390  8915  4898 27279  3850  2778  4313 16990 10152 14350 16787 \n#>     1     1     2     1     1     2     1     1     1     2     1     1     1 \n#>  9790 23324 24739 15207 29640  8080 22631 20654 12546 26776 20428 18278  6118 \n#>     2     1     2     1     1     1     2     2     1     2     1     1     1 \n#>  4507  1663 28444  9826 28697  2207 28254 19377  9933 16603 29559 25818 18248 \n#>     2     1     1     2     1     2     2     1     2     2     2     1     1 \n#>  1855 20933 26728 14372 13837 19890 26401 23545 29014  3110  9895  4669  1521 \n#>     1     2     1     1     1     3     2     2     1     2     1     1     1 \n#>  6199 16438 23567  6323  8575 20317 16295 25837 22935 16039 18205 22307 16440 \n#>     1     1     1     1     3     3     1     2     2     3     3     1     1 \n#>  8923  2867 12926 29041 11829  9533  7925 13676 28797 19641  6203 22569 28677 \n#>     1     1     2     2     1     1     2     2     2     1     2     2     1 \n#> 24170  8728 25451 24162  2873 23228  8664 18085  9014 23197 21314 10774 28582 \n#>     2     2     1     2     1     3     1     1     1     2     1     1     2 \n#>  3646 15837 17969 29144 10495  1764 26854  7963 17453  8847 22314 10972 16303 \n#>     2     2     1     2     1     1     1     1     2     1     1     1     1 \n#> 16967  4603  2576 18067 28852 15284 15411 19427  4684 17274  8361 12035  5306 \n#>     1     2     2     1     2     2     1     1     1     1     1     1     1 \n#> 16195 11197 25486 26736 18540  9216 23553 12428 29981  3610 24568 16878 12608 \n#>     1     1     1     2     3     1     1     1     1     2     2     1     1 \n#> 14669 26098 29458  8716  5391 15463  5971 20266 22926  1300 14438 10691    84 \n#>     1     1     2     2     1     1     1     1     1     1     1     2     1 \n#> 14195 11355  6288 28523 23357 29793  5636  6213 19405 29483 19724 13660 18821 \n#>     2     1     2     1     1     2     1     1     1     3     1     2     2 \n#> 15029 27758   596 14803 28360   372  1298 28497 22300 12558 14471 24866 27795 \n#>     1     1     1     2     2     1     1     1     2     1     1     1     1 \n#> 28047 13820  9614 29836 16592 19058 11918 10565 16582  6776 23292 21872 12485 \n#>     1     1     1     3     1     2     1     1     1     1     1     2     1 \n#> 15966 12647 16469 13406 26264 18950 10470  3489 15462 12161  5119 21122 27901 \n#>     2     2     1     1     1     1     1     2     2     2     1     3     2 \n#> 20358  1774  2684  2068 17401 18472 19631 21402 16997 11511  1732 28447 26066 \n#>     1     1     1     1     1     1     1     2     1     2     3     1     1 \n#> 26491 24947  6179  8574  1574 22582 13910 20217 19872 16379  7526 23906  5117 \n#>     1     1     1     1     1     1     2     2     1     1     1     1     1 \n#> 20137 13325 26959 16515  8835 24244 28278 24763 15619  1265 25119  5527 27726 \n#>     1     1     1     1     1     2     1     1     1     1     1     2     1 \n#>  8786  8659 16920 19240 11237 24488  3651 22579 20073 16964 27885 28271 25200 \n#>     1     1     2     3     1     1     1     1     1     1     1     1     2 \n#> 24677 19417  7864 28778 24394 18405 20564 14808 12673  6904 18291  7589 11848 \n#>     2     1     3     2     1     1     1     1     1     1     2     1     1 \n#> 29954 27422 21023 14603 24505 16669 22046  6757  3490  8695 20024 28743   373 \n#>     1     1     1     2     3     1     1     1     2     1     2     1     1 \n#> 24893  9392 13189 21841 22973 24456 17215 10837 19169 21702  3239 12888  3424 \n#>     2     1     1     1     2     1     2     1     2     1     1     1     1 \n#> 28221 10709  6011 22260 26454 22392 28257 15247 16215  4493  4208 15838   830 \n#>     2     1     2     3     1     3     1     1     3     1     1     3     1 \n#> 13287   653  2277 19952  1827  6061 24232  9142 11027 22707 13264  4439  9667 \n#>     2     1     1     2     2     1     1     1     1     1     1     1     1 \n#>  1256 23085  7206 23839  6309 16730  3189 28738  3515  4432  2229 14392  5324 \n#>     1     2     2     3     1     2     2     1     1     2     1     1     1 \n#>   396   366 15907 24395  6886 24335 22487  3436 26778  5691 10824 10769  6634 \n#>     2     1     1     1     1     1     2     1     1     2     1     2     2 \n#>  5656 28526  4824 15072   747  2678 23703 13323 20776 29254 23991 24314   684 \n#>     1     1     2     1     1     1     1     1     1     1     1     1     2 \n#>  4920 24413  5315 19696 21338 15925 23534 11457   664 29273  1616  9804 19587 \n#>     1     1     1     2     1     2     2     1     1     2     3     2     3 \n#> 10434  4242 16986  4099 26842  2604 19273 21613 28479 29511 10559   877 19657 \n#>     1     1     1     1     1     1     1     1     1     1     1     2     2 \n#> 23699  9940 20994 14533 21834 21833  1843  2736  3237  2268  6521 29639 19286 \n#>     1     1     2     1     2     1     1     1     2     1     3     2     2 \n#> 16203 11330 29241  1787 24117 24962 23335 13349 27363 10056 25226  6883 18943 \n#>     2     2     1     1     2     1     1     3     2     1     1     1     1 \n#> 15148  8608 19195 11977  2537  9824  2877 29680  4149 12065 24179 22545 22325 \n#>     1     2     1     1     1     1     1     2     1     1     1     2     2 \n#> 17495 21190 15220 24415  3958 18229 21147 17259 25349  2022  9371 27018 11144 \n#>     1     1     2     1     1     1     1     1     2     1     1     1     2 \n#>  7477 27046 22422  6660  9321 29029 29056  8162   255 12348  1603  4074 15146 \n#>     2     2     2     2     1     1     1     2     1     1     2     1     1 \n#> 16623 15123 29324 17377  6754 25401 23136 19274 10597  1155 25614 24734 12387 \n#>     1     1     1     2     2     2     2     1     2     2     2     2     2 \n#> 13772 12477  3353  6774   336 10299  2837 26843 12670  8644  6354 17587 23704 \n#>     2     1     1     1     1     1     1     3     3     2     1     1     1 \n#> 12983 18199 27064 14196 29322 23219  9424  8027  4260 25646  9650 22020 18715 \n#>     2     3     1     1     3     1     1     1     2     2     1     1     1 \n#> 18494   332 27090 29341 23277 11672  2048 18042 16511 13196 29714  1204 13694 \n#>     3     2     3     1     1     1     2     1     3     2     2     2     2 \n#>  9289  8075  4360 11718 15627 13627 25111 24203 12302 28977 27240 18316 20791 \n#>     1     1     1     1     1     1     1     1     1     2     2     1     1 \n#>  3926 25388  1338 20124 27695 24841 13941 15273 12884 18976 12862  4696 23922 \n#>     1     2     1     2     1     1     3     3     1     1     1     1     3 \n#> 15927 18387 10544 19402    75 19308 18065 26883   821  5091  1531  7489 24140 \n#>     1     1     2     1     1     1     1     2     1     1     1     1     2 \n#> 22371 17608 13013  6250 20165 12603  2446  5826 25964 19144   838  9049  5414 \n#>     1     2     2     1     1     2     1     1     1     1     2     1     1 \n#> 23428 16529 16144 12658  3084  8618  5420  9515 23517 20772 28995 24632 29596 \n#>     2     2     2     2     1     2     1     2     1     1     2     3     3 \n#> 22906 14667 26920  3023 13893 14900 20494   407 17977  1107 21669   358 11363 \n#>     1     1     1     2     1     1     1     1     1     1     2     1     2 \n#>  6808 19774  3652 24060 17952 27850 24929 24542 27297 27803 29499 12986 13214 \n#>     2     1     2     1     2     1     1     2     1     2     1     1     1 \n#> 16225 12179 14592  8827 25023 26405 13683 11165 25951   329  3367  8751   320 \n#>     2     1     2     2     2     1     1     1     1     2     1     2     1 \n#> 19755 16401  5281 17684 18300 20478 20616 19699 29378  1906 19625 15700 13328 \n#>     1     2     2     1     1     1     1     2     1     1     1     2     2 \n#> 12169  9081 11935 12995  5217 15342 20127 20705 14893 25128  6891 21343 12646 \n#>     2     1     1     3     2     1     1     1     1     1     3     2     1 \n#> 10073  1172   541  9082 21220   319 15727 22236 19017  6002 14417 12177  3325 \n#>     1     1     3     1     2     1     1     1     2     3     1     1     1 \n#> 19336  6449 24357 20628 16526 23329 11884 13000 24293 18138 19953 16090  6300 \n#>     1     2     1     1     3     2     1     1     1     1     1     1     1 \n#> 11423 26106 17711 14439  4831  4134 12550 29964   235  4971  3275 21881 22233 \n#>     1     1     3     1     1     1     2     2     1     1     1     1     1 \n#>  4207 12993  9386 22050 18990  5305  5605 20946 10967 19758 27633 21319 28385 \n#>     1     1     1     2     1     1     1     2     1     1     1     1     3 \n#>  9335 29189 21149 17015  3600  9176  1889 19469 20503   898 28306  6260 26022 \n#>     1     1     2     1     3     2     1     1     1     1     1     1     2 \n#> 15616 18195  7561   370 17757 27765 26856 27272 27621 15436 29348  8008 24027 \n#>     1     2     1     1     2     1     2     1     1     2     2     2     3 \n#>   392   668 14090 23361 10537 25289  2394 21487  3950  1863 27032 12016 16962 \n#>     1     1     1     1     1     2     2     1     1     1     2     2     1 \n#>  2465 29517  7644 16359  7165 20389    48 12806 19261 25406 27092 22212 19649 \n#>     1     3     1     1     1     1     1     2     1     1     1     2     1 \n#> 18870 29860 16677 27922 13897 22448  4783  2244  1615 23933 21685 29059 22657 \n#>     2     1     2     1     1     1     1     1     1     2     2     2     2 \n#>   785 11110 25686 27585  2329 27879  7490 13138  8126 29610  5235    79  9245 \n#>     1     1     1     1     1     1     1     1     2     2     1     2     3 \n#> 12895 25560  6362 26560 18812 22634  8360 22788   771  1331 24660 25450  5081 \n#>     2     1     3     2     1     1     1     1     1     1     1     1     2 \n#> 27397 15234  9420  7590 18769 21788  8596 11693 14868 24987 11744 18198 17551 \n#>     1     1     2     1     2     2     1     2     1     1     3     3     2 \n#>  8957 13947 10412 18386 25718 22815 16000  5021 28963 28787   694 26330 11974 \n#>     2     2     2     1     2     1     1     1     2     1     1     1     1 \n#> 29999 25624 25589 22078  4002 18005  9746 22978 17580 14423  1665  7318  2296 \n#>     1     2     2     1     2     2     1     1     2     1     2     1     1 \n#>  9958 13127 26652  7810 20414 25779 19357  8918 28210 21055  2768 24758 18510 \n#>     1     1     1     3     1     2     1     3     2     1     1     2     1 \n#> 14971 16249 18672 14478 21531 14903 14835 21410 23696  5171 10649 20524  2623 \n#>     2     1     1     1     2     1     2     1     2     1     1     1     3 \n#> 12364 26580 25739 23736  3769  6124 23104 23759 10710 29210 10432 23433 23488 \n#>     1     1     2     2     1     1     1     1     1     1     2     2     2 \n#>  9390 22258 23071  5943 16820 26983  3531 14754 23234 24797 19034  1130  8285 \n#>     1     1     2     2     1     2     1     1     2     2     1     1     1 \n#> 19803   275  8781 12077 11215 12769 27567  4441  5015 17800 10341  4577  3466 \n#>     2     1     1     1     1     2     1     2     1     1     1     1     1 \n#> 14872 14238 10011 23312  6889 10173 14092  6036 13786 17476 18071  7461 13333 \n#>     2     1     1     2     1     1     2     2     1     2     1     1     1 \n#> 27186 16520  3999  4566 14898 14650 17998 19456  7616  6846 21155 19617  7567 \n#>     2     1     2     1     2     1     2     1     2     1     1     1     1 \n#>  9983 12237 26487 11018 14481 10476 26206 27380  5869  5609 13483 29938  3013 \n#>     1     1     1     1     1     1     2     1     1     3     1     1     1 \n#> 14184 29733  6093  1323 12270  4876  7345 11053 18238 16482  5062 17133 17997 \n#>     1     1     2     1     1     1     2     1     2     2     2     2     1 \n#>   725 12913 15847 28862 14734   753 23847 15265 23073 13449  7752 25155 19516 \n#>     1     1     2     1     1     2     1     2     1     1     2     1     1 \n#> 15121  7748 15759 13131  7575 23694  5299 28624 27173 29325 29904  4734 17013 \n#>     2     1     1     1     1     2     1     2     1     1     1     1     1 \n#> 24809 28007 14601 29737 25835  8297 24435 28511 29852 23514  4457  2488 20826 \n#>     2     1     1     2     1     1     1     1     1     2     1     1     2 \n#> 11020  8177 29975 19447  5751 24473  4979 13117 26077 15929 17156 29139  3270 \n#>     1     2     1     2     2     1     2     1     2     3     1     1     1 \n#> 19297 28563 25724 25338  9387 16877  3173 22630 17039  4064 27084 24019 24443 \n#>     1     2     1     1     1     1     1     1     1     1     1     1     1 \n#> 29089  6063 14905 14865 14518 26526  8624  6933   123 26603  1302 11823 29647 \n#>     2     1     1     1     1     1     1     1     1     1     1     1     2 \n#> 13126 21940   654 16675 27383 29123 28812 26494 13206  9139 28880  2391 22946 \n#>     3     1     1     2     1     1     1     1     1     1     2     1     1 \n#> 13354  1677  6778  5855 27011 20362 19485  2911  6753 23054  9684 27847  9706 \n#>     1     2     1     1     1     1     1     2     1     1     2     1     1 \n#> 19284 21035 26931 10180 10124 29253  2504 24108  7717 14007 17962 27222 17196 \n#>     2     1     1     1     1     1     1     1     2     1     2     1     2 \n#>  9871  5079  3312  5108 18841 14283  8358 20891  2026 23833 12440  5172  7860 \n#>     3     1     2     1     2     2     3     2     2     1     3     2     3 \n#> 27517   308  7423 12786 25569  8113 23798 16854 13819   974 13952 19143  2764 \n#>     1     1     1     1     2     1     1     1     2     1     1     1     1 \n#> 24249  2389 17780   227 25709  6158  8062 27907 28602 16838 29994 22507  1060 \n#>     1     1     1     1     1     1     2     2     1     1     1     1     2 \n#> 28400 22473  9810 26576 13552 20081  2612  8590   635 19495 28247 26766 14958 \n#>     2     2     1     2     2     1     2     3     3     1     2     2     2 \n#> 23658 25158 14220  1910 28789 13658 22961  7292  6282 15692 24990 16112 21761 \n#>     1     1     1     2     1     3     1     2     1     2     1     1     2 \n#> 27419 25466 18483 19353  1452 25899  8312 24148 14393  7176 22772 10300 19386 \n#>     2     1     3     2     1     2     1     1     1     1     2     2     2 \n#> 17193  4102  5136 14182 22036  2530 22888 28374  1529  2606  3700 15174 29022 \n#>     1     2     1     2     1     1     2     2     1     1     1     2     1 \n#> 17473 10274 14353  9064 23570 20125 16570 15735 10267 19592 22585  9184  3920 \n#>     1     1     2     2     2     1     2     1     1     1     1     1     1 \n#> 27843 25559  6230 12849  8808 16801 24579 11596     6 12879 11263 19019  1260 \n#>     1     1     1     2     2     1     2     1     3     1     1     1     1 \n#> 27984 14190 22366  9043   855  2941  7030 29547 29917 13697 29837 24862 23001 \n#>     1     1     1     1     1     1     2     1     2     2     1     3     1 \n#>  6726 18862 17345  7888  1464 12997 19212  1325 19954  6954  8773  3757 18845 \n#>     2     2     1     2     1     1     1     1     1     1     1     3     1 \n#> 17571 22698   146 16414 18694 25266 15554 12718 13330  6084 27121  5219  8910 \n#>     2     1     1     1     2     2     1     1     1     2     2     1     1 \n#> 21100 26380  3160 13267 29662 19865 28986 21386 23265 21575 13168 18677 17411 \n#>     2     1     1     1     3     2     3     2     2     1     2     3     1 \n#>  5486 20977 25416 26917  3182 10105  1466 17789 28496 14873 11591 19493 21596 \n#>     1     1     2     1     1     1     1     1     1     3     3     1     1 \n#>  1769 20057 28242 24498 14992  9645 25059  9077 14342 20361 28471 12116  1278 \n#>     1     1     2     1     1     2     2     1     1     1     1     2     1 \n#> 10919 16840  1039  4310  8627  9301 12741 19886 26151 14374 26357 18421 22188 \n#>     1     1     1     1     1     1     1     1     1     2     1     2     2 \n#>   220 12596 22511  8053  3813 14609  8498   576 14068 25162 27524 17553 26933 \n#>     1     1     1     1     2     2     2     2     1     2     1     1     1 \n#> 10479 28468 16941 11862  4397  1698 17700 26248 18476  3997  6208 22844   432 \n#>     1     1     1     1     1     1     1     2     2     2     2     2     1 \n#>    60 12080  4984  4188 27804  4097 11547  9450 23801 13163 26800 26159 21496 \n#>     1     1     1     1     2     1     1     2     1     1     1     1     3 \n#> 29704 21139 25509  2828 12100 16803 26312 16084 16035  5185 17378 28032  6057 \n#>     1     1     3     1     3     1     1     2     2     2     2     2     3 \n#> 25923 10004 20210 23907  9004 27586 14826     1 21504 24332 24373  4948 18992 \n#>     1     1     1     1     1     2     2     1     1     1     1     1     2 \n#> 17121  9926 21408  6702 21929  4330 17885  1746 15799  8622  9368  6387  2813 \n#>     2     1     2     1     1     1     1     2     1     2     1     1     1 \n#> 16129  4388  2503  1667 16106 18573 26434 21354  9005 11866 28205  7571   776 \n#>     1     1     2     2     3     1     2     1     1     3     2     1     1 \n#> 12242 13903  8927 15617  8925  5817 21550  1526 19450 15906 15975  8348 18721 \n#>     1     1     1     2     1     2     2     1     1     2     1     2     2 \n#> 10771 16443 13758  9518  1689 24296   849 16289 10763 16562 19206  4047 16758 \n#>     1     1     1     2     1     1     2     2     1     2     2     1     1 \n#>  2720 29281 23623 17065 15859 20670 14730 14377 11315 17863 13675 12968 18162 \n#>     1     2     2     1     2     1     2     2     1     1     2     1     2 \n#> 18290 26393 19817 13416 19763 26388  5135   804 10852 22828 23969  2366  2950 \n#>     1     2     1     3     2     1     2     1     1     3     3     1     2 \n#> 17459 24427 23689 20511  7626 21063  5717 11808 16460 25910  6864 11722  8341 \n#>     1     1     1     1     2     1     1     1     1     2     1     2     2 \n#>  6418  2072 17379 27780  4673 17493 22949  8939 23743 18873 22245 28627   163 \n#>     1     1     1     1     1     2     1     1     1     1     1     1     1 \n#> 25330 10059  9909  1419  9541  2586 21598 11007  2061 17281  5690 14720 26939 \n#>     2     1     2     1     2     1     2     1     1     2     1     2     1 \n#>  1341   354  5259 22508  9698 26139 10356 21830 10914  2436  4443  4111  6435 \n#>     2     2     2     1     2     1     1     2     1     1     1     1     1 \n#>  6148  1650 29883  1314   711 21683 17417 25794 17627 21973  8219  3984 18833 \n#>     1     1     1     1     1     2     3     2     2     2     2     1     1 \n#>  2962 20257 24510 23111 13920  9295 15808 18461 26132 13518 16721 19828  8427 \n#>     1     1     1     1     2     3     1     3     1     1     1     2     3 \n#>  7149 18310 12623  1851 22132 15475  3109 10277 19494 28115 27659  6797   595 \n#>     1     2     1     1     1     1     2     2     1     2     1     1     1 \n#> 23411 10893 18954 25182  7244  4445 28358  4318 24966 26179  8466 16599 14913 \n#>     1     1     2     2     1     1     1     1     1     2     1     1     1 \n#> 24560 29362 23985  5699  5565 26977 25576   124 20393    39  6531 21736 22836 \n#>     2     3     3     1     1     3     2     2     2     1     2     1     2 \n#>  2741  5807 24202   437 10574  8300 29277 21367  1690 11174 13624   253 11319 \n#>     2     1     1     3     1     1     2     2     1     1     2     1     2 \n#> 17149  8428 19289 18197 14021 28543 22671 10027 18406 28674 26646 15509  4511 \n#>     1     1     1     1     2     1     2     1     1     1     1     2     1 \n#>  3293  4690 19663 16981 25372  2755  2722 15308  8770 29153 23189 26824 20845 \n#>     2     1     1     1     2     1     1     1     1     3     2     2     2 \n#> 13607 16327 17776 10077 23814 17949 28643 10703 24139 23555 25786  4707 26202 \n#>     1     1     1     1     1     2     1     1     1     2     1     2     1 \n#>  9507  1884 17344 10921  2227 15706 21628 10974  6317 21003 13946 27157 21071 \n#>     2     1     2     3     1     1     1     1     1     1     1     1     1 \n#> 17562 28603 14647 21271  3533  5399 15672  2662 14282 27834 29655 27844   279 \n#>     1     2     2     2     2     1     1     1     1     1     1     2     2 \n#>  4610 15081 26273   439 14019 27048 19690  9008  6340 18315  3002 17984 17925 \n#>     1     2     1     1     1     1     1     2     2     2     2     1     1 \n#>  8687 18495 20161 13578 10137 21431  2357  1688 15487  8293 11115  1171 14121 \n#>     1     1     1     1     1     1     2     2     1     2     2     2     1 \n#>  9323  7671   629  4059  7842 25550 24776  2642  1540  7859 24220  8151 18636 \n#>     2     1     3     1     1     1     1     1     1     2     2     1     1 \n#> 17057 13860 18340  2239 22423 16778 13073 15080 15972 14083   224 11926 22172 \n#>     2     1     2     2     1     1     1     3     2     1     2     1     2 \n#> 16020 16499 13689 21515 26557 18857 14648 26213 21283  7771 25310 11500 14846 \n#>     1     1     1     3     1     1     2     1     1     1     2     2     2 \n#>  7654 14235  3047 25500  5946  7785   374 22552 21521 19057  6164 22852  2290 \n#>     1     3     3     1     1     2     3     1     1     2     3     1     1 \n#> 28195 12111 22951 23635 20991 20160 10843 24119  8120 21289 29675 26163 14313 \n#>     2     1     1     1     1     1     1     2     1     2     2     1     2 \n#> 20489   503 19085 26101   359 27068 15424 21195 16843 24101  7705  9784 24360 \n#>     2     1     1     1     1     1     3     1     2     1     2     1     3 \n#> 21612 22031   524 12640 27105  9126  7633 18265 23500 11171 24994 24731 29810 \n#>     1     1     3     2     1     3     1     1     1     1     3     1     1 \n#>  6262  9287 22759  4792 22693 25212 29825 18706  3957  3654 12253 17077 15113 \n#>     1     1     3     1     1     2     1     1     1     1     1     1     1 \n#> 25759  8351 26008 14419 19205  3743  1631 20577  4650 19159 20194  8295 17247 \n#>     3     3     2     2     1     2     1     1     1     1     1     1     2 \n#>  2439  9785 27475 27859  7216 25299 26661 11886  3504  2176 25430 22884 25093 \n#>     1     1     1     1     1     1     2     1     1     1     2     1     1 \n#> 21951 13701  2005 26459 22016   693 16638 27387 11399  4609  9532  6548 10217 \n#>     3     1     1     1     1     2     1     1     2     1     2     1     1 \n#> 14274 21358  9485 29099 19072   520  6110 19525 16407 28745 10903  1231  3296 \n#>     1     1     2     2     1     1     3     1     2     2     1     2     1 \n#>  5907 26057  5585   347 12346 16254 24358 29570  3624  2287  9205 11164  5661 \n#>     1     1     1     3     2     3     1     1     1     1     1     1     1 \n#> 26138 27747  1524 18788 25816   532 22661 25475 15104 16900  9190  3363 17333 \n#>     2     1     1     2     1     1     2     1     1     1     1     2     1 \n#> 12228  4799 16813 18281 26503 15971 28371 14098 24945 18209 21546    54 20787 \n#>     1     1     1     1     2     1     2     1     2     1     1     1     1 \n#>  3672 14305 25542  9528 29886 26421 21482 17828 20491 15555 15422  9228 26965 \n#>     1     1     2     1     3     1     2     1     2     1     1     1     1 \n#> 12059 13337 19399 18240  9002 26142  4622 22830 22087 19568  2010 16822  3304 \n#>     1     1     2     2     1     2     1     2     1     2     2     1     1 \n#>  3518  8746  3862 13634 15452 27265  7606    86  9688 17062  7360 15345 22193 \n#>     2     1     1     1     1     1     1     1     1     1     2     2     2 \n#> 19853   925 24516 25517 12748 11898 27845 17593 11545 23358   342 29039 24412 \n#>     1     2     1     2     1     1     1     2     1     3     1     1     3 \n#>  8017 10145 16855 11439 10949   960 18263 16503  2204 23110 12619 27661  5321 \n#>     1     1     2     1     1     1     1     2     1     1     2     1     1 \n#> 10002 23659 25784 14990 27014 20051 20104  8507  7818 11400  4503 28111 27568 \n#>     1     2     1     3     2     1     1     1     1     1     1     1     2 \n#> 18846 25731  4567 17165 24070 29661 21880 23772 19385 19538 11112 12326 16332 \n#>     1     2     3     1     1     1     1     1     2     2     1     1     1 \n#>  6368  1257 10715  1742 14795 25807 23143 27822 25473 29527 17727 12003 10780 \n#>     2     1     2     1     1     2     2     1     1     1     1     1     1 \n#> 24287 26170 27487  9226 12932 22645  6918 12996  8787 13924 27012 17332 20587 \n#>     2     1     2     1     2     2     1     1     1     3     1     1     1 \n#> 18059 12720 10413 27148 23326 25318 17674 20598 29319 10193 24046  2028 24812 \n#>     1     1     1     1     2     2     1     2     2     2     1     2     1 \n#> 20714 14230  3878 29612  9401 28105  8951 29495 28012  9177 28823  8851 15050 \n#>     1     1     1     2     1     2     1     2     2     1     1     1     1 \n#>  4893 27392 26478 17540 13900  8208  8897  7566  3249  4058  1844 19812 10232 \n#>     1     1     1     1     1     1     2     1     1     2     1     1     1 \n#>  5841  9980 18243 23984 16660 18836  3343   236 19569 16542   623 12081 26571 \n#>     1     1     1     1     1     1     2     2     1     2     1     2     2 \n#> 15820  5293  7529 26665 17598 24305 17774  5883 29221 15169 19605 25181 24952 \n#>     2     1     2     1     2     1     3     1     1     1     1     2     2 \n#> 26679  9131 15354 29608 22426 21183  2206 16708  9510 13922 17879 17352 28967 \n#>     2     2     3     1     2     1     1     1     2     1     1     1     2 \n#> 28803  4157 27334 20284 27100 11738  8426 15316 15056  5342 16109 17597 20282 \n#>     2     1     1     2     1     1     2     1     1     2     1     2     2 \n#> 11507 10066 11611 12056  6180 29446 25141  7448  5382 28120 19941 10349  3540 \n#>     1     1     2     1     2     1     1     1     1     2     1     1     1 \n#>  8878 10236 15576  5651 23089  5356  2960 28192  2146 15811  6130 10141 19962 \n#>     2     1     2     1     2     2     1     1     1     1     1     1     1 \n#> 24597 17272 28460 11086 14556  8958 16067  8937  7675 17488  7236 18625  1447 \n#>     1     1     1     2     1     2     1     3     1     2     1     2     1 \n#> 15263 26198  5603 28785 27438 19994 23307 15647 13487 14619 14297 13816  8118 \n#>     1     2     1     2     2     1     1     2     1     1     2     2     1 \n#> 28799  5704 23999 18766 24377  8963  5446 19266 22184 28710  6721 19609  7786 \n#>     1     1     1     1     1     2     1     3     1     1     1     1     1 \n#> 26995  5162 15150  4179  9907  6658  2939 10667 29717 26250 27636 28289 14972 \n#>     1     2     1     2     1     1     1     1     2     1     1     1     1 \n#> 21985  1502  1233 22581  3236  9094 11564 24997  9124 12563   324 21828  6468 \n#>     1     2     2     1     1     1     2     2     1     1     1     2     2 \n#>  8757  4699 28431 27720  7187 21595  4132 18881 29858 24303  5034  3903  4790 \n#>     1     1     1     1     1     1     1     2     2     2     1     1     2 \n#> 23276  7425   813 17779  6270  2298  1929 20830 23412 22341  2011  7504 21611 \n#>     1     2     2     2     2     2     1     1     1     1     1     2     1 \n#> 11856 27231 12138 15007 13092 26461 11772 12338 17240   529  4585  2356 20377 \n#>     2     2     1     1     1     2     2     2     1     3     3     1     1 \n#>  9201  1020 28780 17185 18126  5297 29891 12464 18591 24088 28664  9075 21290 \n#>     1     2     2     1     1     1     2     2     2     1     1     1     1 \n#> 12426 23082 15489  7371  6583 29748 15089 28589  8561  9720  2189 21304   538 \n#>     1     3     1     2     1     1     1     3     1     1     2     1     1 \n#> 23142 14924 22640 17067 23448 19467 29812 14080 16740 14965  7542  7406 20375 \n#>     1     1     2     1     1     1     1     1     1     1     2     1     1 \n#> 19626 25689 17529  3601  1759 10912 23619 20320  4765 22710 23356  3620 18381 \n#>     1     2     1     2     1     1     1     1     1     3     3     2     1 \n#> 16657 21473 22100 27350 18791 24091   592 14652  2276 23860 13832 23070 22768 \n#>     1     1     1     2     2     2     1     1     1     2     2     2     3 \n#> 22478  6100 11377 18396  9157 12872  1816  4719 13703 25847  9385 21899 28670 \n#>     2     1     2     1     1     1     1     2     1     1     2     2     2 \n#> 12869 13165 29359  8036 13472 15409 29874 10615 19468  6499  5483   691 22930 \n#>     1     1     1     2     2     1     1     1     1     1     2     2     3 \n#>  2617 12038 17615  5613  5689 24047 23937 13417  9751 13799 11817 13114 27107 \n#>     1     2     1     1     1     1     1     1     1     1     3     1     1 \n#> 24531 16865 19693 23831  9757   206  2857 21236  2760 16181 28031 25496 28516 \n#>     1     2     1     2     1     3     1     1     1     1     1     1     1 \n#>  8478   247 16058 24354 12826 18776 19599 15676  7303 10543  6473 25815 13489 \n#>     3     1     1     1     1     2     2     1     1     1     1     1     1 \n#> 17423 12377  7076  2817  7338  2855  6225  2908  1596  5792 17649 28630 29679 \n#>     1     1     2     1     1     1     1     1     2     2     1     1     2 \n#> 10050 23724 26423 27618 24437 15317 13049 20898  3879 28913 17500 14926 27685 \n#>     1     3     1     2     1     2     1     1     2     1     1     2     2 \n#> 15613  1814  8101  3500  2747  1390  6257 15450 12336  4178 27284 29670  3608 \n#>     1     1     2     1     1     1     1     2     1     1     2     1     1 \n#> 22228  6662 19864  8828  2734 14213 22833 18433 29087  5157  6913 18965 28713 \n#>     1     1     1     1     2     2     1     1     3     1     1     2     2 \n#> 25011 20485 21434 28101 16976 23541 15645 16432 28314 14171  2669 19754  5541 \n#>     2     1     2     1     1     1     1     1     1     1     1     2     2 \n#> 26262  4161 10405  4836  2208  7240 13230  6083 20044 14059  6197 23144   505 \n#>     1     2     1     1     1     1     1     2     1     1     1     1     1 \n#> 15620  5386 18520 25053 22988 10876  8672 27485  7405  2344 19008 10098 26084 \n#>     1     2     3     1     2     1     2     1     1     1     2     3     1 \n#> 10641 27840 15471 24555   746 14706 25153 12614 20058 11735 12448  2982   198 \n#>     1     2     1     1     1     1     2     1     2     2     2     2     1 \n#> 12819 29351 12128 10111  7215 28199 25143 23753 22290 22430 28292  6987  9636 \n#>     3     1     2     1     1     1     1     2     2     1     1     1     1 \n#>  3616 29387 20529  2944 10247 22525  4590 10042 11258 29413  3729  8203 16040 \n#>     1     2     1     1     2     1     1     2     2     2     1     1     2 \n#> 23164  3259 13937 16604  8268 18900 19378 21509 10057 11527  2826  4232 23212 \n#>     2     2     1     1     1     1     2     3     1     1     1     1     3 \n#>  7419 15644  9447  2179 25467 23360 12321 24594 13175 11004  9904 16082 13273 \n#>     1     1     1     1     1     1     2     2     2     2     1     2     1 \n#>  5318   490 12962 20191 23877   967 25189 27772  3156 23819 17260 20550 11890 \n#>     1     1     2     2     1     1     2     1     1     1     1     1     1 \n#> 22361 19006 21712 28822 24830  8563 26490 23074  3991 20627 21913 28390 28435 \n#>     2     1     3     2     1     1     1     1     1     2     3     1     2 \n#> 29921   846 15558 26032  8399 21162  3041 25135 16686 21485 21594 13367     9 \n#>     1     1     1     3     1     1     2     1     1     1     2     1     2 \n#> 17458 15105 18383 18203 26168  4794  4886 10889 25802 10114 11055 25462  5158 \n#>     2     3     1     2     2     2     2     2     2     1     2     2     2 \n#> 16188 17583 21802  1652 21691 22178 22832 14228  7665 19873  2152  5210 16251 \n#>     2     2     1     1     1     1     3     2     3     2     1     1     1 \n#> 20696  2621  6880   484  1535 23374 12307 10463  4583 27533 15897 15747 25267 \n#>     2     1     1     1     2     1     1     1     1     1     2     1     1 \n#>  2613 14586  7793 15082 17669  7560 20716 16809  7759  5936 28397 12484 18301 \n#>     1     1     1     1     1     1     1     1     1     1     1     1     1 \n#> 29827 14324  8799 13265 17147  8134 15129 20136   878 15227 10046 23426 23897 \n#>     1     2     1     1     2     1     3     1     1     1     1     2     2 \n#> 16751  5810  3828 14952 23161  9939 14191  1494 12262  6500 11832 16666 21946 \n#>     3     1     2     2     1     1     1     2     1     1     1     2     1 \n#> 26127 10102  3141  6970  5077 12954 11844 13388 27248  3061 14799 19363   888 \n#>     1     1     1     1     1     1     2     1     1     1     2     2     2 \n#> 25623 18801 14890 29415 25747 11344 19283  8509 11649 28149 17986 11585 21076 \n#>     2     1     1     3     1     2     1     1     2     3     1     2     1 \n#> 13442 16937 14583 21522   115 27170 17983 11254  5842  3877 12191 27458  5413 \n#>     1     1     1     1     2     1     3     2     2     1     2     2     1 \n#>  7978 27969 13896  3038 19852 18847 17566 11265 10454 21539 14453  2094 20765 \n#>     1     1     1     1     1     2     1     2     1     1     1     1     1 \n#> 24478 16621 27493 19776 26619 16643  2040    77 22575 19732 22927  2627 28127 \n#>     2     1     1     1     3     2     1     1     1     2     2     2     2 \n#> 25403 28403 17457 29681  6231 10234 21857 15304  9923   244 21512 28933 26049 \n#>     2     2     1     1     1     2     2     1     1     1     2     1     1 \n#> 11369 15420  2933 20910 29526  7939  8619 21179 10831 21061 13441 19832  5168 \n#>     2     1     1     1     1     1     1     1     1     3     1     1     2 \n#>  1848 25109  7629   418  8007 23527  1822 23769  3710 23384  9230 18364 19282 \n#>     1     1     1     1     2     1     2     1     2     2     1     2     1 \n#> 15768 19155 20855 25778 22158  6895  9137 13844 13452 18996 10018 24996  6631 \n#>     1     1     2     2     2     2     2     2     2     2     1     1     1 \n#> 16919  7256 10221  7010  5702 13579 22764 13119 12864 10501 10917  6491 27914 \n#>     1     1     1     1     1     1     2     1     2     1     1     1     3 \n#> 15486 23451 18397 29355 16408  8476  9303 28776 11517 20935 21095 12763 13721 \n#>     2     3     1     1     2     2     2     1     2     3     2     1     3 \n#>  2547 22734 16672 17943 21201 17253  7388 19623  8732 29925   527 14615  4744 \n#>     2     1     1     1     2     1     2     1     2     2     3     1     3 \n#> 28616  7797 20706 17370  8567 27740  7515 11838 19851 27753 26426 12931 12283 \n#>     1     1     1     1     2     1     1     1     1     1     1     1     1 \n#> 22352  3464  5493 18726 28881 23355  6890 11305 19081   763 29238 19154 25687 \n#>     1     1     1     3     2     1     1     1     2     1     1     2     2 \n#>   661   564  4057 20684 28028  5957  8839 20404  2112 26354  4352 24553 28160 \n#>     2     2     1     1     1     1     2     1     1     2     2     1     1 \n#> 13294 16719  4523 10696 18575 16402 23739 20725 20602  2075 10086 19778 19077 \n#>     1     1     1     1     2     1     1     1     2     1     1     1     2 \n#>  1478 19541  9032  2021  3262  4258 14641   953 19895  8519  4189 20476  6590 \n#>     1     3     2     2     1     1     1     2     1     2     1     1     1 \n#> 22056 21667  9491  9279  2225 10506 25977  8693  9057 15503 24116 22959  1862 \n#>     1     1     2     2     1     1     1     2     1     1     1     2     1 \n#> 25060 23902  9758  8706 21488  7328  9213 13596 21394 12098 27763 19396  5682 \n#>     3     2     2     1     1     1     2     1     1     2     2     1     1 \n#> 28715 13752  4542 14017 29713  5014 22538 23259 23077 27792  8815 26969 13930 \n#>     1     1     1     2     1     1     1     1     1     1     1     2     1 \n#>  3842  3140 21610  6657 23476 19565 17328 18761 11270 18853 15649 15528 17765 \n#>     1     1     1     1     1     2     1     2     1     1     2     1     2 \n#> 26781  7977 16173  9127 16085 27077  6135  2820   475 15885  6929  6226  9444 \n#>     1     2     2     2     1     1     1     1     1     1     1     1     2 \n#> 14932  2466 25869 11335 12124 12637 11030 24842 16071 13979 29768 10822  1621 \n#>     1     1     1     1     1     1     1     1     1     2     2     1     2 \n#> 15914   765 21679  1948 22612 26584 25086 20067 16132 23190 19624 19244  1782 \n#>     2     1     1     1     2     3     2     2     1     1     1     2     2 \n#>  1599  7258 12385  8100  2948 13002 24653 29557 19255 14408 24887 10313  1396 \n#>     1     2     1     1     1     3     3     1     1     2     1     2     1 \n#> 12102  1153  1191  3585 28927  3523  8195  3514 17729 24925 28628  8016   756 \n#>     2     1     2     2     1     2     2     1     2     2     2     2     2 \n#> 10841  7332 10719 24593  3044 10328  3747  6439  9622  1927 21234 22666 28379 \n#>     1     2     2     1     1     2     1     2     2     2     2     1     1 \n#> 23107  4506  2607  6341 13226 12661  6359 21291 25926 22505 22060 13192  9300 \n#>     2     1     1     2     1     1     1     1     2     1     2     2     1 \n#>   522 19049 16489 15513 11568  2250 24750  5020 25828 14702 28489 20276 17299 \n#>     2     1     1     1     2     2     2     1     3     1     1     1     1 \n#> 16430 24034 24200  1904  6219 19654 13179 12715  6985 24307 11205  8788 20196 \n#>     1     3     1     2     1     1     1     1     1     1     2     1     1 \n#> 10441 12664 29309 26512 26998 19551 29353  2805 25661  2814 14128 14644 11285 \n#>     1     1     1     1     3     2     2     2     1     1     1     1     2 \n#> 12717  1368 26144 27033 23671   862 16370 21112  6652  6539 23145 16264 28671 \n#>     1     1     2     1     2     1     1     2     3     2     1     1     1 \n#> 11670  8381  1679 19387 26336 21189 13791  6558  4896  1583 29122 22720 16259 \n#>     1     2     2     1     2     2     2     2     2     1     1     1     2 \n#> 28605 27716 29467 23223 25292 17300 22600 10584 21801 10953 20746 15339 10371 \n#>     2     2     3     2     2     2     3     1     1     1     1     3     2 \n#> 24551 19932 24096 13433  1820  4644 17169 23874 15181 25002  8175  3406 14796 \n#>     1     2     1     2     1     1     1     1     2     1     1     2     1 \n#> 13076 15520  8670 17020  4474 12471 20183  7718 29832 24145 23297 11054 23958 \n#>     1     3     1     1     1     1     1     1     1     1     1     1     2 \n#>  5843 19540  1218 12372 21428  5140  3602 25728  1642  9431 28076 13987 22916 \n#>     1     2     1     2     2     1     1     1     1     1     2     1     1 \n#> 22442 20819 16573 24759 19200  9426 14473 19946 26190 27947 26258 20812 21942 \n#>     2     1     2     1     1     1     1     1     2     2     2     1     2 \n#>  9661  3358 14509   423 25356 25581  7749 10125 24934 24396  9630 14062 20010 \n#>     2     3     1     2     1     1     2     1     2     1     1     1     1 \n#> 26837  9828 24208  5040 20241 24408 27142 28521 10512 16312  2426  7788  2515 \n#>     1     3     2     1     1     1     1     1     1     1     2     1     1 \n#> 28296 11759 28657 25355 29402  1926  1832  9370 20809 10580  4221  7822 13605 \n#>     1     1     2     3     1     1     2     1     1     1     3     1     1 \n#> 19564 16386  5312  7091  7021 14008 11687  6633  1618 25125 27544 26207  2806 \n#>     1     3     2     1     1     1     2     1     2     1     1     1     1 \n#>  2337  7552  6888   164 16753 10190 22838 19660 12414 20651   513 18981  1790 \n#>     2     1     2     1     2     1     2     2     1     2     2     2     2 \n#> 21136 16333  4793 13359 13962 29020 17276  7655  8014 10672 22077 15377 19808 \n#>     1     2     1     3     2     2     1     1     1     2     2     1     1 \n#>  2168 28245 18802 25737 12491 15041   791   789  5397 19420 24474  9747 17691 \n#>     1     1     2     1     2     2     1     1     1     1     1     1     2 \n#> 24741  6385  9038 21085 29400 22198 24490 16525 25176 16230  5554 27027 14701 \n#>     2     2     2     1     1     1     2     2     2     3     1     2     1 \n#> 27372  2761 24831 14634 26192  3494 26905 29776 20595 20327 14475 11283 23051 \n#>     1     1     2     3     2     1     1     1     1     1     1     1     1 \n#>  2251  4030  8656 23284  4631 25811 16595 26071 17320 17064  7838 16455 15132 \n#>     1     2     1     1     1     1     1     1     1     3     1     3     1 \n#> 28542 21597  1604 28919  9361 14578 10196 25993  8389 11470 25202  7872 16512 \n#>     3     1     2     2     1     1     2     2     1     2     2     3     1 \n#>  6488 22550   483 25300 27123  9257 26339  3725 23537 19741 16032 10722 11418 \n#>     1     1     2     1     1     1     2     1     1     1     1     1     1 \n#> 13167 26152  8000 23707 22086 12034   896 29147 22674  3194 16978 28815  3917 \n#>     1     1     1     1     1     1     2     1     1     1     3     3     1 \n#> 10354 25790 28173 22560 14567 27214 29069 20671  7731  6947 10402 16475 11444 \n#>     1     2     2     1     1     1     2     2     1     2     2     3     2 \n#> 23693 25544 10630 26290 14031  2498  6338 21129 16213 24076  7094 10695 13622 \n#>     1     2     2     1     1     1     2     1     1     2     1     2     1 \n#>  5598 22457  1834 19416 15621 28251 27375  3420 16165 12030  1658  3389 20825 \n#>     1     1     1     1     2     1     2     1     1     1     1     2     1 \n#> 10235 25412   179   169 16205 22665  4546 27172  1406 16936  4095 26306 16689 \n#>     2     2     1     1     2     1     1     1     2     1     1     1     2 \n#> 14953  1432  9455  9219 17667 26452  3640   457  4530 27296  6171 17830   908 \n#>     1     2     1     1     2     1     1     1     1     1     2     1     2 \n#>  9121 19343 29923  2381  7070 25723 12040  7974 19326   554 14989 22982  8865 \n#>     3     1     2     2     2     1     2     1     1     1     2     1     1 \n#> 18027  6269  8684 24315 11124 26567 27625 21115  6887 11343 26408 29174  9009 \n#>     1     1     3     1     1     1     2     1     1     1     1     1     1 \n#> 13511 11206 13462 12140 25660  6885  6586 26003 19704 10309 25595 15403  7474 \n#>     2     1     2     1     2     1     1     1     1     2     2     2     2 \n#> 22248 13369  5285  1347  3898 24943 23961  5951  6426 20721 28131 28232  6348 \n#>     2     1     1     1     1     1     1     1     1     2     1     1     2 \n#>  1601 14241 29762  5522  3532 22793 15013 29902 27052 11924 21262 20049 25232 \n#>     2     1     1     1     1     2     1     3     1     1     1     2     1 \n#> 15548 16420  5595 29617 14598 10064 14334  1456 24850  2083 19779 17273 29363 \n#>     1     1     1     1     2     1     2     2     1     1     1     1     2 \n#> 27881 13863 24791 18861 13867  6188 13194 14183 26878 19947 28485 17198 22778 \n#>     2     1     2     1     1     1     2     1     1     1     1     1     2 \n#> 22881 12449  7146 19136  3825  1237 15136 15071 15221 10170 16442 16567 16584 \n#>     2     1     1     2     1     1     2     1     1     1     3     1     1 \n#> 18691 22464 16248 11879 26303 10049 28992 24304  9395  8150 27719  2169 18239 \n#>     2     1     3     1     2     1     2     2     1     1     1     2     2 \n#>  5192 14554  3645 14758 17955 16747  6773 18302 29802 22068 27114 15809 21715 \n#>     2     1     1     2     1     1     1     1     1     2     1     1     1 \n#> 20816 18577 20758 13858 18737 18780  8492 12662 19226 25705  6198 19667 28013 \n#>     1     2     2     2     1     2     2     3     2     1     1     2     3 \n#>     4  9163 14807  7964 22963 27713 18206 19163  4183 24598 13255 14333  1187 \n#>     1     1     3     2     1     2     1     1     1     2     1     2     1 \n#>  3781 23406  8578 19122  8055 24639 21980 22897  6710   187 19325 18851 27218 \n#>     1     2     1     1     2     2     3     3     1     1     1     1     3 \n#> 27539  4238 17841 19408 22767 10547  4841 23963  5499  2571 10475  4712  3805 \n#>     1     2     1     2     1     1     1     1     1     1     1     1     1 \n#> 27245  9719  3067  9145   445 17353  2054 18604  8167  3108 12475  3317  9841 \n#>     1     2     1     1     1     3     1     2     1     2     1     2     2 \n#>  4176  7869 16676 24201 27073 14074   706 10271  1500 19771 24052 19104 24302 \n#>     1     2     1     2     2     2     1     2     2     2     1     1     1 \n#> 13254 28005  8328 11227  1981 11938 21316  2049  9399  2070  9914 12325  3355 \n#>     1     1     2     1     2     2     1     1     1     1     1     1     1 \n#>  1296 25027 10388 20978 16946  5258 20132 21813  9679  9191  9083 13829  5562 \n#>     2     1     1     2     1     2     2     1     1     1     1     1     1 \n#> 16250  7411 20264 10167 26210  3453 15835  9313 12146 25103  6047 21079  1152 \n#>     1     1     1     1     1     1     1     2     1     1     1     1     1 \n#>  3632 28128 23029 17447 23720 13617 20340 21810  1845   362 26268 21418 12089 \n#>     1     1     2     1     1     1     1     1     1     2     1     2     2 \n#> 14715 21849   237 28285 17479  8759 23172 10087 23311 29721 23651 23289 11913 \n#>     1     1     1     2     1     2     2     2     1     1     1     1     3 \n#> 14418  7241  7989  2437  1894 27728  6327 26935   556 23655 21140 12292  5451 \n#>     1     1     2     1     3     2     2     1     1     1     2     1     1 \n#> 25449 18402 22917 27080 15267 19655  4943  8743 18050 25157 12617  4821 27327 \n#>     1     1     2     2     3     1     1     2     2     1     2     1     1 \n#>  7609 17834  4958  5720 20525 27110 12052 16391 19668 14187 12533  7800 10329 \n#>     1     1     1     2     1     1     1     2     1     2     1     1     1 \n#> 26667  8430 13289 17709  8282 25272  9411 21321 21878 15539 27212 29451 24606 \n#>     1     1     1     1     2     1     1     3     2     1     1     1     1 \n#> 22773 12488  4499  7208 24447 19413 15934  6547 26470  9927 27153  3469  6955 \n#>     2     1     1     3     1     2     1     1     1     2     1     2     1 \n#> 23828   401  6780 20842  9132 16576 12446 23242    51 21168 22270 22921  3678 \n#>     2     1     1     1     1     1     1     1     2     1     1     2     2 \n#> 28791  7913  5742  1951  6048 25633 21407 28155  4031   549  7866  4578 12005 \n#>     1     1     1     1     1     2     1     2     1     1     1     1     1 \n#> 16837 18896 12026 27865  2217 21557 18602 19290 12875 20159 15563  5205 10118 \n#>     1     1     2     1     1     1     2     1     1     2     1     1     2 \n#> 28162 27788 24865  6480 17202 15827 17708 23267 18811  5723  9275 21795  3096 \n#>     2     1     2     1     1     2     1     1     1     2     1     2     1 \n#>  9789  4839 18200 14294 24859  3068 15416 17009 24699 21817 16266 21768 15414 \n#>     2     1     1     2     1     1     2     1     1     1     2     2     1 \n#>  5555  8955 28227 25058 15987  5474  4752 20942 10160 16577  8686 19407 14451 \n#>     1     1     2     1     1     1     1     1     1     1     2     1     1 \n#>  1707 27135 19938 28969  1735 10637 26960 20551 12747 14488 10825  1771 19179 \n#>     1     1     2     1     2     2     1     1     2     1     1     1     1 \n#>  6731 21843  4154 16636 19971   641 13339 11864  4339  5164 23236 29287 20151 \n#>     1     2     1     1     1     1     2     1     1     2     1     1     3 \n#> 28150 11968 22349 23490  8800  2690 20405  5587 13042 11936 19000  6528  6536 \n#>     1     1     1     1     2     1     1     1     2     2     2     1     1 \n#> 21688  5954 19059 27642  9782  2386 21954 22033  8320  4863 17269  9816  9440 \n#>     3     2     2     2     2     2     1     1     1     1     2     2     1 \n#> 14205  8752 12028 24006 26795 10136  5877 25927 19871 22700 26442 21243 27583 \n#>     1     1     1     2     1     1     1     1     1     2     3     2     1 \n#> 19230  5812 26182  3347 10015 21774 12097 28239  2191 19117 25885   987  9862 \n#>     1     1     1     1     1     1     1     2     1     1     1     1     1 \n#>  4702 28827 17187 14963  5644 25394 20233 29135 20969  6645  3613 21433 15437 \n#>     1     1     1     1     2     1     1     1     1     1     1     1     1 \n#> 16557 21318 14240 12342  1111 10942 16026 18225 23439  6402 29598  3942 22895 \n#>     1     1     1     1     3     1     2     2     2     1     1     1     1 \n#> 27473 28441 25919 12539 26256 13633 21777 17913 12288 15309  1736 28959 19905 \n#>     1     1     2     1     2     1     1     2     1     1     1     1     1 \n#> 15845 29060 26321 26691  5988 16535 10860 26542 14798  5151 14122 14054  8255 \n#>     2     1     1     1     2     2     1     1     1     1     1     2     1 \n#>  4939 19263 24276 18799 20316  4633 19491 16782  6601 11169 10150  7324 20683 \n#>     1     2     2     1     1     2     1     3     1     1     3     1     3 \n#>  6594 20565  3386  8869 20069  1372 14727 27614  4079  5818 21208  8889 28556 \n#>     1     1     3     2     1     1     1     2     1     1     1     2     1 \n#> 26789 18212 26650 14165 29027 12306 22156 28687  8081 11887 28649 13440 25858 \n#>     2     1     1     1     1     2     1     1     3     1     2     1     2 \n#>  4425 21552 19893 27042  4652 28997 24796  6638 10706  7004 23442   879 22117 \n#>     1     2     1     2     1     1     1     1     2     2     2     1     2 \n#> 20244 16352 28608 26613 21425 11013 20928  2881 28932 22808 27630 14177 25884 \n#>     1     1     2     1     3     1     3     1     2     1     1     1     1 \n#> 16263  1124 24767  3120 17624  9779  3589 13564 25681 14889  2874  4879 11185 \n#>     1     1     1     1     2     1     3     2     1     1     2     1     2 \n#> 13078 25554 17108 10629  2311  1476 13517  8654 21959 24961 10947 29033  4129 \n#>     1     1     1     3     1     1     1     2     1     1     1     2     1 \n#>  3126 22526  9742 24880 29791 14757  6421 23625  8244  1637  9427 19364 25386 \n#>     1     2     2     1     2     1     1     1     1     3     1     1     2 \n#> 14426  7699  7912 27302 25639 13411 16376  5142 27617 20727 29824 16059 25834 \n#>     3     1     2     1     1     2     1     2     1     1     1     1     1 \n#> 10618 27444 24590  5713 26456  3301 10148 23314 10307 18051 14604 26065   289 \n#>     2     1     3     1     1     1     1     1     1     1     1     2     2 \n#>  9726 26249 20566  2130  8357 17070 28792 14311 19575  9524 19276 14988   869 \n#>     1     2     1     2     2     2     1     2     1     1     1     1     2 \n#> 17400 10620 25218  5990 23624 24309  8767 26121 19745 16797 28432  8837 26122 \n#>     1     1     2     1     1     1     2     2     2     2     1     1     3 \n#> 15534  5968 17026 26195 27227  2038  5710 11863  6923 23093  5916  2256  6005 \n#>     2     1     1     3     1     1     1     1     1     2     1     1     1 \n#>  2006 15961  1657 26045 22440 23366 15979 15981  1201 20513  6388  3584 29443 \n#>     1     2     1     1     1     3     1     2     1     1     1     1     2 \n#> 24390  6777 23726 14800 23853  5845 17737  5145 26407 14908 12785 27855 12988 \n#>     1     2     1     1     2     2     2     2     2     1     2     1     1 \n#>  6043  1168 20373 17257  5173  1391 28132 24326 26641 16899  4040 27290  7768 \n#>     1     1     1     1     1     1     1     1     1     1     1     2     2 \n#> 11756  6239 14277 23349 25603  9195 17484 18834  2566  1326 25374 18036 17516 \n#>     1     1     1     1     2     1     1     1     1     2     1     1     1 \n#>  3733  3933 13248 29968 11902 23295 10666 13682 11574 18959 10902 13147  1015 \n#>     2     1     1     1     2     1     3     1     1     2     3     2     1 \n#> 10929 11354 10767  9990 15156 27615 16174  4463 23036 12846  1283 19543 14060 \n#>     1     2     2     2     3     1     2     2     1     1     1     1     1 \n#> 27463 15517 16902 27587 20555 10074  8045 28119  9170  5910 18984  3179  4304 \n#>     2     1     1     1     1     1     2     1     2     2     2     1     1 \n#> 17490  7137    62 20409 21391 29214  6445 21056  2283  5124 17773 29705 22883 \n#>     1     2     2     1     1     1     2     1     1     3     1     1     1 \n#> 24253 27924 15725  2348  2041 29117 24039 26411 29111 11333  9393  5543 28924 \n#>     2     1     2     1     2     1     1     2     3     1     1     1     1 \n#>  7195 10849 29555 11372 21486 16232 21322  9421  9181 17588 12482 28754 17869 \n#>     1     2     1     2     2     1     1     2     1     1     2     1     1 \n#> 20893  4894 25722  7379 26382   708 12231 18373  7738 20223  4249  8453 28081 \n#>     1     1     1     1     1     1     1     1     1     1     2     1     1 \n#> 29448 29574 12357  5396 21629 27813  9236 12745  4732 27062 12543 20581  1382 \n#>     1     1     2     2     2     1     2     2     1     1     1     2     1 \n#>  5955 29915  1240 20526  2800 21572 22991 18600 18668 20055  2696  4556 28004 \n#>     1     1     3     2     1     1     2     1     1     1     2     1     1 \n#>  8717  3995 11840 10024  8288 16539 17766 15802 13336 29049 18413  5178  6673 \n#>     2     2     2     2     1     2     2     1     2     1     1     1     2 \n#> 11051 15515 24417 12494 16133 20250 29641 11930  6958  5074  3482 18080 29289 \n#>     1     3     1     1     3     1     2     1     1     1     1     2     2 \n#>  9937  3247 18858 19131 17679 17939   345  7694  8400 20533  1890    68 14684 \n#>     2     1     2     1     2     3     1     1     2     1     1     2     2 \n#> 22697 20903 28917  2284 29569 27323 25836 21583  5141 24195 14135 26756 28073 \n#>     1     1     3     2     2     2     2     1     1     2     1     1     2 \n#>  1141 20477 14840  3776  9896 15697 11801  3203 24017 19350  6378  6321 12667 \n#>     1     1     2     1     2     2     1     1     1     2     1     1     1 \n#>  4086  6238 26324 13341 13766 24723 29581  7932  2424 15661 28868 25331 29509 \n#>     1     1     2     1     1     3     1     1     1     1     1     1     2 \n#>   496 14124 22656 25608  5471 23345  4828  7111  4698  2167 18309 20407 13353 \n#>     1     1     2     3     1     2     2     1     2     1     1     2     3 \n#> 18716  7976 27490 22105 11168  8534 23642 10520  8964 28405 15693  3809  5369 \n#>     2     1     1     1     2     2     1     1     3     1     2     1     2 \n#> 21292 13842 14472 25436  1468 10330  2572  5273 13297 28709 26085  3338  3264 \n#>     1     1     1     1     1     1     3     1     1     2     2     1     2 \n#> 15044 13101 11681 11432 26021 24648  8345 24523 14698 22072 20009 16579 20297 \n#>     1     1     1     1     1     2     2     1     2     2     2     1     2 \n#> 21111   266 13351 11650 13522 12246  2203 21742 14618  5772 27410  7471 23908 \n#>     3     1     2     1     1     1     1     2     1     1     1     1     1 \n#> 25307 27070  9624 28096  9364 19949 23159    61 18835   940   474 17442 20582 \n#>     1     1     1     1     1     1     1     1     2     2     1     2     1 \n#>  6654  4319 26761  3328  2888 22672 20929  8833 22257  2721 10171 19573 29186 \n#>     1     1     1     1     1     2     1     1     2     2     2     1     2 \n#> 14133 17412  5517  8233  6517 11802 19702 12382  9418 20164 20416 14153 18568 \n#>     1     1     1     2     2     3     1     2     3     1     1     1     1 \n#> 13466 10509 21036 11379   736 19880 10139 22820 20177 18241  7357 18928    66 \n#>     1     2     2     2     1     1     1     1     2     1     1     1     1 \n#>  2745 16147  9432 25780  5658  6102 24174 26908  7103 18417 18153 21279 12120 \n#>     1     2     1     1     1     1     1     3     1     3     2     2     1 \n#> 29407   989 26551  8775 16693 29615 23232 27696  9675   688  1788  7683 28230 \n#>     1     1     1     2     1     1     2     2     1     1     1     2     2 \n#> 24818 10548 17271 18679 19220  4782   297  8809 21134 12109 22359  6168 27404 \n#>     1     1     1     1     1     1     2     3     2     1     1     2     1 \n#> 11200 21836  1193  1917  2380 11721  3213 13347  4635 13907 23650   615 28359 \n#>     2     3     1     1     1     1     2     1     2     1     1     1     2 \n#> 16494 26694 22069 16814 19834 29956 26941 16743 13519 27425 22014 23522  6027 \n#>     2     2     1     1     2     2     2     3     1     1     2     1     1 \n#> 16781 29183 28506   954  2963 26807 23402 22753 16777 24414 24721 19201  4651 \n#>     1     1     2     1     2     1     1     3     1     1     1     1     1 \n#> 11915 23430 14157  4191  8933 28062 13997  1977  2748 26639 19643 15281  2802 \n#>     1     2     2     2     1     1     1     1     1     1     1     2     1 \n#>  8148 23251 18789 16242  6004 12163  1705  5780  8109 16696 20263 18181 20611 \n#>     1     1     1     1     1     1     1     1     2     1     1     1     2 \n#>  9791 24083 28203  5600  1196 10480  5821 27977 12508 26109 19874 16908 10713 \n#>     2     1     1     1     2     1     1     1     1     1     1     1     2 \n#> 18393  4563 28510 25179  5739 24644  6296 24566  1206  4995  6874 28189  9327 \n#>     3     1     1     1     1     1     1     2     2     1     2     2     1 \n#> 14125  7729 20815 29563  7651 22041 15602 20796 17915 14027  3763  9378 20005 \n#>     1     1     2     2     1     2     1     1     1     2     2     2     1 \n#> 25432  3937 25352  3553 15957 17717 14002 18731  5240 28614 29197 29629 29678 \n#>     1     2     1     2     1     1     1     2     2     2     2     1     2 \n#> 15093 29653 18886  3352   259 15243  2039 13364 26232 23020 25217  4637 19669 \n#>     2     1     1     1     1     1     1     1     1     1     2     1     1 \n#> 13849 21287   600 17205   637  1024  4383 27205 15453 24985 27648  1436 21019 \n#>     1     1     1     2     2     1     1     1     1     1     2     1     1 \n#> 20012  5102  6174  8994 26064 11242   744  1117 21207 20750  5201 22537 21682 \n#>     2     1     2     1     1     1     2     1     1     1     1     1     1 \n#> 24378 17524 26129    40 16253  1461  8856 26228 11236 21094 12714 18689 11454 \n#>     2     1     2     1     2     2     1     1     2     1     1     1     1 \n#> 28954 27427 18338 27551  9291 21471  4604  8832  7987  1738  2319  8127 16176 \n#>     1     1     1     2     1     2     2     1     1     1     1     3     1 \n#> 15701 11944 16271 13885 26035 25917 19156  1741 23760 19726 15288 10653 16886 \n#>     1     2     1     1     2     1     1     1     1     1     3     1     1 \n#> 24781 14336 25150 16975  4938  7828  4888  9150  7152 26444   685 24263 24978 \n#>     2     1     1     2     1     1     2     1     1     1     1     1     2 \n#> 14086   340 11217 23182 18555 20880  5251  5672 10701 27995 18781 19770  3676 \n#>     3     2     2     2     1     1     1     1     2     1     1     2     1 \n#> 29534 24547  6504 27318 12210 15055  8402  4837 29753 12655 20920  9561 27286 \n#>     2     1     1     1     1     1     2     2     1     1     1     2     1 \n#> 20736 17066 17678  5256  4026 27732 25495 10975 10698 26004  7610 25791 12170 \n#>     1     1     2     1     1     2     2     2     2     2     2     2     2 \n#>  3557 20037  9469 22588  2124 12940  2018 27474 13014  1572 14786 23024 25792 \n#>     1     1     2     1     1     2     1     1     2     1     2     1     2 \n#> 28309 14709  8810  9905   176 18311  2751 17406 13532 11351 21873  5619  4731 \n#>     1     1     2     1     1     1     1     1     1     1     2     1     1 \n#> 23550  2879  4008  5997 13161 15103 19911 18624  4287 22384  5453  2880  5798 \n#>     2     1     1     2     1     1     1     2     2     1     1     1     1 \n#>  4726 24986  1439 21909 12361  3915  8661  7544 27626 17444 16358  5816 26126 \n#>     1     1     1     2     1     2     1     2     1     3     2     1     2 \n#>  9557 12544 13142 20947  5727 27668  6617 14391 11394 25429 29607 16422  1983 \n#>     2     1     1     1     1     2     2     2     2     1     1     1     2 \n#> 18275   111  5130 17947 23278 27293 25987 16287 18553 15328 14864   976 24869 \n#>     2     1     1     1     1     1     1     1     1     2     1     2     1 \n#> 24768 11385 19474 23248  6159  5890  9389 13021 21601 26204 10514   734   941 \n#>     1     1     1     1     1     1     1     2     1     1     1     3     2 \n#> 11541 23389 27198  5538 24173  3171 12771  4117 10803  2624  5549 10631  7467 \n#>     1     2     1     1     1     2     1     1     3     2     3     1     1 \n#> 11190 18473 27412 24628  6154  1847  2199 19790  5237 21783   159 29530  2770 \n#>     1     2     1     2     2     1     1     2     1     2     1     2     1 \n#> 29398 20908 17948  6166   330  4592  1593 10280  7260 24685 25754 20751 25045 \n#>     1     1     2     1     1     2     1     1     1     2     1     1     1 \n#>  9750 27893  9880   393 20052 18744  9585  3985  5568 16707 28921 26532  4378 \n#>     2     1     2     2     2     1     3     1     2     2     1     2     1 \n#> 13423 23580 23730 17460 15026 25333 28508 23348 22045 21368  1923  1492 18249 \n#>     1     1     1     2     1     3     1     1     1     1     1     2     2 \n#> 27592 15525  8547 19747 26863 17733 27646  5066 28329 10858 19379 17238 26777 \n#>     2     1     2     1     1     1     3     1     1     1     1     1     1 \n#>  1012 14560 28590 28213 24223 18859 20486 26018 13631  2037 19596 15628  8021 \n#>     1     1     1     2     2     1     2     2     1     1     1     1     1 \n#> 25561 28452 14498  8648 17887 10560 20596 15126 19238 29023 16810 17548 15683 \n#>     1     1     2     1     1     1     1     1     1     1     1     2     1 \n#>  8365 19005 22040 27359 22958 10789 13326 10156 21814 19675  9164 14629 15189 \n#>     1     1     3     1     1     1     1     3     1     3     1     3     1 \n#> 11475  5545 19768  1712 22382 29756 14272  5332  4545  1251 10992 12441 16729 \n#>     1     1     2     1     3     1     1     1     1     1     1     1     2 \n#> 13263 20817  5044  5863 12594 10108    55  5163  1375 27537 28250 26731 15195 \n#>     1     1     1     1     1     3     1     1     2     1     1     2     1 \n#>   462 25348 15564 13007 12912 10800 19126  8059 23288 20548  6946 10194 23053 \n#>     2     1     2     1     1     1     1     1     3     1     1     1     1 \n#> 26751 12566 14821 12403 22639 17046  6967  2673 27706 17254 21032  7095  4263 \n#>     2     1     1     1     1     1     2     1     2     2     1     1     2 \n#> 24092 19105 17881  7127 16309  3297 23079 24651 20396 13134 24138 16114  8373 \n#>     1     1     1     1     2     1     1     1     2     1     2     2     1 \n#> 10001 24037 19223  9932 29708 11999  6881 25488 11943 14386  9151 13223 10552 \n#>     1     1     1     1     1     2     2     2     1     2     2     2     1 \n#> 21042  9130  4450 27770 19645 24484  7554 28320 12358  8930 17811 11523 20286 \n#>     1     1     1     1     2     2     2     2     2     1     1     2     1 \n#> 20764  6265 20324  4141 23746  8582 29344 28338 12789 11116 29979 19507  6587 \n#>     1     1     2     2     1     1     1     1     2     2     1     1     1 \n#>  7203 15052  9240 18121 20838  6826 28434  7294 18202  3970  5330  9102 17362 \n#>     2     1     1     1     1     1     1     1     1     1     1     2     1 \n#> 29940 23529  6988 23864  5653 22801  3675 25438 29097  3361 28015 17999  9010 \n#>     1     1     2     2     2     1     1     2     3     2     2     2     1 \n#> 23106 19710 15246   901 20082 23900  8902 28873 11152 20763  2392 22816 15293 \n#>     1     2     1     1     2     1     1     1     1     2     3     2     1 \n#>  8145 18920   142 17372  5283 27433  2992 15259  2330 11615  6319  9270 13144 \n#>     2     2     2     1     1     1     2     2     2     3     2     1     1 \n#> 12284 17155 24959 27779  1103  6370 22481   511  6111 13584  2695  8677 16926 \n#>     2     1     1     2     2     1     1     1     1     1     1     1     1 \n#> 14381 10075 12493  4729  8202 21383 15217 16909 11248 25264 14645  3929  5402 \n#>     1     1     1     1     2     1     2     1     1     2     1     1     2 \n#>  3031 13911 11971  8858 18518  2473  9666 19805  8758  5200  9536 25376 26654 \n#>     1     1     1     1     1     1     1     1     2     1     2     1     2 \n#> 19367 23965 22128 15801  9166  1353 23119 10853 29895    67   296 12521  8136 \n#>     2     1     1     1     1     2     2     1     2     1     1     2     3 \n#> 22730   230 17306 12192   660 15896  3644  7715  5337 28044 29976  7320  1675 \n#>     1     2     1     2     2     2     3     2     2     2     1     1     1 \n#> 15610 16278  9285 28700 28902  6965  7798  2341  5031 16083 25144   569  2173 \n#>     1     2     1     1     1     1     1     1     1     1     3     2     1 \n#> 25198 27756 18582 16615 21643   480  1980 20623  3292  2383  3223  1272  1562 \n#>     2     1     1     2     2     1     1     1     1     1     2     1     1 \n#> 12886 23118  8269  9113 15236  7089  1610 24113 19457  8647 28437 13409 17742 \n#>     1     1     1     1     2     2     1     1     2     1     2     2     1 \n#> 16586  7281 13932 27065 16799 16419 21106 16371 12578  4411 29396  4275 17472 \n#>     1     2     1     3     2     1     1     1     1     1     1     1     1 \n#> 16220 23034  4845 10671 10362 23997 26281 17665 26793 16585 25619 27962 29256 \n#>     2     1     1     1     1     2     2     1     1     2     2     1     1 \n#> 15407 21448 22873  8948 14285 21933 16437 19742  6743 16037  9830  4892 17209 \n#>     2     1     1     2     3     1     2     1     1     1     2     1     1 \n#> 23338 18088  3835 20854  4187  8363  6073 10740  7725  7832 12764  3199 10404 \n#>     2     1     1     1     1     2     1     1     1     1     1     1     3 \n#> 29082 24552 18642 11932 27943 22915  2205 13172 28240 25612   933 29813  5468 \n#>     1     3     2     3     3     1     1     2     2     2     1     2     1 \n#>  3597  8018 28898 14723 11794  7973 28940  2289  3331 11434 13488 27707 17322 \n#>     1     1     1     1     1     1     3     1     3     1     1     1     1 \n#> 23028  6588 27613 23923 26329 16340 17148 20283 22287 29329 15142 15069  9646 \n#>     2     1     1     1     2     1     2     2     1     2     2     1     1 \n#> 21058 14412 13102  8623  5379  1448 21093  3231  3764 15943 14186 21615  4269 \n#>     1     2     1     1     1     1     1     1     1     1     1     1     1 \n#>  7120 17461  5530  4147 13961 12303 10293  4444 17391  6095 27931 20022 20042 \n#>     1     1     1     2     1     1     2     3     2     1     2     1     1 \n#> 29846 24190  2983 17622 22620 14011  7295 15978  6563 15968 24737 18335  7098 \n#>     2     1     1     2     3     1     1     1     2     2     1     1     1 \n#>  8768  6800 15185 15068 18464 18662 22944 18486 14497 18649 19814   876 17815 \n#>     2     1     2     1     2     1     2     2     2     1     1     1     1 \n#> 10887 27088  1704  2129 18767 24819   795 21730 11324  3459 24983  4887  2056 \n#>     2     1     1     1     1     1     1     1     1     1     1     2     2 \n#> 19689  1053 15800 14009  6125 29720 12538 26319 16552 29912 14070 15817  8236 \n#>     2     2     2     1     2     2     1     1     2     1     3     1     1 \n#> 23369 28083 28393 18878  2562 23404 10399 10325 11044  9486 15144 26896 23099 \n#>     1     2     3     1     2     3     1     1     1     1     1     3     1 \n#> 23698 25038 28773 10594 19199  4302 25758 11367  6928 18462 17036 21281  7044 \n#>     2     1     1     1     1     1     1     2     1     2     1     2     1 \n#> 25167  3867  8673  7296  5695  3981 15758 12853 26067  3576 23147 15526 17430 \n#>     1     1     1     2     1     1     1     2     1     1     1     3     1 \n#> 14596  4754 17934 14982  3830 14605 16815 15582 10257 28229  2197  2295 26539 \n#>     2     2     1     1     1     1     2     2     1     1     1     1     1 \n#> 25939 26958 10646 12340 27689 11626 29148 22914 22763 15662  9088 26544 12998 \n#>     2     1     1     1     1     1     1     2     2     3     1     1     1 \n#> 10600 23094 12433  3419 24078 12838   343 23246 29948 29649  9702 27089 28922 \n#>     2     1     1     1     1     2     1     1     3     2     1     1     1 \n#>  4237 15264  3097 21726 24230  7457 29316 23097  8867  7697 18521 23887  1360 \n#>     2     1     2     1     1     2     1     1     2     1     1     3     1 \n#> 17694 10826  3936 13529 29909  7868 15391  3310   512 24702  5443 14888 19572 \n#>     3     2     1     2     1     2     2     1     1     1     1     1     1 \n#> 23450 27224 13146 14232  2657 23453 13436 28537 11893 18354 20356 16179  8102 \n#>     2     1     2     1     1     2     1     1     1     1     2     2     2 \n#> 29092 28878 17898 10617 17094  6286  5622 22858    32 24744  8776 12560 15168 \n#>     1     1     2     2     1     1     1     1     2     1     1     2     1 \n#>  6041  5973 24538   866 24847 22918  6333  2505 17872 14588   506  9754 29452 \n#>     1     1     2     1     1     1     1     2     2     2     1     1     3 \n#> 10459 26520 11129 15046 20441 11566 12722 10370 28998 21170  8658 13789 27920 \n#>     2     1     2     1     1     2     1     1     3     1     1     1     1 \n#> 25691 25979  8853 13043  8110 26243   519 20301 14834  3057 19248 19513  5049 \n#>     1     1     1     1     2     1     1     1     2     1     1     2     1 \n#> 19269 14102 19224 21167  5445 16434  8715 10983  9991  5725 23911 23146 21404 \n#>     1     2     1     1     1     2     1     1     1     2     1     2     2 \n#> 13012 24878 13332 18805 14486  3834 13927 19437 10481 17883 14538 25005  1813 \n#>     2     1     1     1     2     1     1     1     1     2     1     1     2 \n#> 22107  4426  2969 11903 21525 23562 20836 24697 19217 29188  9851  6572 20446 \n#>     1     1     1     1     1     2     1     1     1     3     1     1     1 \n#> 14683 26041  3777 21241 23843 21675 20313  5470 12978  9936  8418  4596 21790 \n#>     1     2     1     1     2     1     2     1     2     3     1     1     1 \n#>  4017 12261 27394 17340  3475  9211 10567 12643 21747  6811 29416 26387 15853 \n#>     1     1     1     1     1     1     2     1     1     1     2     1     2 \n#> 27699  6365 11415 22513 21224 11798 29175  2824 20507  4393  5983 23319  7735 \n#>     2     1     3     1     1     2     1     1     1     1     1     1     1 \n#> 26340 24824 22912 25788 11642 13554 26293 10460 16128 16012  5572 10368 25599 \n#>     1     2     1     3     1     1     1     2     2     2     1     1     2 \n#>   300 11235  2500 20673  2382   276 22096 20895 21308 19149 21780  6009 24286 \n#>     1     1     1     1     1     2     1     1     2     2     2     1     2 \n#> 15775 25034 10052 25022 17265 22071 25463 21317 22408 23123 28428 27159 24939 \n#>     1     2     1     1     1     2     1     1     1     1     1     2     3 \n#>  9978   101 23194 28843 27749 14416 18880 17410 29475  8568  4549 10359 11522 \n#>     2     1     1     1     1     2     1     2     2     1     2     1     1 \n#> 25319  9344 23552 25243 14114  4589 13902 18938  4222 24087 27978 16518 22732 \n#>     1     2     1     2     2     1     1     3     1     2     2     2     1 \n#> 18855 17022 22000 16888 24237  9854 22337  4272  3784 17514 29667  6244  4570 \n#>     1     1     2     2     2     2     2     1     1     1     2     1     1 \n#>  6030 14379  2570 14930  1042 19138 16836 17118  1811 23644 11588 18097 16546 \n#>     1     3     1     2     1     2     2     2     1     1     1     1     1 \n#> 28182  2234 10856 26015 29504 29710 20344  9616 29438 22875  9238 21500  4532 \n#>     1     1     2     3     1     3     1     2     1     1     2     1     2 \n#> 26116 29519 18863 15353 10419 25242  7573  6307 27944  5472 26418  5662  2419 \n#>     1     2     2     1     2     1     1     2     2     1     1     2     1 \n#> 25035  8356 19785 17492 12758 17616  5575 15492  3711 20248 10189 21698 20831 \n#>     1     2     1     1     1     1     1     2     1     1     1     1     1 \n#>  2192 24718  4903 19885  7065 20212 21520 27746 20916  2162  4290 15042  3824 \n#>     1     2     2     1     2     2     2     1     1     1     1     1     2 \n#> 27576 20866  9846 28771 27364 15008 12344  6740  2241  8289 27254 22682  2579 \n#>     2     1     1     1     3     1     2     1     1     1     1     2     1 \n#> 26174 12680 21130 16685 13525  3677   415 29871   640 13317 29306  9070 23745 \n#>     1     1     1     1     1     2     2     1     1     1     1     2     1 \n#>  1781  2444 17842  4345 24135 27386 12462 27167 21918 24535 20648 11873 10946 \n#>     1     2     1     1     1     1     1     1     1     1     1     1     1 \n#>  7006  9111  5388 26415 14735 17480 26020  3629 24711 25026 12134  6446 12072 \n#>     1     1     1     1     1     1     2     1     1     1     1     1     1 \n#> 26630 15918 27899 25563 24988  3510  5835  6804 16811 22494 19306 21984 13516 \n#>     2     2     2     2     2     2     1     3     1     1     1     2     2 \n#> 14660 19432 19957 11950 16602  4011 16468 13132 12429 21086 13583 11042 22407 \n#>     1     1     1     1     2     1     1     2     1     1     2     1     2 \n#>  3107  9946 11779  2998 16281  1462 15537 25422 17433 18082   277 24870 19421 \n#>     1     2     2     1     1     2     2     3     3     1     1     1     2 \n#>  7584 20105  4113 22894 20726 10008 27717 11210 24571 14784  9265 23568 14087 \n#>     1     2     1     1     1     2     1     2     2     1     1     1     1 \n#> 10850 10714   906 19813 28288 29518 25238 13398 16103  4981 18515 18532 20966 \n#>     1     2     2     1     2     1     1     1     2     1     2     2     1 \n#>  3405  7766 21769 22804 27009 11602 12401  8861 15625 21579 15012 17310 27143 \n#>     1     1     1     2     2     1     1     2     1     1     1     2     2 \n#> 29162 15153 27911 26927 22249 29707   724  7016 22410  4482 21215   325  4278 \n#>     2     2     2     2     3     2     1     1     2     2     1     1     2 \n#> 21693 11711  2599 29373 25107 13996  5454  6495 16316  7365 29159 14032 24053 \n#>     2     2     1     1     2     1     1     1     2     2     1     1     1 \n#> 21910 11796 11953 17470 22155 22796 14445  1457 25303   780 12490 19035  5669 \n#>     1     1     1     2     1     1     1     1     1     1     1     1     1 \n#>  2674 21272 29138 26527  2096   778 25937 16310 15596 18254  7834 20461  8290 \n#>     1     1     1     2     1     1     2     1     1     2     1     2     3 \n#> 20592 13515 24104 10081 15591  4741 20174  8489  3567 16662 19160 28899  5482 \n#>     1     1     1     2     1     1     1     1     1     1     2     1     1 \n#> 22298  5027  3679 21185 26158 14582 13413 19628  3201 23989 15165 14363  7758 \n#>     1     2     1     1     1     1     2     1     3     2     2     2     2 \n#> 23294  2200 23975  8703 25851  2469  1089 12528 12650 20168 26162 15440  2693 \n#>     1     2     2     1     2     2     1     2     1     1     2     1     1 \n#>  2279 21653 13104 13953 19253  7938 25971  6406  2187  5365  6008 24288 29613 \n#>     1     1     1     2     1     1     1     1     2     2     2     2     1 \n#>  3228 10204  8900 11083 22076  9248 22133  8133 17385 15077  6830 11570 28742 \n#>     2     1     1     2     2     1     1     2     1     1     1     1     1 \n#> 17807  4155 18337 24468  7815  6263 11872  9738 21313 21592 26289 10776 13856 \n#>     1     2     1     1     2     2     1     1     1     1     2     2     1 \n#> 22668 12070  5463 20309 18453 25472 26023 12820 26739 27251  1264 10681 16785 \n#>     2     2     2     3     3     2     2     1     1     1     1     2     1 \n#> 23291 16600 11498 23627 15498 18471 20885 17650  3924  8090  9198  3054 21099 \n#>     1     1     3     1     1     1     2     2     1     1     1     2     2 \n#> 14638 22211 18116  5850 27260 25065  8538  9312   467 22769 13563 19314 22342 \n#>     3     2     2     1     1     2     2     1     1     1     2     1     2 \n#>  5100  6763 12909 25881 25170 13972 13024 17030 12139 28904 18635 18075  3401 \n#>     1     1     1     2     1     2     1     1     2     1     2     1     2 \n#>  1459  7512 21438 18942 12874 20805 28281  3508  8580 21645 28979  1339  4386 \n#>     2     2     1     2     3     1     2     2     2     2     1     1     1 \n#> 17041 24289 28581 16300  6902 27769  1262 22660 26246 19716 16347 28525 23970 \n#>     2     2     1     2     1     3     1     2     2     1     2     1     1 \n#>  3235 13520  1605 29260  9658  6064  1479 10606  8227 24642 21478 13785 23044 \n#>     1     1     2     3     2     1     1     2     2     2     1     2     1 \n#> 26404  7668 17368 29311  5432  2683 14104 11688 12037 27606  4554 19666 24845 \n#>     1     1     2     1     2     2     1     1     1     1     1     2     1 \n#> 10758 11151 11870  5248 21148  2864 18401 29284 13150 20418  9227 10203 12467 \n#>     1     2     1     2     1     1     1     1     1     1     2     1     1 \n#>  4283 14169 18444 10275  3229  4342 19352  3516 23643 10201 14437  7495 11494 \n#>     1     2     1     1     2     2     1     1     1     2     2     1     1 \n#>  5639  3224 27295 17190 18267 27904 16645 23507  7926  9996 17206 22322 22851 \n#>     1     1     1     3     1     1     2     2     2     1     3     2     1 \n#> 23895 12873   270 29644 13022  8606  8761 13777 14257 21681  6905  5032 16864 \n#>     1     2     1     2     1     2     1     1     1     2     1     2     1 \n#> 11064 12160  6315 11996  1154 12989 26987 26285 19557 14637 26355  7493 26114 \n#>     1     2     1     1     2     1     1     1     1     1     1     3     1 \n#> 22486 15376  8287 26678 20860 15282 11789 18617 21844  6700 25227  6212   956 \n#>     1     1     1     3     1     2     2     1     2     3     2     1     2 \n#> 19868 19262  5121 17095  7803 22253 14401  1739 27815 28735 24732 15624 11708 \n#>     2     2     2     3     1     1     1     1     1     2     1     2     1 \n#> 29192  8483  3754 14790   978 28263 20374 13202 12415  6534  4009 23572 10279 \n#>     2     2     1     1     1     2     1     1     1     1     1     2     2 \n#>  1861  4029  5191 17085 28912 21172 20664  9053  9581 12671 15877 18485 11046 \n#>     1     3     1     2     1     1     2     2     1     1     1     3     1 \n#> 27349 29298 21261 24629  5351 11153 13094  8556 16283  3595 19369  5576  5750 \n#>     1     1     2     1     1     1     1     1     2     1     1     3     1 \n#>  9006 25306 23935  3546 21963  8326 19711 20588 20330 16670 13470   412  2423 \n#>     1     1     1     1     2     1     1     2     2     2     1     1     1 \n#> 19846  8153 24670 19553  5193 19190 25373 21646 16267 14785 15212   580 29658 \n#>     3     1     1     2     3     1     2     1     3     1     1     2     1 \n#> 10842 28333 13798 29548  8550 12029 10333  7002 13986 18786 13716 18864  4722 \n#>     1     1     2     2     1     3     1     2     1     2     2     2     2 \n#> 26558 11314  2454 18025   502 28212  7914  5090 28377 17506  3671  6010 24154 \n#>     2     2     1     1     2     2     2     1     1     1     1     2     2 \n#>  9041  1266 23854  7449 12411 25795  1455 17535 28571  4022 22536  2595  7432 \n#>     2     1     1     1     1     1     1     1     1     1     1     1     3 \n#> 16857  5899 10624 11739 18044 11581 15855 20352  1519  5809  5269  7685  4357 \n#>     1     1     1     1     1     1     1     2     2     2     2     1     1 \n#> 14697 17760 17418 29372  9925 10440 17635 23081 15681 12066 19614 27407 10154 \n#>     1     2     2     2     3     1     3     1     1     2     1     1     2 \n#>  4148 29916 22641 28365 27643 21957  9456  3869 29761 14685 20185 29996  6714 \n#>     1     2     2     2     1     2     1     1     2     1     2     1     1 \n#>  4080  7349 19746 13645 22532 12815  1651 25277 18101  6094 24228 12043   508 \n#>     2     2     1     2     2     2     1     1     1     1     1     2     2 \n#>  3723  7986  1454  5422  5995 29203  1460 16282  8439 23271 14176 29736 28651 \n#>     1     1     1     1     2     1     1     2     2     3     1     2     1 \n#> 13614 16233 19574  1817  2363 13744 23046 10012  9272 16556  1010  1357  2910 \n#>     2     1     1     2     1     1     1     1     1     2     1     3     1 \n#> 10745 25865  7322  4917 20944 18912 13680  1378 18194 17210 22910  1673 16633 \n#>     1     1     2     2     1     2     1     1     1     1     1     2     1 \n#> 12711  4496  5072   652 27811 11267 23709  6718  4951 11985 27023 22180  8814 \n#>     3     1     1     1     2     2     2     1     2     1     1     1     1 \n#>  5404 25860 17117 28058 18831 26286 29011 25282 24601  9306 21695  1086 11556 \n#>     1     1     1     2     1     1     1     1     2     3     2     1     1 \n#> 23558 12305 16966 17871 11587 20465 10344  6836 23027  2553  3559 23204  2175 \n#>     1     2     1     2     1     1     1     1     1     3     1     1     1 \n#>  3739  4088  8681 10013  2676  8040  7106   833 19993 10589 26686 12148 24637 \n#>     1     2     1     2     1     1     1     2     1     2     2     2     1 \n#> 23378  6430  2493  3000 10756   720 17802  8475  6755 29157 15705  8508 13463 \n#>     1     1     1     1     1     2     1     2     1     2     1     1     1 \n#> 18341 18707 23479  4586 27385 14345 15953 17901 23734 14413 15434  9511  7291 \n#>     1     2     1     2     2     1     1     1     1     1     1     1     1 \n#> 13603 12400 25469 16678 16999  5323  6416 15140 17517 20269 27484 10316  5371 \n#>     1     1     1     2     1     1     2     1     2     1     1     1     1 \n#> 27303 26897 15883 22367 28597  9576 21789  8464  5743 12079 23064 12254 22353 \n#>     1     1     1     2     3     2     2     1     3     1     2     1     1 \n#> 20442  7523 28640 21075 16481  6438 17642 29461 16184 28158 16062 29723 25404 \n#>     1     1     1     1     1     2     1     3     1     2     1     1     1 \n#> 24362 11250  3063  7488 19012 12613  9298 14344 21311 13840  9799 24788 29549 \n#>     2     1     1     2     1     1     2     2     2     1     1     2     1 \n#> 28099 13709  8189 13759  4392  9262 24635 18729 25943  8852  6815 11561 24164 \n#>     1     1     1     2     1     1     1     1     1     1     1     1     2 \n#> 16788 26720 24603  1515  7980 16086 19775 19889 24339 23244  1733  9474 24371 \n#>     1     1     2     2     1     1     1     2     1     1     1     1     2 \n#>  3089 25190  4588  6268  1498 18119 22615  8024  5161 16101  8801 22289  9207 \n#>     2     2     2     1     1     1     2     1     1     1     1     3     1 \n#> 19347 18865 18192 25941 12099 19906   518 20591  5909   526  8372 18646 22571 \n#>     2     2     1     3     2     1     1     2     2     1     2     1     2 \n#> 10578  3029  9084  1638 18246 10347 25083 16361 22465  5390 12551  5440  8701 \n#>     2     1     1     1     1     2     2     1     3     1     1     1     1 \n#> 15368 17701 15858 11371 22548 15254 20675  6684 12977  2789 15731  5224 10786 \n#>     1     1     3     1     1     2     2     1     2     2     1     2     1 \n#> 12032  4916  6903  8137 21138  3081  6460   450 15186  5076 14033 15225 15396 \n#>     2     1     1     1     1     1     3     2     1     2     1     1     1 \n#> 21249 15211  6686 13942   914  2388 17846 10072  2927 27897 17380  9670 22934 \n#>     2     2     2     1     2     2     1     2     1     1     3     1     1 \n#>  6345  3641 12436 18284 23556 14643  4110 13821  1182 21326 29766 17505 11625 \n#>     1     1     1     1     1     2     1     1     1     3     3     1     1 \n#> 11466  4185  4316 10084 27684 11338 20336 20085  5364 26909 10406 18724  2924 \n#>     1     2     2     1     2     1     1     2     1     1     2     1     2 \n#> 14670  3294 26301 23408 27975 15390 12690 28593 24700 25981 22061 18151  9062 \n#>     2     1     1     2     1     1     1     2     2     3     1     3     2 \n#> 16433  7513  7597  6255 27664 28053 16368  6649  8072 13384 15633  2713 14289 \n#>     1     2     2     1     1     2     2     1     1     1     1     1     1 \n#>  8146 16100  7450  9210 17249 16555 27418   644  8172  7398  4483  8790 29043 \n#>     1     1     1     1     2     1     1     2     1     1     1     1     1 \n#> 28175 21010  9296  8294 18734 19882  9763  4170  3472  1744 21641 20803 22613 \n#>     1     1     1     1     1     2     2     1     1     2     1     1     2 \n#> 24715 17817 28350  1912 11416 16197  6632 21554 14690  8772  1643 24057 20380 \n#>     2     3     1     1     1     1     1     1     1     2     1     1     1 \n#>  9952 24805 24097 26371 14841 24192  1619 14201 27366 11736 29719 21443 17402 \n#>     1     1     1     1     2     1     2     2     1     1     1     1     1 \n#>  3211  5501  4906 10304  1446  7325 27139  1518  9319 29242 18398  6865  1301 \n#>     1     3     1     1     2     2     1     2     1     1     1     2     1 \n#> 20982 21087 17738  8396  8500  7483  8699  4611  2009  5976 17164 21782  2651 \n#>     1     2     1     2     1     2     1     1     1     2     1     1     1 \n#>  2650 25250 29829 11449 19984 26257  3696  1925 23169 20023  2012  5902 19860 \n#>     2     1     3     2     1     1     1     1     1     1     1     1     1 \n#> 21756 20410 12159  3168 16383 11061 23207  5128  4796 26744   315 26788 22321 \n#>     1     2     1     1     3     1     2     2     2     2     1     1     1 \n#>  4092 19793 27750 13125 22614 16757 11813 10519 20173  7796 14464  1393 18447 \n#>     2     1     1     2     2     1     1     2     1     2     1     1     1 \n#> 22055 10047 17852 14724 13512 11781 28021 20229 15147 20118 24688 18946 19943 \n#>     1     2     1     1     1     2     2     2     2     1     1     1     1 \n#> 25399 25685 15033   103 28215  2172 14178 11619 23162 13374 26566  6948 21274 \n#>     1     2     1     1     2     1     2     2     1     2     1     1     2 \n#>  7279 16847 11751 11085  6960 17632  6360 28355 14747 21886 14465 20774  7982 \n#>     1     2     1     1     2     1     1     1     2     2     1     1     2 \n#> 14091 29243 22952 17382 28848 22267  6386 14517 27511 10269 13741 15039 22738 \n#>     2     1     3     1     1     1     1     1     1     1     1     1     2 \n#> 17648 26432   477 19804 20216 14668 19235 13988  9215 23080  2487  8457 16697 \n#>     1     2     2     1     1     2     1     2     2     2     2     2     1 \n#>  2609  7733 21481  8560 18953 10128  6568 18505 15599 16804  1129  3859  2895 \n#>     2     2     3     1     2     1     1     2     2     1     2     1     1 \n#> 16795 25271 21466 20152 28201 11982 23328 15322  5998 21046 12217 24318 27611 \n#>     1     1     1     2     2     1     3     1     2     2     1     1     2 \n#>  6526 18975 17748  1957 17426  6921 13065 29928  4534 17921 27759 26433  5069 \n#>     1     1     1     1     2     3     2     1     2     2     1     1     1 \n#> 14514 13186 20983  1290  1869 29471 29442 16457 16646 11141 11139 20593 16850 \n#>     1     1     1     2     1     1     1     1     1     3     1     2     2 \n#>  9023  2409  5092 21964 17574 28209 20204 15313  2883 27982 22678  8830  7625 \n#>     2     1     1     2     2     2     2     1     1     2     1     2     2 \n#> 25389 20517 25863 21305 17844 10987  6356 22993 16767  7196 17788 26904 28461 \n#>     2     2     2     1     1     2     1     1     1     1     2     1     1 \n#> 15774  4494  1763 25854 13391 15413 25124  2008 29017 22802  9835 14015 10730 \n#>     1     1     1     1     1     2     1     1     1     1     2     1     1 \n#> 25702 27177 18722 11459 19349  8108  1627  3366 24839 10834 13269 28280 27215 \n#>     1     1     1     1     1     1     1     1     1     1     1     1     1 \n#> 25579 12813 14222 24595  4003 29350 26024  3885  7674  4416  3185 29843 27491 \n#>     1     1     2     1     1     2     2     1     2     1     1     1     1 \n#> 16049 27565 21043 28193  3556  6334 21444 26961 13372 21618 23405  8626 25973 \n#>     2     2     1     1     1     1     1     2     2     1     2     3     1 \n#>  7667  3427 17584 20630  6850  8621 12925 13213 29061 26413  3409 12799 16613 \n#>     2     1     3     2     1     1     2     1     1     1     1     1     2 \n#> 18968 11009 19783 18543  7773  6279  1267  1003 16682 15432  9984 18018 12648 \n#>     1     2     1     2     2     1     1     1     1     1     2     2     1 \n#> 27954  3648  6316 25142 16681 17926  4068   105 11166 12390 17754 16377 18956 \n#>     3     1     1     1     1     1     2     3     1     1     2     3     1 \n#>  9086 29160   803 15865 16025 18941  4193 28440 12830  4825 25295 14688 25169 \n#>     2     2     2     1     2     1     1     1     1     1     1     1     1 \n#> 16314  9917 27106 17088 10426 21242 28770 21585 26653 28790 24835 20197  2640 \n#>     1     1     1     1     2     1     1     1     2     1     1     1     1 \n#>  1297 17866 28421  4460 10746  9253 26971  7308 14501 12211  1622  5771 16768 \n#>     1     1     1     3     2     2     1     1     1     1     1     1     1 \n#>  3582 22562  3873  2331  4216 10945 28809 29158  7564 20395 28923  5607 20692 \n#>     1     2     3     1     2     1     1     1     2     2     2     3     2 \n#> 23392  4124 19920  4120 18439 13965 27973 15229 26808  8711 23151 18650  6281 \n#>     3     1     1     1     2     1     1     1     2     1     3     2     2 \n#> 22601   857 27229  1175 15826  9778 27132 12495  4779 12299 25719 29604 24569 \n#>     3     1     2     1     1     1     1     1     1     2     2     1     1 \n#> 15134 23320 19879 25588 18757 11496 28018  8467 14815 16380 23231 18534 15490 \n#>     2     1     2     1     1     2     1     1     3     2     1     1     1 \n#>  1333   583 21759  3519 15163  8831 24266  6637 28126 20087 11849 27283  1754 \n#>     1     1     2     1     1     2     2     1     1     1     2     1     1 \n#> 14899  9403  5476 23663 19054 19067 12800  2511  1725  4680 20655 22945 12024 \n#>     3     2     1     1     1     3     1     1     1     1     1     1     1 \n#>  7110 29224  6314 26383 18070 11345  1905 14189 16390  1818 17567 27820  8513 \n#>     2     1     1     1     1     1     2     1     1     2     1     1     1 \n#> 23776 26886 19910 13586  1984 24261 21978 26314   262  6123 20896  4576 11346 \n#>     1     1     1     1     1     1     2     1     2     2     1     2     1 \n#>  6214 11181 17241  2615 12062 13693  6039  2645 13968 26359 23863  7780 26730 \n#>     2     1     1     2     1     1     1     1     2     1     1     1     2 \n#>  7052 23543 11882 18156 15270 10718  4932  4264 23415 13282 14132 25396  1646 \n#>     1     2     1     2     1     1     1     1     1     2     3     1     1 \n#> 25283 16663  3260  4875 27099 18875 16418 24283 28804 25606 17806 24687 15810 \n#>     1     1     2     1     2     1     2     1     1     1     1     1     2 \n#>  3845 14978 22137  7881 17478 24349  4582  1881   406   823   157 28724  2440 \n#>     1     1     1     1     1     1     1     1     1     1     1     1     2 \n#> 19376 15541 21007  9423   835 19933  7966  2892 13145 11180 19582  6380   178 \n#>     1     2     1     2     2     1     1     1     1     1     1     1     2 \n#> 13725 22791 20333 22221 29840 21089 11460 23615 17653  5872 21553 22468 21503 \n#>     1     1     1     1     1     1     2     2     2     3     1     1     1 \n#> 20167 14483 14881 23875 25655 28172 18382 21786  7199  7130 21717   219  1976 \n#>     1     1     2     2     2     2     1     2     1     1     3     1     2 \n#>   671  2373  1629 14405  7508 14076 20659  4961   911 10910 26757  9592  5680 \n#>     3     1     1     2     1     2     1     1     2     1     1     1     1 \n#>  6459 11411 15032 21413 22546  1749  2132 22652 20360 21609  8419 24457 19512 \n#>     2     2     1     2     1     1     1     2     2     1     2     1     1 \n#>  5375 26963 11810 25315  5465  4648 26504 26642 26000 25411 22243  9095  7067 \n#>     1     1     1     1     3     2     1     1     2     1     2     3     1 \n#> 27464 17592 28259 20015 19264 26849  2978 29665 29356  5854 29352  2970   171 \n#>     2     1     1     1     1     1     3     1     1     1     1     1     2 \n#>  8592 14776 29888  6006 18442   650 14876 21401 19411  6144  3337 23078 29437 \n#>     1     2     1     1     1     2     1     1     1     1     1     2     2 \n#>  1987 18727 19151 26295 18816 27066  4203  3636 19140 27146  5503 25080  1229 \n#>     2     2     1     1     1     1     1     2     1     1     1     1     3 \n#> 11846  5674 26518  6898  4911  4430  1953 15750 11146 25407   548 15023 11717 \n#>     2     1     2     2     2     1     1     1     2     2     1     1     2 \n#> 18294  5700 19935 29484 12512 26271 14224 22779  2333 10071 11307  1072 12592 \n#>     1     1     1     2     1     2     1     2     1     1     2     1     1 \n#> 28484 10738  3164  8425 14239 23632  9449 29693 29696 20851 27655 24313 14245 \n#>     1     1     1     2     2     2     2     2     1     1     2     2     1 \n#> 22283 27908 13528 \n#>     1     2     1 \n#> \n#> Within cluster sum of squares by cluster:\n#> [1]  801109.2  922232.7 1041005.6\n#>  (between_SS / total_SS =  78.9 %)\n#> \n#> Available components:\n#> \n#> [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#> [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\nfviz_cluster(km, data = teens_2 , stand=F)\nset.seed(123)\nfviz_nbclust(teens_2 , kmeans, method = \"wss\")\nset.seed(123)\nfviz_nbclust(teens_2, kmeans, method = \"wss\")\nset.seed(123)\nfviz_nbclust(df, kmeans, method = \"silhouette\")"},{"path":"clustering.html","id":"cluster-intuition","chapter":"6 Clustering","heading":"6.3 Cluster intuition","text":"cluster intuition k-means, apply methods, hierarchical clustering.teen_clusters <- kmeans(data, k)Transforming matrix, making plot.\n.matrix(teen_clusters$centers)Function make bar plot","code":"\nteens_scale<-as.data.frame(lapply(teens_2[,2:41], scale))\nsummary(teens_scale)\n#>      gender             age               friends          basketball     \n#>  Min.   :-2.0727   Min.   :-3.306980   Min.   :-0.8569   Min.   :-0.3403  \n#>  1st Qu.: 0.4824   1st Qu.:-0.829209   1st Qu.:-0.7455   1st Qu.:-0.3403  \n#>  Median : 0.4824   Median :-0.007369   Median :-0.2720   Median :-0.3403  \n#>  Mean   : 0.0000   Mean   : 0.000000   Mean   : 0.0000   Mean   : 0.0000  \n#>  3rd Qu.: 0.4824   3rd Qu.: 0.843353   3rd Qu.: 0.3687   3rd Qu.:-0.3403  \n#>  Max.   : 0.4824   Max.   : 2.396008   Max.   :11.8721   Max.   :15.4215  \n#>     football           soccer           softball         volleyball     \n#>  Min.   :-0.3696   Min.   :-0.2452   Min.   :-0.2245   Min.   :-0.2259  \n#>  1st Qu.:-0.3696   1st Qu.:-0.2452   1st Qu.:-0.2245   1st Qu.:-0.2259  \n#>  Median :-0.3696   Median :-0.2452   Median :-0.2245   Median :-0.2259  \n#>  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n#>  3rd Qu.:-0.3696   3rd Qu.:-0.2452   3rd Qu.:-0.2245   3rd Qu.:-0.2259  \n#>  Max.   :13.8005   Max.   :28.5005   Max.   :16.2346   Max.   :17.2642  \n#>     swimming        cheerleading       baseball          tennis       \n#>  Min.   :-0.2829   Min.   :-0.211   Min.   :-0.193   Min.   :-0.1663  \n#>  1st Qu.:-0.2829   1st Qu.:-0.211   1st Qu.:-0.193   1st Qu.:-0.1663  \n#>  Median :-0.2829   Median :-0.211   Median :-0.193   Median :-0.1663  \n#>  Mean   : 0.0000   Mean   : 0.000   Mean   : 0.000   Mean   : 0.0000  \n#>  3rd Qu.:-0.2829   3rd Qu.:-0.211   3rd Qu.:-0.193   3rd Qu.:-0.1663  \n#>  Max.   :15.8853   Max.   :16.050   Max.   :25.838   Max.   :22.9600  \n#>      sports             cute              sex               sexy        \n#>  Min.   :-0.2982   Min.   :-0.4074   Min.   :-0.2128   Min.   :-0.2697  \n#>  1st Qu.:-0.2982   1st Qu.:-0.4074   1st Qu.:-0.2128   1st Qu.:-0.2697  \n#>  Median :-0.2982   Median :-0.4074   Median :-0.2128   Median :-0.2697  \n#>  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n#>  3rd Qu.:-0.2982   3rd Qu.:-0.4074   3rd Qu.:-0.2128   3rd Qu.:-0.2697  \n#>  Max.   :25.1558   Max.   :18.1970   Max.   :46.3166   Max.   :22.5086  \n#>       hot              kissed            dance              band        \n#>  Min.   :-0.2614   Min.   :-0.1958   Min.   :-0.3761   Min.   :-0.2958  \n#>  1st Qu.:-0.2614   1st Qu.:-0.1958   1st Qu.:-0.3761   1st Qu.:-0.2958  \n#>  Median :-0.2614   Median :-0.1958   Median :-0.3761   Median :-0.2958  \n#>  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n#>  3rd Qu.:-0.2614   3rd Qu.:-0.1958   3rd Qu.:-0.3761   3rd Qu.:-0.2958  \n#>  Max.   :18.5623   Max.   :45.3319   Max.   :18.7830   Max.   :19.2610  \n#>     marching           music              rock             god         \n#>  Min.   :-0.1366   Min.   :-0.6308   Min.   :-0.339   Min.   :-0.4049  \n#>  1st Qu.:-0.1366   1st Qu.:-0.6308   1st Qu.:-0.339   1st Qu.:-0.4049  \n#>  Median :-0.1366   Median :-0.6308   Median :-0.339   Median :-0.4049  \n#>  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.000   Mean   : 0.0000  \n#>  3rd Qu.:-0.1366   3rd Qu.: 0.1974   3rd Qu.:-0.339   3rd Qu.: 0.4508  \n#>  Max.   :33.7073   Max.   :21.7306   Max.   :24.272   Max.   :21.8440  \n#>      church            jesus            bible             hair        \n#>  Min.   :-0.2773   Min.   :-0.194   Min.   :-0.105   Min.   :-0.4028  \n#>  1st Qu.:-0.2773   1st Qu.:-0.194   1st Qu.:-0.105   1st Qu.:-0.4028  \n#>  Median :-0.2773   Median :-0.194   Median :-0.105   Median :-0.4028  \n#>  Mean   : 0.0000   Mean   : 0.000   Mean   : 0.000   Mean   : 0.0000  \n#>  3rd Qu.:-0.2773   3rd Qu.:-0.194   3rd Qu.:-0.105   3rd Qu.:-0.4028  \n#>  Max.   :47.2192   Max.   :49.328   Max.   :37.729   Max.   :17.0580  \n#>      dress             blonde             mall            shopping      \n#>  Min.   :-0.2432   Min.   :-0.1838   Min.   :-0.3705   Min.   :-0.4936  \n#>  1st Qu.:-0.2432   1st Qu.:-0.1838   1st Qu.:-0.3705   1st Qu.:-0.4936  \n#>  Median :-0.2432   Median :-0.1838   Median :-0.3705   Median :-0.4936  \n#>  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n#>  3rd Qu.:-0.2432   3rd Qu.:-0.1838   3rd Qu.:-0.3705   3rd Qu.: 0.8737  \n#>  Max.   :20.2934   Max.   :39.8700   Max.   :16.5196   Max.   :10.4445  \n#>     clothes          hollister        abercrombie           die         \n#>  Min.   :-0.3166   Min.   :-0.2004   Min.   :-0.1825   Min.   :-0.3078  \n#>  1st Qu.:-0.3166   1st Qu.:-0.2004   1st Qu.:-0.1825   1st Qu.:-0.3078  \n#>  Median :-0.3166   Median :-0.2004   Median :-0.1825   Median :-0.3078  \n#>  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n#>  3rd Qu.:-0.3166   3rd Qu.:-0.2004   3rd Qu.:-0.1825   3rd Qu.:-0.3078  \n#>  Max.   :16.1428   Max.   :22.1627   Max.   :23.7896   Max.   :26.2269  \n#>      death             drunk             drugs             female       \n#>  Min.   :-0.2529   Min.   :-0.2217   Min.   :-0.1757   Min.   :-2.0727  \n#>  1st Qu.:-0.2529   1st Qu.:-0.2217   1st Qu.:-0.1757   1st Qu.: 0.4824  \n#>  Median :-0.2529   Median :-0.2217   Median :-0.1757   Median : 0.4824  \n#>  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n#>  3rd Qu.:-0.2529   3rd Qu.:-0.2217   3rd Qu.:-0.1757   3rd Qu.: 0.4824  \n#>  Max.   :30.9186   Max.   :19.6631   Max.   :32.2532   Max.   : 0.4824\nset.seed(2345)\n#Ayer  \nteen_clusters <- kmeans(teens_scale, 2)\n\ncentroids<-teen_clusters$centers\nclass(centroids)\n#> [1] \"matrix\" \"array\"\nfviz_cluster(teen_clusters, data = teens_2 , stand=F)\nbarplot(height =centroids,main=\"Centroids\",legend.text = TRUE,\n        beside = TRUE,col=c(\"red\",\"blue\"),las=2)\n# data is a matrix object with the centroids\n# name is the plot name (main argument)\nmy_plot<-function(data,name){\n  \nbarplot(height =data,main=name,legend.text = TRUE,\n        beside = TRUE,col=c(\"red\",\"blue\"),las=2)}\n\nse<-seq(1,2,1) \nhc_caract<-centroids[,c(\"gender\",\"age\",\"friends\")]\n\nmy_plot(hc_caract,\"Características generales\")"}]
