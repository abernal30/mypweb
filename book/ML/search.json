[{"path":"index.html","id":"machine-learning-introductory-guide","chapter":"Machine learning introductory guide","heading":"Machine learning introductory guide","text":"book Machine learning introductory guide!work Aturo Bernal\nVisit GitHub repository site.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"text examples spirit generating basic guide Machine Learning (ML) methodology. writing book inspired students AI concentration Tecnologico de Monterrey spring 2022. moment, document based mainly (Géron 2019), text James et al. (2017) Lantz (2015). However, expect fill whit materials experience.\nStudents welcome contribute, case, get credit text.\ndocument follows practical example: ) continuous variables California Housing Prices data set presented Géron (2019); ii) categorical variables, follow Lending Club fintech data set Kaggle analyze credit default. financial institution grants personal loans; iii) categorical variable Bit-coin prediction.following steps may differ books, data scientists, experts many ways deal whit machine learning. precisely richness area expertise. existed one way apply machine learning, everybody use !!R files stored GitHub repository site.","code":""},{"path":"preface.html","id":"outline","chapter":"Preface","heading":"Outline","text":"","code":""},{"path":"big-picture.html","id":"big-picture","chapter":"1 Big picture","heading":"1 Big picture","text":"","code":""},{"path":"big-picture.html","id":"frame-the-problem","chapter":"1 Big picture","heading":"1.1 Frame the Problem","text":"Answer following:\nbusiness objective? company (client) expect use benefit model?Knowing objective important determine frame problem, algorithms select.housing prices, objective predict district’s median housing price detect investment opportunities, applying OLS ordinary least squares OLS. example, prediction may show house located latitude x, longitude y, housing median age z, total rooms n, etc. median house value 162,000. However, house sale $145,000, may opportunity buy something cheaper cost.Lending club credit default analysis, focus predicting new customer default loan (repay loan ).current solution looks like ()?important comparison, visualize solution. case, current situation often give reference performance, well insights solve problem.case district housing prices, currently estimated house prices manually experts: team gathers --date information district, get median housing price, estimate using complex rules.Regarding Lending club, evidence Lending club applies machine learning methods analyze credit applications. assume many credit financial institution, apply method manually.","code":""},{"path":"big-picture.html","id":"select-a-performance-measure","chapter":"1 Big picture","heading":"1.2 Select a Performance Measure","text":"","code":""},{"path":"big-picture.html","id":"for-continuous-variables","chapter":"1 Big picture","heading":"1.2.1 For continuous variables","text":"Root Mean Square Error (RMSE). gives idea much error system typically makes predictions, higher weight large errors. Shows mathematical formula compute RMSE. mathematical formula compute RMSE :\\[RMSE =\\frac{1}{n}\\ \\sum_{=1}^{n} (y_{}-\\hat{f(x_{}))^{2}} \\]$ $ prediction ith observation (actual), $ y_{} $ observation ith independent variable, n number observations.\\[\\hat{f(x_{})}=\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n}\\]RMSE computed using training data used fit model, accurately referred training RMSE.general, really care well method works training training data. Rather, interested accuracy predictions obtain apply method previously unseen\ntest data.Mean absolute error (MAE). Even though RMSE generally preferred performance measure regression tasks, contexts may prefer use another function. example, suppose many outlier districts. case, may consider using mean absolute error (MAE, also called average absolute deviation;Mean absolute error (MAE).\\[MAE =\\frac{1}{n}\\ \\sum_{=1}^{n} |y_{}-\\hat{f(x_{})|}\\]","code":""},{"path":"big-picture.html","id":"for-categorical-variables-classification-methods","chapter":"1 Big picture","heading":"1.2.2 For categorical variables (classification methods)","text":"Confusion matrixIs table categorizes predictions according whether match actual value. One table’s dimensions indicates possible categories predicted values, dimension indicates actual values. Although seen 2 x 2 confusion matrices far, matrix can created models predict number class values. following figure shows generic confusion matrix.True Positive (TP): Correctly classified class interest. True Negative (TN) Correctly classified class interest. False Positive (FP) Incorrectly classified class interest. False Negative (FN): Incorrectly classified class interest.confusion matrix, one mesures interest accuracy, defined :\\[ accuracy =\\frac{TP+TN}{TP+TN+FP+FN}\\]formula, terms TP, TN, FP, FN refer number times model’s predictions fell categories. accuracy therefore proportion represents number true positives true negatives, divided total number predictions.error rate proportion incorrectly classified examples specified :\\[ error\\ rate =\\frac{FP+FN}{TP+TN+FP+FN}=1-acuracy\\]\nNotice error rate can calculated one minus accuracy. Intuitively, makes sense; model correct 95 percent time incorrect 5 percent time.Sensitivity specificityFinding useful classifier often involves balance predictions overly conservative overly aggressive. example, e-mail filter guarantee eliminate every spam message aggressively eliminating nearly every ham message time. hand, guaranteeing ham message inadvertently filtered might require us allow unacceptable amount spam pass filter. pair performance measures captures trade : sensitivity specificity.sensitivity model (also called true positive rate) measures proportion positive examples correctly classified. Therefore, shown following formula, calculated number true positives divided total number positives, correctly classified (true positives) well incorrectly classified (false negatives):\\[sensitivity =\\frac{TP}{TP+FN}\\]True Positive (TP): Correctly classified class interest False Negative (FN): Incorrectly classified class interestThe specificity model (also called true negative rate) measures proportion negative examples correctly classified. sensitivity, computed number true negatives, divided total number negatives—true negatives plus false positives:\\[ specificity =\\frac{TN}{TN+FP}\\]True Negative (TN) Correctly classified class interest False Positive (FP) Incorrectly classified class interest","code":""},{"path":"data-collection.html","id":"data-collection","chapter":"2 Data collection","heading":"2 Data collection","text":"data sets use text stored Github. house pricing data set retrived :credit data set retrieve :credit data interested Fully Paid Charged features, equivalent -default default respectively.Charge ” means credit grantor wrote account receivables loss, closed future charges. account displays status “charge ,” means account closed future use, although debt still owed.create variable Default, variable loan_status, Fully Paid 0, Charged , 1. Use ifelse function create variable, call Default. Also, eliminate loan_status variable (otherwise duplicated).Bit-coin:","code":"\nlibrary(openxlsx)\nhouse<-read.csv(\"https://raw.githubusercontent.com/abernal30/ml_book/main/housing.csv\")\nlibrary(openxlsx)\ncredit<-read.csv(\"https://raw.githubusercontent.com/abernal30/ml_book/main/credit.csv\")## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nbit<-read.csv(\"https://raw.githubusercontent.com/abernal30/ml_book/main/bit.csv\")"},{"path":"data-collection.html","id":"take-a-quick-look-at-the-data-structure","chapter":"2 Data collection","heading":"2.1 Take a Quick Look at the Data Structure","text":"review data classTo review outliers Nas.","code":"\nstr(house)## 'data.frame':    20640 obs. of  10 variables:\n##  $ median_house_value: int  452600 358500 352100 341300 342200 269700 299200 241400 226700 261100 ...\n##  $ longitude         : num  -122 -122 -122 -122 -122 ...\n##  $ latitude          : num  37.9 37.9 37.9 37.9 37.9 ...\n##  $ housing_median_age: int  41 21 52 52 52 52 52 52 42 52 ...\n##  $ total_rooms       : int  880 7099 1467 1274 1627 919 2535 3104 2555 3549 ...\n##  $ total_bedrooms    : int  129 1106 190 235 280 213 489 687 665 707 ...\n##  $ population        : int  322 2401 496 558 565 413 1094 1157 1206 1551 ...\n##  $ households        : int  126 1138 177 219 259 193 514 647 595 714 ...\n##  $ median_income     : num  8.33 8.3 7.26 5.64 3.85 ...\n##  $ ocean_proximity   : chr  \"NEAR BAY\" \"NEAR BAY\" \"NEAR BAY\" \"NEAR BAY\" ...\nsummary(house)##  median_house_value   longitude         latitude     housing_median_age\n##  Min.   : 14999     Min.   :-124.3   Min.   :32.54   Min.   : 1.00     \n##  1st Qu.:119600     1st Qu.:-121.8   1st Qu.:33.93   1st Qu.:18.00     \n##  Median :179700     Median :-118.5   Median :34.26   Median :29.00     \n##  Mean   :206856     Mean   :-119.6   Mean   :35.63   Mean   :28.64     \n##  3rd Qu.:264725     3rd Qu.:-118.0   3rd Qu.:37.71   3rd Qu.:37.00     \n##  Max.   :500001     Max.   :-114.3   Max.   :41.95   Max.   :52.00     \n##                                                                        \n##   total_rooms    total_bedrooms     population      households    \n##  Min.   :    2   Min.   :   1.0   Min.   :    3   Min.   :   1.0  \n##  1st Qu.: 1448   1st Qu.: 296.0   1st Qu.:  787   1st Qu.: 280.0  \n##  Median : 2127   Median : 435.0   Median : 1166   Median : 409.0  \n##  Mean   : 2636   Mean   : 537.9   Mean   : 1425   Mean   : 499.5  \n##  3rd Qu.: 3148   3rd Qu.: 647.0   3rd Qu.: 1725   3rd Qu.: 605.0  \n##  Max.   :39320   Max.   :6445.0   Max.   :35682   Max.   :6082.0  \n##                  NA's   :207                                      \n##  median_income     ocean_proximity   \n##  Min.   : 0.4999   Length:20640      \n##  1st Qu.: 2.5634   Class :character  \n##  Median : 3.5348   Mode  :character  \n##  Mean   : 3.8707                     \n##  3rd Qu.: 4.7432                     \n##  Max.   :15.0001                     \n## "},{"path":"data-collection.html","id":"create-the-training-and-test-set","chapter":"2 Data collection","heading":"2.2 Create the training and Test Set","text":"wise machine learning practice train model score accuracy data set. far superior technique test model varying model parameter values unseen test set.common practice divide data set two parts training test. However, authors sucha Lantz (2015) suggest spit Training Set, Validation Set Test Set. latter case, proposal train model training set (60% data), perform model selection (tuning parameters) validation set (20% data) ready, test model test set (20% data).\nset. seed function helps get aleatory partition data.","code":""},{"path":"data-collection.html","id":"for-cros-sectional-data-or-no-time-series","chapter":"2 Data collection","heading":"2.2.1 For cros sectional data or no time series","text":"case time relevant, partition usually random 80% training set 20% test set.housing prices data set:credit data set","code":"\nset.seed (41)\ndim<-dim(house)\ntrain_sample<-sample(dim[1],dim[1]*.8)\nhouse_train <- house[train_sample, ]\nhouse_test  <- house[-train_sample, ]\nset.seed (43)\ndim<-dim(credit)\ntrain_sample<-sample(dim[1],dim[1]*.8)\ncredit_train <- credit[train_sample, ]\ncredit_test  <- credit[-train_sample, ]"},{"path":"data-collection.html","id":"for-time-series","chapter":"2 Data collection","heading":"2.2.2 For time series","text":"time series, usually interested predicting future, convenient split training-test last section. time series partition periods time. training period preceding test. example use Bit Coin data series, case also add independent variables. case, assume want predict variable Direction, categorical variable, takes value one current price bigger close price ten days ago, zero otherwise.\\[Direction (t)=\\alpha\\ +\\beta1 x1+\\beta2 x2 + ..+e \\]case, make training-test partition, suggest transform data frame xts object, allow make date subsets.Finally, apply subset function.","code":"## Loading required package: zoo## \n## Attaching package: 'zoo'## The following objects are masked from 'package:base':\n## \n##     as.Date, as.Date.numeric## \n## Attaching package: 'xts'## The following objects are masked from 'package:dplyr':\n## \n##     first, last\n#library(xts)\ndatedata<-bit[,\"Date\"]\ndatedata<-as.Date(datedata,format=\"%m/%d/%Y\")\ndatax<-xts(bit,order.by = as.Date(datedata),as.POSIXct(\"%d/%m/%Y\"))\ndatax<-datax[,-1]\nhead(datax)##            Direction SMA         SMA.1        Bit.3          macd           \n## 2015-11-20 \"0\"       \"  322.022\" \"  324.0855\" \"2.918218e+00\" \" -1.632425443\"\n## 2015-11-21 \"1\"       \"  326.927\" \"  324.4745\" \"3.468358e+00\" \" -0.900212731\"\n## 2015-11-22 \"0\"       \"  324.536\" \"  325.7315\" \"1.690686e+00\" \"  0.253303853\"\n## 2015-11-23 \"0\"       \"  323.046\" \"  323.7910\" \"1.053604e+00\" \" -0.105435504\"\n## 2015-11-24 \"0\"       \"  320.046\" \"  321.5460\" \"2.121320e+00\" \" -0.646634021\"\n## 2015-11-25 \"1\"       \"  328.206\" \"  324.1260\" \"5.769994e+00\" \"  0.051702842\"\n##            signal          rsi           \n## 2015-11-20 \" -1.008662580\" \"  0.00000000\"\n## 2015-11-21 \" -1.266319087\" \" 54.30699991\"\n## 2015-11-22 \" -0.323454439\" \" 67.22869686\"\n## 2015-11-23 \"  0.073934174\" \"  0.00000000\"\n## 2015-11-24 \" -0.376034762\" \"  0.00000000\"\n## 2015-11-25 \" -0.297465589\" \" 73.11828920\"\nbit_train<-subset(datax,\n  +index(datax)>=\"2019-11-01\" &\n  +index(datax)<=\"2021-10-01\") \n\nbit_test<-subset(datax,\n  +index(datax)>=\"2021-10-02\" &\n  +index(datax)<=\"2021-11-01\") "},{"path":"discover-and-visualize-the-data-to-gain-insights.html","id":"discover-and-visualize-the-data-to-gain-insights","chapter":"3 3 Discover and Visualize the Data to Gain Insights","heading":"3 3 Discover and Visualize the Data to Gain Insights","text":"","code":""},{"path":"discover-and-visualize-the-data-to-gain-insights.html","id":"looking-for-relationships-among-features-and-correlations","chapter":"3 3 Discover and Visualize the Data to Gain Insights","heading":"3.1 Looking for relationships among features and Correlations","text":"Exploring relationships among features – correlation matrixFor simplicity, start analysis removing NasVisualizing relationships among features – scatterplot matrixOr combination correlation relationships among features","code":"\ncolnames(house)##  [1] \"median_house_value\" \"longitude\"          \"latitude\"          \n##  [4] \"housing_median_age\" \"total_rooms\"        \"total_bedrooms\"    \n##  [7] \"population\"         \"households\"         \"median_income\"     \n## [10] \"ocean_proximity\"\nh_vars<-c(\"median_house_value\",\"total_rooms\",\"total_bedrooms\")\n\ncor(na.omit(house_train[,h_vars]))##                    median_house_value total_rooms total_bedrooms\n## median_house_value         1.00000000   0.1351186     0.05206914\n## total_rooms                0.13511863   1.0000000     0.93145142\n## total_bedrooms             0.05206914   0.9314514     1.00000000\npairs(na.omit(house_train[,h_vars]))\npanel.cor <- function(x, y, digits = 2, prefix = \"\", cex.cor, ...)\n{\n    usr <- par(\"usr\"); on.exit(par(usr))\n    par(usr = c(0, 1, 0, 1))\n    r <- abs(cor(x, y,use=\"pairwise.complete.obs\"))\n    txt <- format(c(r, 0.123456789), digits = digits)[1]\n    txt <- paste0(prefix, txt)\n    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)\n    text(0.5, 0.5, txt, cex = cex.cor * r)\n}\npairs(na.omit(house_train[,h_vars]),upper.panel=panel.cor,na.action = na.omit)"},{"path":"discover-and-visualize-the-data-to-gain-insights.html","id":"experimenting-with-attribute-combinations","chapter":"3 3 Discover and Visualize the Data to Gain Insights","heading":"3.2 Experimenting with Attribute Combinations","text":"One last thing may want preparing data Machine Learning algorithms try various attribute combinations. example, total number rooms district useful don’t know many households . want number rooms per household. Similarly, total number bedrooms useful: probably want compare number rooms. population per household also seems like interesting attribute combination look . Let’s create new attributesIf look correlations , see “total_bedrooms”,“rooms_per_household”,“bedrooms_per_room” correlated whit median house value total_rooms”,“total_bedrooms”.","code":"\nrooms_per_household<-house_train[,\"total_rooms\"]/house_train[,\"households\"]\nbedrooms_per_room<-house_train[,\"total_bedrooms\"]/house_train[,\"total_rooms\"]\npopulation_per_household<-house_train[,\"population\"]/house_train[,\"households\"]\n\nhouse_train<-cbind(house_train,rooms_per_household,bedrooms_per_room,population_per_household)\nh_vars<-c(\"median_house_value\",\"total_rooms\",\"total_bedrooms\",\"rooms_per_household\",\"bedrooms_per_room\")\n\npanel.cor <- function(x, y, digits = 2, prefix = \"\", cex.cor, ...)\n{\n    usr <- par(\"usr\"); on.exit(par(usr))\n    par(usr = c(0, 1, 0, 1))\n    r <- abs(cor(x, y,use=\"pairwise.complete.obs\"))\n    txt <- format(c(r, 0.123456789), digits = digits)[1]\n    txt <- paste0(prefix, txt)\n    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)\n    text(0.5, 0.5, txt, cex = cex.cor * r)\n}\npairs(house_train[,h_vars],upper.panel=panel.cor,na.action = na.omit)"},{"path":"prepare-the-data-for-machine-learning-algorithms.html","id":"prepare-the-data-for-machine-learning-algorithms","chapter":"4 4 Prepare the Data for Machine Learning Algorithms","heading":"4 4 Prepare the Data for Machine Learning Algorithms","text":"","code":""},{"path":"prepare-the-data-for-machine-learning-algorithms.html","id":"data-cleaning","chapter":"4 4 Prepare the Data for Machine Learning Algorithms","heading":"4.1 Data Cleaning","text":"Common data quality issues:might missing erroneous values data setThere might categorical (Textual, Boolean) values data set algorithms work well textual values.features might larger values others required transformed equal importance.","code":""},{"path":"prepare-the-data-for-machine-learning-algorithms.html","id":"missing-values","chapter":"4 4 Prepare the Data for Machine Learning Algorithms","heading":"4.2 Missing values","text":"Machine Learning algorithms work missing values, analyze best way deal white . saw earlier total_bedrooms attribute missing values, let’s fix . , least, three options:Get rid corresponding districts (rows).Get rid whole attribute (column).Set values value (zero, mean, median, etc.).","code":""},{"path":"prepare-the-data-for-machine-learning-algorithms.html","id":"handling-text-and-categorical-attributes","chapter":"4 4 Prepare the Data for Machine Learning Algorithms","heading":"4.3 Handling Text and Categorical Attributes","text":"might missing erroneous values data set\nmight categorical (Textual, Boolean) values data set algorithms work well textual values.\nfeatures might larger values others required transformed equal importance.run regression including ocean_proximity, notice regression estimates coefficient category variable ocean_proximity. applying Machine Learning ML forecasting pourpuses, convenient transform categorical . need one coefficient associated variable ocean_proximity. need transform variable numeric.According Jame et al. (2017), expected test MSE, given value x(0), can decomposed sum three fundamental quantities:\\[E (y_{0}-\\hat{f(x_{0}))^{2}}= Var(\\hat{f(x_{0}))}+[Bias\\ \\hat{f(x_{0}))}]^2+Var[\\epsilon]\\]\n\\(E (y_{0}-\\hat{f(x_{0}))^{2}}\\) expected test MSE. meaning expected average test MSE obtain repeatedly estimated test MSE f using large number training sets, tested x0. overall expected test MSE can computed averaging.Variance refers amount \\(\\hat{f}\\) change estimated using different training data set. Since training data used fit statistical learning method, different training data sets result different \\(\\hat{f}\\).hand, bias refers error introduced approximating real-life problem, may extremely complicated, much simpler model. example, linear regression assumes linear relationship Y X1,X2, . . . , Xp. unlikely real-life problem truly simple linear relationship, performing linear regression undoubtedly result bias estimate f.Finally, \\(\\epsilon\\) error term.number observations, n, much larger number independent variables, x, least squares estimates tend also low variance, \\(Var(\\hat{f(x_{0}))}\\), consequently reducing expected test MSE, \\(E (y_{0}-\\hat{f(x_{0}))^{2}}\\), improving accuracy. However, number x, relative number observations, can lot variability least squares fit, resulting overfitting consequently poor predictions future observations used model training. terms equation\\[Var(\\hat{f(x_{})}) = Var(\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n})\\], parameters estimated, biger variance , andWhen apply applying Ordinary Least Squares OLS looking causality, trying explain dependent variable,looked top five rows, probably noticed values ocean_proximity column repetitive, means probably categorical attribute. can find categories exist many districts belong category applying duplicated function:run regression including ocean_proximity, notice regression estimates coefficient category variable ocean_proximity. applying Machine Learning ML forecasting pourpuses, convenient transform categorical . need one coefficient associated variable ocean_proximity. need transform variable numeric.apply applying Ordinary Least Squares OLS looking causality, trying explain dependent variable,case, objective make forecast, convenient transform categorical values numeric. apply function asnum library dataclean.Now see ocean_proximity variable onpy one coefficient.","code":"\ncol=\"ocean_proximity\"\nhouse_train[,col][!duplicated(house_train[,col])]## [1] \"INLAND\"     \"<1H OCEAN\"  \"NEAR BAY\"   \"NEAR OCEAN\" \"ISLAND\"\ndep<-\"median_house_value\"\nmodel<-lm(median_house_value~.,data=house_train)\nsummary(model)## \n## Call:\n## lm(formula = median_house_value ~ ., data = house_train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -406981  -41703   -9752   28771  453689 \n## \n## Coefficients:\n##                             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)               -2.432e+06  9.861e+04 -24.662  < 2e-16 ***\n## longitude                 -2.759e+04  1.141e+03 -24.185  < 2e-16 ***\n## latitude                  -2.605e+04  1.128e+03 -23.096  < 2e-16 ***\n## housing_median_age         1.069e+03  4.835e+01  22.103  < 2e-16 ***\n## total_rooms                2.955e+00  1.057e+00   2.796  0.00518 ** \n## total_bedrooms             1.355e+01  8.978e+00   1.510  0.13118    \n## population                -4.696e+01  1.349e+00 -34.808  < 2e-16 ***\n## households                 1.174e+02  9.481e+00  12.383  < 2e-16 ***\n## median_income              4.200e+04  4.232e+02  99.249  < 2e-16 ***\n## ocean_proximityINLAND     -3.486e+04  1.940e+03 -17.972  < 2e-16 ***\n## ocean_proximityISLAND      2.265e+05  4.781e+04   4.739 2.17e-06 ***\n## ocean_proximityNEAR BAY   -4.954e+03  2.114e+03  -2.343  0.01913 *  \n## ocean_proximityNEAR OCEAN  4.077e+03  1.724e+03   2.365  0.01805 *  \n## rooms_per_household        2.595e+03  3.163e+02   8.205 2.47e-16 ***\n## bedrooms_per_room          2.970e+05  1.547e+04  19.197  < 2e-16 ***\n## population_per_household   5.979e+02  1.254e+02   4.766 1.89e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 67560 on 16320 degrees of freedom\n##   (176 observations deleted due to missingness)\n## Multiple R-squared:  0.6584, Adjusted R-squared:  0.6581 \n## F-statistic:  2097 on 15 and 16320 DF,  p-value: < 2.2e-16\nlibrary(datapro)\nhouse_train<-asnum(house_train)\ndep<-\"median_house_value\"\nmodel<-lm(median_house_value~.,data=house_train)\nsummary(model)## \n## Call:\n## lm(formula = median_house_value ~ ., data = house_train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -435692  -42148  -10821   29840  455544 \n## \n## Coefficients:\n##                            Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)              -3.584e+06  7.403e+04 -48.417  < 2e-16 ***\n## longitude                -4.146e+04  8.458e+02 -49.023  < 2e-16 ***\n## latitude                 -4.100e+04  7.901e+02 -51.898  < 2e-16 ***\n## housing_median_age        1.137e+03  4.772e+01  23.835  < 2e-16 ***\n## total_rooms               2.336e+00  1.068e+00   2.188   0.0287 *  \n## total_bedrooms            1.412e+01  9.073e+00   1.556   0.1198    \n## population               -4.767e+01  1.363e+00 -34.978  < 2e-16 ***\n## households                1.221e+02  9.579e+00  12.743  < 2e-16 ***\n## median_income             4.319e+04  4.215e+02 102.454  < 2e-16 ***\n## ocean_proximity          -5.589e+01  4.052e+02  -0.138   0.8903    \n## rooms_per_household       2.920e+03  3.192e+02   9.149  < 2e-16 ***\n## bedrooms_per_room         3.335e+05  1.551e+04  21.504  < 2e-16 ***\n## population_per_household  5.758e+02  1.268e+02   4.542 5.62e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 68290 on 16323 degrees of freedom\n##   (176 observations deleted due to missingness)\n## Multiple R-squared:  0.6509, Adjusted R-squared:  0.6506 \n## F-statistic:  2536 on 12 and 16323 DF,  p-value: < 2.2e-16"},{"path":"prepare-the-data-for-machine-learning-algorithms.html","id":"feature-scaling","chapter":"4 4 Prepare the Data for Machine Learning Algorithms","heading":"4.4 Feature Scaling","text":"One important transformations need apply data feature scaling. exceptions, Machine Learning algorithms don’t perform well input numerical attributes different scales. case housing data: total number rooms ranges 6 39,320, median incomes range 0 15. Note scaling target values (y, dependent variable) generally required.two common ways get attributes scale: min-max scaling standardization.Min-max scaling (many people call normalization) simplest: values shifted rescaled end ranging 0 1. subtracting min value dividing max minus min.Standardization. first subtracts mean value (standardized values always zero mean), divides standard deviation resulting distribution unit variance.","code":""},{"path":"select-and-train-and-evaluate-the-model.html","id":"select-and-train-and-evaluate-the-model","chapter":"5 5 Select and Train and evaluate the Model","heading":"5 5 Select and Train and evaluate the Model","text":"","code":""},{"path":"select-and-train-and-evaluate-the-model.html","id":"for-continuous-variables-1","chapter":"5 5 Select and Train and evaluate the Model","heading":"5.1 For continuous variables","text":"continuous variables, goal machine learning model linear regression OLS predict variable y. example, “median_house_value” house pricing data set.\\[y=\\beta_{0}+\\beta_{1}x_{1}+,..,+\\beta_{n}x_{n} +\\epsilon \\]\n\\(x_{1}\\), \\(x_{n}\\) independent variables \\(\\epsilon\\) error term.case, predicted value \\(\\hat{y}\\) :\\[\\hat{y}=\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n} \\]\\[\\hat{y}=\\hat{\\beta_{0}}+\\hat{\\beta_{1}}Z_{1}+,..,+\\hat{\\beta_{n}}Z_{m} \\]“median_house_value” house pricing data set, use function lm make Ordinary Least squares OLS estimation:","code":"\ndep<-\"median_house_value\"\nhouse_model<-lm(median_house_value~.,data=house_train)\nsummary(house_model)## \n## Call:\n## lm(formula = median_house_value ~ ., data = house_train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -435692  -42148  -10821   29840  455544 \n## \n## Coefficients:\n##                            Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)              -3.584e+06  7.403e+04 -48.417  < 2e-16 ***\n## longitude                -4.146e+04  8.458e+02 -49.023  < 2e-16 ***\n## latitude                 -4.100e+04  7.901e+02 -51.898  < 2e-16 ***\n## housing_median_age        1.137e+03  4.772e+01  23.835  < 2e-16 ***\n## total_rooms               2.336e+00  1.068e+00   2.188   0.0287 *  \n## total_bedrooms            1.412e+01  9.073e+00   1.556   0.1198    \n## population               -4.767e+01  1.363e+00 -34.978  < 2e-16 ***\n## households                1.221e+02  9.579e+00  12.743  < 2e-16 ***\n## median_income             4.319e+04  4.215e+02 102.454  < 2e-16 ***\n## ocean_proximity          -5.589e+01  4.052e+02  -0.138   0.8903    \n## rooms_per_household       2.920e+03  3.192e+02   9.149  < 2e-16 ***\n## bedrooms_per_room         3.335e+05  1.551e+04  21.504  < 2e-16 ***\n## population_per_household  5.758e+02  1.268e+02   4.542 5.62e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 68290 on 16323 degrees of freedom\n##   (176 observations deleted due to missingness)\n## Multiple R-squared:  0.6509, Adjusted R-squared:  0.6506 \n## F-statistic:  2536 on 12 and 16323 DF,  p-value: < 2.2e-16"},{"path":"select-and-train-and-evaluate-the-model.html","id":"linear-model-selection","chapter":"5 5 Select and Train and evaluate the Model","heading":"5.1.1 Linear Model Selection","text":"spirit mention previous section, regarding expected test MSE, given value x(0),\\[E (y_{0}-\\hat{f(x_{0}))^{2}}= Var(\\hat{f(x_{0}))}+[Bias\\ \\hat{f(x_{0}))}]^2+Var[\\epsilon]\\]words, look subset independent variables predictors believe related response. fit model using least squares reduced set variables, consequence reducing variance \\(\\hat{f}\\), test MSE, improves prediction accuracy.","code":"\nstepw<-step(house_model, direction = \"both\",trace = F)\nsummary(stepw)## \n## Call:\n## lm(formula = median_house_value ~ longitude + latitude + housing_median_age + \n##     total_rooms + total_bedrooms + population + households + \n##     median_income + rooms_per_household + bedrooms_per_room + \n##     population_per_household, data = house_train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -435760  -42172  -10800   29838  455571 \n## \n## Coefficients:\n##                            Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)              -3.581e+06  7.073e+04 -50.637  < 2e-16 ***\n## longitude                -4.143e+04  8.111e+02 -51.080  < 2e-16 ***\n## latitude                 -4.098e+04  7.731e+02 -53.005  < 2e-16 ***\n## housing_median_age        1.137e+03  4.769e+01  23.845  < 2e-16 ***\n## total_rooms               2.329e+00  1.066e+00   2.184    0.029 *  \n## total_bedrooms            1.409e+01  9.071e+00   1.554    0.120    \n## population               -4.765e+01  1.355e+00 -35.157  < 2e-16 ***\n## households                1.221e+02  9.578e+00  12.744  < 2e-16 ***\n## median_income             4.319e+04  4.196e+02 102.950  < 2e-16 ***\n## rooms_per_household       2.919e+03  3.191e+02   9.148  < 2e-16 ***\n## bedrooms_per_room         3.335e+05  1.550e+04  21.509  < 2e-16 ***\n## population_per_household  5.754e+02  1.267e+02   4.540 5.67e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 68290 on 16324 degrees of freedom\n##   (176 observations deleted due to missingness)\n## Multiple R-squared:  0.6509, Adjusted R-squared:  0.6506 \n## F-statistic:  2767 on 11 and 16324 DF,  p-value: < 2.2e-16"},{"path":"select-and-train-and-evaluate-the-model.html","id":"select-the-model-for-categorical-variables","chapter":"5 5 Select and Train and evaluate the Model","heading":"5.2 Select the model for categorical variables","text":"","code":""},{"path":"select-and-train-and-evaluate-the-model.html","id":"clasification-models","chapter":"5 5 Select and Train and evaluate the Model","heading":"5.2.1 Clasification models","text":"linear regression assumes response variable Y quantitative. many situations, response variable instead qualitative. example, eye color qualitative, taking values blue, brown, green. Often qualitative variables referred categorical ; use terms interchangeably. chapter, study approaches predicting qualitative responses, process known classification. Predicting qualitative response observation can referred classifying observation, since involves assigning observation category, class. hand, often methods used classification first predict probability categories qualitative variable, basis making classification.sense also behave like regression methods.discuss three widely-used classifiers: logistic regression, linear discriminant analysis, \nlogistic regression linear discriminant analysis\nK-nearest neighbors.Examples classification:\n1. person arrives emergency room set symptoms possibly attributed one three medical conditions. three conditions individual ?online banking service must able determine whether transaction performed site fraudulent, basis user’s IP address, past transaction history, forth.online banking service must able determine whether transaction performed site fraudulent, basis user’s IP address, past transaction history, forth.basis DNA sequence data number patients without given disease, biologist like figure DNA mutations deleterious (disease-causing) notOn basis DNA sequence data number patients without given disease, biologist like figure DNA mutations deleterious (disease-causing) ","code":""},{"path":"select-and-train-and-evaluate-the-model.html","id":"logistic-regression","chapter":"5 5 Select and Train and evaluate the Model","heading":"5.2.1.1 Logistic Regression","text":"binary response model, interest lies primarily response probability:\\[P(y=1/X)=P(y=1|x_{1},x_{2},..,x_{n}) \\]\nuse X denote full set explanatory variables. example, y default variable house pricing example direction bit coin example. hand, x set independent variables. previous equation reads: probability y=1, given X equal probability y=1, given \\(x_{1},x_{2},..,x_{n}\\).simplify, usually use binary response c(0,1). One result interest, default example, zero case, default. ensures result predictions binary response c(0,1), also. First estimate response probabilities strictly zero one. Specifying model:\\[P(y=1/X)=G(\\beta_{0}+\\beta_{1}x_{1}+,..,+\\beta_{n}x_{n}) \\]\nG function taking values strictly zero one: 0 < G < 1.Various nonlinear functions suggested function G make sure probabilities zero one. common logit model, G logistic function:\\[G(z)=\\frac{exp(z)}{1-(exp(z)}\\]\nzero one real numbers z. cumulative distribution function standard logistic random variable.logit model z predicted model.\\[ z=\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n}\\]\\[G(z)=\\frac{exp(z)}{1-(exp(z)}\\]\n,\\[P(y=1/X)=\\frac{exp(\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n})}{1-(exp(\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n})}\\]equation use making prediction. However, can use OLS estimate model, linear binary response model. apply maximum likelihood estimation (MLE).credit data set use function glm estimate Logistic regression., trasnform categorical values numerival eliminate Nas.apply stepwise methodology estimate model using argument “” trace = F:Take stepwise result predict final model time using test dataset, print head taiThe argument type = “response” transform automatically results probability. words, applies formula exp(x)/(1+exp(x)). use argument, correct transform probability.\npredicted_test <- predict(model, newdata=)Transform probabilistic model, applying formula exp(x)/(1+exp(x)). Print head tail result:Now measure Accuracy applying confusion Matrix:","code":"\ncredit_train<-asnum(credit_train)\ncredit_train<-na.omit(credit_train)\n\ncredit_test<-asnum(credit_test)\ncredit_test<-na.omit(credit_test)## \n## Call:\n## glm(formula = Default ~ ., family = binomial(), data = credit_train)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.6715  -0.5833  -0.4045  -0.2269   2.7163  \n## \n## Coefficients: (1 not defined because of singularities)\n##                        Estimate Std. Error z value Pr(>|z|)   \n## (Intercept)          -4.146e+00  2.695e+00  -1.538  0.12405   \n## loan_amnt            -1.659e-04  1.074e-04  -1.546  0.12222   \n## term                  2.136e+00  7.051e-01   3.030  0.00245 **\n## int_rate             -6.764e-01  4.270e-01  -1.584  0.11317   \n## installment           5.081e-03  3.390e-03   1.499  0.13397   \n## grade                -2.724e-01  4.041e-01  -0.674  0.50019   \n## sub_grade             5.775e-01  3.035e-01   1.903  0.05708 . \n## emp_title             5.335e-04  7.294e-04   0.731  0.46456   \n## emp_length            6.027e-02  4.280e-02   1.408  0.15909   \n## home_ownership        3.385e-01  1.383e-01   2.448  0.01437 * \n## annual_inc           -2.877e-06  4.169e-06  -0.690  0.49008   \n## verification_status  -2.748e-01  1.594e-01  -1.724  0.08479 . \n## issue_d                      NA         NA      NA       NA   \n## purpose              -6.848e-03  8.858e-02  -0.077  0.93838   \n## title                 8.493e-02  8.682e-02   0.978  0.32796   \n## dti                   1.688e-02  1.542e-02   1.094  0.27381   \n## earliest_cr_line     -1.137e-03  1.318e-03  -0.863  0.38825   \n## open_acc              5.082e-02  2.991e-02   1.699  0.08928 . \n## pub_rec               4.611e-01  2.138e-01   2.157  0.03101 * \n## revol_bal            -1.050e-05  1.120e-05  -0.937  0.34851   \n## revol_util            4.008e-03  5.821e-03   0.689  0.49111   \n## total_acc            -3.877e-03  1.334e-02  -0.291  0.77136   \n## initial_list_status   8.969e-01  6.913e-01   1.297  0.19451   \n## application_type     -7.003e-01  1.230e+00  -0.569  0.56921   \n## mort_acc             -1.104e-01  7.737e-02  -1.427  0.15358   \n## pub_rec_bankruptcies -4.767e-01  3.707e-01  -1.286  0.19838   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 631.12  on 697  degrees of freedom\n## Residual deviance: 521.51  on 673  degrees of freedom\n## AIC: 571.51\n## \n## Number of Fisher Scoring iterations: 5## \n## Call:\n## glm(formula = Default ~ term + sub_grade + home_ownership + verification_status + \n##     open_acc + pub_rec + mort_acc + annual_inc, family = binomial(), \n##     data = credit_train)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.1902  -0.6038  -0.4188  -0.2584   2.6427  \n## \n## Coefficients:\n##                       Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)         -4.085e+00  5.892e-01  -6.933 4.11e-12 ***\n## term                 1.023e+00  2.731e-01   3.745  0.00018 ***\n## sub_grade            8.677e-02  2.148e-02   4.040 5.35e-05 ***\n## home_ownership       3.066e-01  1.323e-01   2.318  0.02048 *  \n## verification_status -2.955e-01  1.521e-01  -1.943  0.05207 .  \n## open_acc             3.959e-02  1.937e-02   2.043  0.04101 *  \n## pub_rec              3.551e-01  1.704e-01   2.084  0.03711 *  \n## mort_acc            -1.327e-01  7.199e-02  -1.843  0.06531 .  \n## annual_inc          -4.849e-06  3.321e-06  -1.460  0.14427    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 631.12  on 697  degrees of freedom\n## Residual deviance: 536.64  on 689  degrees of freedom\n## AIC: 554.64\n## \n## Number of Fisher Scoring iterations: 5\n#credit_test<-asnum(credit_test)##         15         19         24         26         32         34 \n## -1.4750804 -2.2810764 -0.9013844 -3.4618845 -0.5504505 -3.1938712##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n## 0.01140 0.05696 0.10391 0.15324 0.19847 0.67279## Loading required package: ggplot2## Loading required package: lattice## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction   1   0\n##          1   6   1\n##          0  22 146\n##                                           \n##                Accuracy : 0.8686          \n##                  95% CI : (0.8093, 0.9148)\n##     No Information Rate : 0.84            \n##     P-Value [Acc > NIR] : 0.1775          \n##                                           \n##                   Kappa : 0.2979          \n##                                           \n##  Mcnemar's Test P-Value : 3.042e-05       \n##                                           \n##             Sensitivity : 0.21429         \n##             Specificity : 0.99320         \n##          Pos Pred Value : 0.85714         \n##          Neg Pred Value : 0.86905         \n##              Prevalence : 0.16000         \n##          Detection Rate : 0.03429         \n##    Detection Prevalence : 0.04000         \n##       Balanced Accuracy : 0.60374         \n##                                           \n##        'Positive' Class : 1               \n## "},{"path":"select-and-train-and-evaluate-the-model.html","id":"linear-discriminant-analysis-lda","chapter":"5 5 Select and Train and evaluate the Model","heading":"5.2.1.2 Linear Discriminant Analysis LDA","text":"need another method, logistic regression?\nseveral reasons:• classes well-separated, parameter estimates \nlogistic regression model surprisingly unstable. Linear discriminant\nanalysis suffer problem.• number observations n small distribution predictors X approximately normal classes, linear discriminant model stable logistic regression model.apply LDA model credit data set. However, takeout variable issue_d, otherwise model estimated.","code":"\ncredit_train<-credit_train[,-12]\ncredit_test<-credit_test[,-12]\nlibrary (MASS)## \n## Attaching package: 'MASS'## The following object is masked from 'package:dplyr':\n## \n##     select\nlda.fit<-lda(Default~.,data=credit_train)\nlda.fit## Call:\n## lda(Default ~ ., data = credit_train)\n## \n## Prior probabilities of groups:\n##         0         1 \n## 0.8323782 0.1676218 \n## \n## Group means:\n##   loan_amnt     term int_rate installment    grade sub_grade emp_title\n## 0  14992.13 1.173838 11.30895    453.9710 2.401033  9.772806  262.6816\n## 1  15283.55 1.461538 14.55598    437.5981 3.316239 14.504274  272.5556\n##   emp_length home_ownership annual_inc verification_status  purpose    title\n## 0   5.117040       1.752151   81069.05            1.764200 3.258176 5.067126\n## 1   5.512821       2.094017   70985.66            1.786325 3.418803 5.188034\n##        dti earliest_cr_line open_acc   pub_rec revol_bal revol_util total_acc\n## 0 18.90582         154.9449 12.38554 0.2271945  18348.62   49.06936  26.30465\n## 1 21.41171         151.0171 13.74359 0.3418803  15609.53   50.56667  27.13675\n##   initial_list_status application_type mort_acc pub_rec_bankruptcies\n## 0            1.955250         1.008606 1.975904            0.1376936\n## 1            1.974359         1.008547 1.316239            0.1794872\n## \n## Coefficients of linear discriminants:\n##                                LD1\n## loan_amnt            -2.627499e-04\n## term                  2.977285e+00\n## int_rate             -4.457492e-01\n## installment           7.977948e-03\n## grade                -1.942167e-01\n## sub_grade             3.744479e-01\n## emp_title             3.137385e-04\n## emp_length            5.217647e-02\n## home_ownership        3.159080e-01\n## annual_inc           -1.086106e-06\n## verification_status  -2.259384e-01\n## purpose              -1.458813e-02\n## title                 7.267585e-02\n## dti                   1.467009e-02\n## earliest_cr_line     -1.054073e-03\n## open_acc              5.127933e-02\n## pub_rec               4.644621e-01\n## revol_bal            -4.349356e-06\n## revol_util            1.522495e-03\n## total_acc            -6.584011e-03\n## initial_list_status   6.992175e-01\n## application_type     -6.901451e-01\n## mort_acc             -6.956526e-02\n## pub_rec_bankruptcies -4.437124e-01\nlda.pred<-predict(lda.fit, credit_test)\n#names(lda.pred)\nlda.pred$class##   [1] 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n##  [38] 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n##  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n## Levels: 0 1\nDefaultf_lda<-factor(lda.pred$class,levels=c(1,0))\ncredit_testf<-factor(credit_test$Default,levels=c(1,0))\n\nconfusionMatrix(Defaultf_lda,credit_testf)## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction   1   0\n##          1   5   4\n##          0  23 143\n##                                           \n##                Accuracy : 0.8457          \n##                  95% CI : (0.7835, 0.8958)\n##     No Information Rate : 0.84            \n##     P-Value [Acc > NIR] : 0.468215        \n##                                           \n##                   Kappa : 0.2087          \n##                                           \n##  Mcnemar's Test P-Value : 0.000532        \n##                                           \n##             Sensitivity : 0.17857         \n##             Specificity : 0.97279         \n##          Pos Pred Value : 0.55556         \n##          Neg Pred Value : 0.86145         \n##              Prevalence : 0.16000         \n##          Detection Rate : 0.02857         \n##    Detection Prevalence : 0.05143         \n##       Balanced Accuracy : 0.57568         \n##                                           \n##        'Positive' Class : 1               \n## "},{"path":"select-and-train-and-evaluate-the-model.html","id":"training-and-evaluating-on-the-training-set","chapter":"5 5 Select and Train and evaluate the Model","heading":"5.2.2 Training and Evaluating on the Training Set","text":"Estimating RMSE","code":"\n#dep=\"median_house_value\"\n#rmse<-cbind(test[,dep],predicted_test)\n#sqrt(mean((rmse[,2]-rmse[,1])^2,na.rm = T ))"},{"path":"select-and-train-and-evaluate-the-model.html","id":"resampling-cross-validation","chapter":"5 5 Select and Train and evaluate the Model","heading":"5.3 Resampling (cross-validation)","text":"Resampling methods indispensable tool modern statistics. involve repeatedly drawing samples training set refitting model interest sample order obtain additional information fitted model. example, order estimate variability linear\nregression fit, can repeatedly draw different samples training data, fit linear regression new sample, examine extent resulting fits differ. approach may allow us obtain information available fitting model using original training sample.One common resampling method K Fold Cross Validation. Cross-validation can used estimate test\nerror associated given statistical learning method order evaluate performance, select appropriate level flexibility. process evaluating model’s performance known model assessment, whereas model\nprocess selecting proper level flexibility model known assessment model selection.K Fold Cross ValidationThis approach involves randomly k-fold CV dividing set observations k groups, folds, approximately equal size. first fold treated validation set, method fit remaining k − 1 folds. mean squared error, MSE1, computed observations held-fold. procedure repeated k times; time, different group observations treated\nvalidation set.\\[CV_{k} =\\frac{1}{k}\\ \\sum_{=1}^{k} MSE_{} \\]\npractice, one typically performs k-fold CV using k = 5 k = 10.","code":""},{"path":"fine-tune-or-tune-the-ml-model.html","id":"fine-tune-or-tune-the-ml-model","chapter":"6 6 Fine-Tune or Tune the ML Model","heading":"6 6 Fine-Tune or Tune the ML Model","text":"Tuning machine learning model iterative process. Data scientists typically run numerous experiments train evaluate models, trying different features, different loss functions, different AI/ML models, adjusting model parameters hyperparameters. Examples steps involved tuning training machine learning model include feature engineering, loss function formulation, model testing selection, regularization, selection hyperparameters Krishnan (2022).Let’s assume now shortlist promising models. now need fine-tune . Let’s look ways can .retrieved optimum values individual model parameters can use grid search obtain combination hyperparameter (parameters also known hyperparameters) values model can give us highest accuracy.Grid Search evaluates possible combinations parameter values.Grid Search exhaustive uses brute-force evaluate accurate values. Therefore computationally intensive task.","code":""},{"path":"fine-tune-or-tune-the-ml-model.html","id":"analyze-the-best-models-and-their-errors","chapter":"6 6 Fine-Tune or Tune the ML Model","heading":"6.1 Analyze the Best Models and Their Errors","text":"","code":""},{"path":"fine-tune-or-tune-the-ml-model.html","id":"evaluate-your-system-on-the-test-set","chapter":"6 6 Fine-Tune or Tune the ML Model","heading":"6.2 Evaluate Your System on the Test Set","text":"","code":""},{"path":"references.html","id":"references","chapter":"7 References","heading":"7 References","text":"Géron, . (2019) Hands-Machine Learning Scikit-Learn, Keras, TensorFlow, 2nd Edition. O’Reilly, Sebastopol, CA.James, G, Witten, D, Hastie, T Tibshirani, R (2017). Introduction Statistical Learning Applications R. Springer, NY.Krishnan, N. (2022). Enterprise AI Machine Learning Managers. Retrieved c3.ai: https://c3.ai/publications/enterprise-ai--machine-learning--managers/Lantz, B (2015). Machine Learning R Second Edition. Packt Publishing, Birmingham, UK.Wooldridge (2013). Introductory econometrics. Cengage Learning. Mason, OH","code":""}]
