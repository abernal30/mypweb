[["index.html", "Machine learning introductory guide Machine learning introductory guide", " Machine learning introductory guide L. Arturo Bernal 2022-11-05 Machine learning introductory guide This is the for the book Machine learning introductory guide! This is a work by Aturo Bernal Visit the GitHub repository for this site. "],["preface.html", "Preface Outline", " Preface This text and examples have the spirit of generating a basic guide for the Machine Learning (ML) methodology. I am writing the book inspired by the students of the AI concentration of Tecnologico de Monterrey in the spring of 2022. For the moment, this document is based mainly on (Géron2019?), and some text from James et al. (2017) and Lantz (2015). However, I expect to fill it whit other materials and experience. Students are welcome to contribute, in which case, they will get credit for this text. This document follows a practical example: i) for the continuous variables of the California Housing Prices data set presented in Géron (2019); ii) for the categorical variables, we will follow the Lending Club fintech data set from Kaggle to analyze a credit default. Which is a financial institution that grants personal loans; iii) and a categorical variable for a Bit-coin prediction. The following steps may differ from other books, data scientists, or experts because there are many ways to deal whit machine learning. But that is precisely the richness of this area of expertise. If there existed only one way to apply machine learning, everybody could use it!! The R and other files are stored in the GitHub repository for this site. Outline "],["big-picture.html", "Chapter 1 Big picture 1.1 Frame the Problem 1.2 Select a Performance Measure", " Chapter 1 Big picture 1.1 Frame the Problem Answer the following: what is the business objective? or How does the company (client) expect to use and benefit from this model? Knowing the objective is important because it will determine how you frame the problem, which algorithms you will select. For the housing prices, the objective is to predict a district’s median housing price to detect investment opportunities, applying OLS ordinary least squares OLS. For example, your prediction may show that a house located in latitude x, longitude y, housing median age z, total rooms n, etc. has a median house value of 162,000. However, the house is for sale for $145,000, it may be an opportunity to buy something cheaper than it should cost. For the Lending club credit default analysis, we will focus on predicting if a new customer would default the loan or not (will repay the loan or not). what the current solution looks like (if any)? Is important to have a comparison, to visualize the solution. For this case, The current situation will often give you a reference for performance, as well as insights on how to solve the problem. For the case of the district housing prices, they currently estimated house prices manually by experts: a team gathers up-to-date information about a district, and when they cannot get the median housing price, they estimate it using complex rules. Regarding the Lending club, we do not have evidence that Lending club applies machine learning methods to analyze the credit applications. But we will assume that as many other credit financial institution, the apply the method manually. 1.2 Select a Performance Measure 1.2.1 For continuous variables Root Mean Square Error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight for large errors. Shows the mathematical formula to compute the RMSE. The mathematical formula to compute the RMSE is: \\[RMSE =\\frac{1}{n}\\ \\sum_{i=1}^{n} (y_{i}-\\hat{f(x_{i}))^{2}} \\] where $ $ is the prediction for the ith observation (the actual), $ y_{i} $ is the observation ith of the independent variable, and n is the number of observations. \\[\\hat{f(x_{i})}=\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n}\\] The RMSE is computed using the training data that was used to fit the model, and so should more accurately be referred to as the training RMSE. But in general, we do not really care how well the method works training on the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data. Mean absolute error (MAE). Even though the RMSE is generally the preferred performance measure for regression tasks, in some contexts you may prefer to use another function. For example, suppose that there are many outlier districts. In that case, you may consider using the mean absolute error (MAE, also called the average absolute deviation;Mean absolute error (MAE). \\[MAE =\\frac{1}{n}\\ \\sum_{i=1}^{n} |y_{i}-\\hat{f(x_{i})|}\\] 1.2.2 For categorical variables (classification methods) Confusion matrix Is a table that categorizes predictions according to whether they match the actual value. One of the table’s dimensions indicates the possible categories of predicted values, while the other dimension indicates the same for actual values. Although we have only seen 2 x 2 confusion matrices so far, a matrix can be created for models that predict any number of class values. The following figure shows a generic confusion matrix. Where True Positive (TP): Correctly classified as the class of interest. True Negative (TN) is Correctly classified as not the class of interest. False Positive (FP) is Incorrectly classified as the class of interest. False Negative (FN): Incorrectly classified as not the class of interest. In the confusion matrix, one of the mesures of interest is the accuracy, defined as: \\[ accuracy =\\frac{TP+TN}{TP+TN+FP+FN}\\] In this formula, the terms TP, TN, FP, and FN refer to the number of times the model’s predictions fell into each of these categories. The accuracy is therefore a proportion that represents the number of true positives and true negatives, divided by the total number of predictions. The error rate or the proportion of the incorrectly classified examples is specified as: \\[ error\\ rate =\\frac{FP+FN}{TP+TN+FP+FN}=1-acuracy\\] Notice that the error rate can be calculated as one minus the accuracy. Intuitively, this makes sense; a model that is correct 95 percent of the time is incorrect 5 percent of the time. Sensitivity and specificity Finding a useful classifier often involves a balance between predictions that are overly conservative and overly aggressive. For example, an e-mail filter could guarantee to eliminate every spam message by aggressively eliminating nearly every ham message at the same time. On the other hand, guaranteeing that no ham message is inadvertently filtered might require us to allow an unacceptable amount of spam to pass through the filter. A pair of performance measures captures this trade off: sensitivity and specificity. The sensitivity of a model (also called the true positive rate) measures the proportion of positive examples that were correctly classified. Therefore, as shown in the following formula, it is calculated as the number of true positives divided by the total number of positives, both correctly classified (the true positives) as well as incorrectly classified (the false negatives): \\[sensitivity =\\frac{TP}{TP+FN}\\] Where True Positive (TP): Correctly classified as the class of interest and False Negative (FN): Incorrectly classified as not the class of interest The specificity of a model (also called the true negative rate) measures the proportion of negative examples that were correctly classified. As with sensitivity, this is computed as the number of true negatives, divided by the total number of negatives—the true negatives plus the false positives: \\[ specificity =\\frac{TN}{TN+FP}\\] True Negative (TN) is Correctly classified as not the class of interest and False Positive (FP) is Incorrectly classified as the class of interest "],["data-collection.html", "Chapter 2 Data collection 2.1 Take a Quick Look at the Data Structure 2.2 Create the training and Test Set", " Chapter 2 Data collection The data sets we use for this text are stored in Github. The house pricing data set is retrived from: library(openxlsx) house&lt;-read.csv(&quot;https://raw.githubusercontent.com/abernal30/ml_book/main/housing.csv&quot;) The credit data set is retrieve from: library(openxlsx) credit&lt;-read.csv(&quot;https://raw.githubusercontent.com/abernal30/ml_book/main/credit.csv&quot;) In the credit data We are interested only in the Fully Paid and Charged Off features, which are equivalent to no-default and default respectively. Charge off” means that the credit grantor wrote your account off of their receivables as a loss, and it is closed to future charges. When an account displays a status of “charge off,” it means the account is closed to future use, although the debt is still owed. Then we create a variable Default, which was the variable loan_status, if Fully Paid then 0, and Charged Off, 1. Use the ifelse function to create that variable, call it Default. Also, eliminate the loan_status variable (otherwise it would be duplicated). ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union And for the Bit-coin: bit&lt;-read.csv(&quot;https://raw.githubusercontent.com/abernal30/ml_book/main/bit.csv&quot;) 2.1 Take a Quick Look at the Data Structure To review the data class str(house) ## &#39;data.frame&#39;: 20640 obs. of 10 variables: ## $ median_house_value: int 452600 358500 352100 341300 342200 269700 299200 241400 226700 261100 ... ## $ longitude : num -122 -122 -122 -122 -122 ... ## $ latitude : num 37.9 37.9 37.9 37.9 37.9 ... ## $ housing_median_age: int 41 21 52 52 52 52 52 52 42 52 ... ## $ total_rooms : int 880 7099 1467 1274 1627 919 2535 3104 2555 3549 ... ## $ total_bedrooms : int 129 1106 190 235 280 213 489 687 665 707 ... ## $ population : int 322 2401 496 558 565 413 1094 1157 1206 1551 ... ## $ households : int 126 1138 177 219 259 193 514 647 595 714 ... ## $ median_income : num 8.33 8.3 7.26 5.64 3.85 ... ## $ ocean_proximity : chr &quot;NEAR BAY&quot; &quot;NEAR BAY&quot; &quot;NEAR BAY&quot; &quot;NEAR BAY&quot; ... To review outliers or Nas. summary(house) ## median_house_value longitude latitude housing_median_age ## Min. : 14999 Min. :-124.3 Min. :32.54 Min. : 1.00 ## 1st Qu.:119600 1st Qu.:-121.8 1st Qu.:33.93 1st Qu.:18.00 ## Median :179700 Median :-118.5 Median :34.26 Median :29.00 ## Mean :206856 Mean :-119.6 Mean :35.63 Mean :28.64 ## 3rd Qu.:264725 3rd Qu.:-118.0 3rd Qu.:37.71 3rd Qu.:37.00 ## Max. :500001 Max. :-114.3 Max. :41.95 Max. :52.00 ## ## total_rooms total_bedrooms population households ## Min. : 2 Min. : 1.0 Min. : 3 Min. : 1.0 ## 1st Qu.: 1448 1st Qu.: 296.0 1st Qu.: 787 1st Qu.: 280.0 ## Median : 2127 Median : 435.0 Median : 1166 Median : 409.0 ## Mean : 2636 Mean : 537.9 Mean : 1425 Mean : 499.5 ## 3rd Qu.: 3148 3rd Qu.: 647.0 3rd Qu.: 1725 3rd Qu.: 605.0 ## Max. :39320 Max. :6445.0 Max. :35682 Max. :6082.0 ## NA&#39;s :207 ## median_income ocean_proximity ## Min. : 0.4999 Length:20640 ## 1st Qu.: 2.5634 Class :character ## Median : 3.5348 Mode :character ## Mean : 3.8707 ## 3rd Qu.: 4.7432 ## Max. :15.0001 ## 2.2 Create the training and Test Set It is not a wise machine learning practice to train your model and score its accuracy on the same data set. It is a far superior technique to test your model with varying model parameter values on an unseen test set. Is a common practice to divide your data set into two parts training and test. However, other authors sucha as Lantz (2015) suggest to spit into Training Set, Validation Set and a Test Set. in the latter case, the proposal is that to train the model on the training set (60% of the data), then perform model selection (tuning parameters) on validation set (20% of the data) and once you are ready, test your model on the test set (20% of the data). The set. seed function helps to get the same aleatory partition in the data. 2.2.1 For cros sectional data or no time series For the case where time is not relevant, the partition is usually random 80% the training set and 20% the test set. For the housing prices data set: set.seed (41) dim&lt;-dim(house) train_sample&lt;-sample(dim[1],dim[1]*.8) house_train &lt;- house[train_sample, ] house_test &lt;- house[-train_sample, ] For the credit data set set.seed (43) dim&lt;-dim(credit) train_sample&lt;-sample(dim[1],dim[1]*.8) credit_train &lt;- credit[train_sample, ] credit_test &lt;- credit[-train_sample, ] 2.2.2 For time series For time series, usually we are interested in predicting the future, then is not convenient to split the training-test as we did in the last section. For time series the partition is in periods of time. The training is a period preceding the test. For this example we use the Bit Coin data series, in which case we also add some independent variables. For this case, assume that we want to predict a variable Direction, which is a categorical variable, which takes the value of one when the current price is bigger than the close price ten days ago, and zero otherwise. \\[Direction (t)=\\alpha\\ +\\beta1 x1+\\beta2 x2 + ..+e \\] In this case, before make the training-test partition, I suggest to transform the data frame into a xts object, which allow to make date subsets. ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric ## ## Attaching package: &#39;xts&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## first, last #library(xts) datedata&lt;-bit[,&quot;Date&quot;] datedata&lt;-as.Date(datedata,format=&quot;%m/%d/%Y&quot;) datax&lt;-xts(bit,order.by = as.Date(datedata),as.POSIXct(&quot;%d/%m/%Y&quot;)) datax&lt;-datax[,-1] head(datax) ## Direction SMA SMA.1 Bit.3 macd ## 2015-11-20 &quot;0&quot; &quot; 322.022&quot; &quot; 324.0855&quot; &quot;2.918218e+00&quot; &quot; -1.632425443&quot; ## 2015-11-21 &quot;1&quot; &quot; 326.927&quot; &quot; 324.4745&quot; &quot;3.468358e+00&quot; &quot; -0.900212731&quot; ## 2015-11-22 &quot;0&quot; &quot; 324.536&quot; &quot; 325.7315&quot; &quot;1.690686e+00&quot; &quot; 0.253303853&quot; ## 2015-11-23 &quot;0&quot; &quot; 323.046&quot; &quot; 323.7910&quot; &quot;1.053604e+00&quot; &quot; -0.105435504&quot; ## 2015-11-24 &quot;0&quot; &quot; 320.046&quot; &quot; 321.5460&quot; &quot;2.121320e+00&quot; &quot; -0.646634021&quot; ## 2015-11-25 &quot;1&quot; &quot; 328.206&quot; &quot; 324.1260&quot; &quot;5.769994e+00&quot; &quot; 0.051702842&quot; ## signal rsi ## 2015-11-20 &quot; -1.008662580&quot; &quot; 0.00000000&quot; ## 2015-11-21 &quot; -1.266319087&quot; &quot; 54.30699991&quot; ## 2015-11-22 &quot; -0.323454439&quot; &quot; 67.22869686&quot; ## 2015-11-23 &quot; 0.073934174&quot; &quot; 0.00000000&quot; ## 2015-11-24 &quot; -0.376034762&quot; &quot; 0.00000000&quot; ## 2015-11-25 &quot; -0.297465589&quot; &quot; 73.11828920&quot; Finally, we apply the subset function. bit_train&lt;-subset(datax, +index(datax)&gt;=&quot;2019-11-01&quot; &amp; +index(datax)&lt;=&quot;2021-10-01&quot;) bit_test&lt;-subset(datax, +index(datax)&gt;=&quot;2021-10-02&quot; &amp; +index(datax)&lt;=&quot;2021-11-01&quot;) "],["discover-and-visualize-the-data-to-gain-insights.html", "Chapter 3 3 Discover and Visualize the Data to Gain Insights 3.1 Looking for relationships among features and Correlations 3.2 Experimenting with Attribute Combinations", " Chapter 3 3 Discover and Visualize the Data to Gain Insights 3.1 Looking for relationships among features and Correlations Exploring relationships among features – the correlation matrix For simplicity, we start the analysis removing the Nas colnames(house) ## [1] &quot;median_house_value&quot; &quot;longitude&quot; &quot;latitude&quot; ## [4] &quot;housing_median_age&quot; &quot;total_rooms&quot; &quot;total_bedrooms&quot; ## [7] &quot;population&quot; &quot;households&quot; &quot;median_income&quot; ## [10] &quot;ocean_proximity&quot; h_vars&lt;-c(&quot;median_house_value&quot;,&quot;total_rooms&quot;,&quot;total_bedrooms&quot;) cor(na.omit(house_train[,h_vars])) ## median_house_value total_rooms total_bedrooms ## median_house_value 1.00000000 0.1351186 0.05206914 ## total_rooms 0.13511863 1.0000000 0.93145142 ## total_bedrooms 0.05206914 0.9314514 1.00000000 Visualizing relationships among features – the scatterplot matrix pairs(na.omit(house_train[,h_vars])) Or a combination of correlation and relationships among features panel.cor &lt;- function(x, y, digits = 2, prefix = &quot;&quot;, cex.cor, ...) { usr &lt;- par(&quot;usr&quot;); on.exit(par(usr)) par(usr = c(0, 1, 0, 1)) r &lt;- abs(cor(x, y,use=&quot;pairwise.complete.obs&quot;)) txt &lt;- format(c(r, 0.123456789), digits = digits)[1] txt &lt;- paste0(prefix, txt) if(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt) text(0.5, 0.5, txt, cex = cex.cor * r) } pairs(na.omit(house_train[,h_vars]),upper.panel=panel.cor,na.action = na.omit) 3.2 Experimenting with Attribute Combinations One last thing you may want to do before preparing the data for Machine Learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. Let’s create these new attributes rooms_per_household&lt;-house_train[,&quot;total_rooms&quot;]/house_train[,&quot;households&quot;] bedrooms_per_room&lt;-house_train[,&quot;total_bedrooms&quot;]/house_train[,&quot;total_rooms&quot;] population_per_household&lt;-house_train[,&quot;population&quot;]/house_train[,&quot;households&quot;] house_train&lt;-cbind(house_train,rooms_per_household,bedrooms_per_room,population_per_household) If we look at the correlations again, we see that “total_bedrooms”,“rooms_per_household”,“bedrooms_per_room” are more correlated whit median house value than total_rooms”,“total_bedrooms”. h_vars&lt;-c(&quot;median_house_value&quot;,&quot;total_rooms&quot;,&quot;total_bedrooms&quot;,&quot;rooms_per_household&quot;,&quot;bedrooms_per_room&quot;) panel.cor &lt;- function(x, y, digits = 2, prefix = &quot;&quot;, cex.cor, ...) { usr &lt;- par(&quot;usr&quot;); on.exit(par(usr)) par(usr = c(0, 1, 0, 1)) r &lt;- abs(cor(x, y,use=&quot;pairwise.complete.obs&quot;)) txt &lt;- format(c(r, 0.123456789), digits = digits)[1] txt &lt;- paste0(prefix, txt) if(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt) text(0.5, 0.5, txt, cex = cex.cor * r) } pairs(house_train[,h_vars],upper.panel=panel.cor,na.action = na.omit) "],["prepare-the-data-for-machine-learning-algorithms.html", "Chapter 4 4 Prepare the Data for Machine Learning Algorithms 4.1 Data Cleaning 4.2 Missing values 4.3 Handling Text and Categorical Attributes 4.4 Feature Scaling", " Chapter 4 4 Prepare the Data for Machine Learning Algorithms 4.1 Data Cleaning Common data quality issues: There might be missing or erroneous values in the data set There might be categorical (Textual, Boolean) values in the data set and not all algorithms work well with textual values. Some features might have larger values than others and are required to be transformed for equal importance. 4.2 Missing values Most Machine Learning algorithms cannot work with missing values, so analyze the best way to deal white them. We saw earlier that the total_bedrooms attribute has some missing values, so let’s fix this. You have, at least, three options: Get rid of the corresponding districts (rows). Get rid of the whole attribute (column). Set the values to some value (zero, the mean, the median, etc.). 4.3 Handling Text and Categorical Attributes There might be missing or erroneous values in the data set There might be categorical (Textual, Boolean) values in the data set and not all algorithms work well with textual values. Some features might have larger values than others and are required to be transformed for equal importance. If you run a regression including ocean_proximity, you will notice that the regression estimates a coefficient by each category of the variable ocean_proximity. When applying Machine Learning ML for forecasting pourpuses, is more convenient to transform the categorical . We need only one coefficient associated with the variable ocean_proximity. Then we need to transform the variable into numeric. According to Jame et al. (2017), the expected test of the MSE, for a given value of x(0), can be decomposed into the sum of three fundamental quantities: \\[E (y_{0}-\\hat{f(x_{0}))^{2}}= Var(\\hat{f(x_{0}))}+[Bias\\ \\hat{f(x_{0}))}]^2+Var[\\epsilon]\\] where \\(E (y_{0}-\\hat{f(x_{0}))^{2}}\\) is the expected test MSE. It has the meaning of the expected average test MSE that we would obtain if we repeatedly estimated test MSE f using a large number of training sets, and tested at x0. The overall expected test MSE can be computed by averaging. Variance refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different \\(\\hat{f}\\). On the other hand, bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. For example, linear regression assumes that there is a linear relationship between Y and X1,X2, . . . , Xp. It is unlikely that any real-life problem truly has such a simple linear relationship, and so performing linear regression will undoubtedly result in some bias in the estimate of f. Finally, \\(\\epsilon\\) is the error term. When the number of observations, n, is much larger than the number of independent variables, x, then the least squares estimates tend to also have low variance, \\(Var(\\hat{f(x_{0}))}\\), and consequently reducing expected test of the MSE, \\(E (y_{0}-\\hat{f(x_{0}))^{2}}\\), improving the accuracy. However, the more the number of x, relative to the number of observations, the then there can be a lot of variability in the least squares fit, resulting in overfitting and consequently poor predictions on future observations not used in model training. In terms of the equation \\[Var(\\hat{f(x_{i})}) = Var(\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n})\\] Then, the more parameters would be estimated, the biger the variance would be, and When do we not apply when we are applying Ordinary Least Squares OLS looking for a causality, or trying to explain the dependent variable, When you looked at the top five rows, you probably noticed that the values in the ocean_proximity column were repetitive, which means that it is probably a categorical attribute. You can find out what categories exist and how many districts belong to each category applying the duplicated function: col=&quot;ocean_proximity&quot; house_train[,col][!duplicated(house_train[,col])] ## [1] &quot;INLAND&quot; &quot;&lt;1H OCEAN&quot; &quot;NEAR BAY&quot; &quot;NEAR OCEAN&quot; &quot;ISLAND&quot; If you run a regression including ocean_proximity, you will notice that the regression estimates a coefficient by each category of the variable ocean_proximity. When applying Machine Learning ML for forecasting pourpuses, is more convenient to transform the categorical . We need only one coefficient associated with the variable ocean_proximity. Then we need to transform the variable into numeric. When do we not apply when we are applying Ordinary Least Squares OLS looking for a causality, or trying to explain the dependent variable, dep&lt;-&quot;median_house_value&quot; model&lt;-lm(median_house_value~.,data=house_train) summary(model) ## ## Call: ## lm(formula = median_house_value ~ ., data = house_train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -406981 -41703 -9752 28771 453689 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.432e+06 9.861e+04 -24.662 &lt; 2e-16 *** ## longitude -2.759e+04 1.141e+03 -24.185 &lt; 2e-16 *** ## latitude -2.605e+04 1.128e+03 -23.096 &lt; 2e-16 *** ## housing_median_age 1.069e+03 4.835e+01 22.103 &lt; 2e-16 *** ## total_rooms 2.955e+00 1.057e+00 2.796 0.00518 ** ## total_bedrooms 1.355e+01 8.978e+00 1.510 0.13118 ## population -4.696e+01 1.349e+00 -34.808 &lt; 2e-16 *** ## households 1.174e+02 9.481e+00 12.383 &lt; 2e-16 *** ## median_income 4.200e+04 4.232e+02 99.249 &lt; 2e-16 *** ## ocean_proximityINLAND -3.486e+04 1.940e+03 -17.972 &lt; 2e-16 *** ## ocean_proximityISLAND 2.265e+05 4.781e+04 4.739 2.17e-06 *** ## ocean_proximityNEAR BAY -4.954e+03 2.114e+03 -2.343 0.01913 * ## ocean_proximityNEAR OCEAN 4.077e+03 1.724e+03 2.365 0.01805 * ## rooms_per_household 2.595e+03 3.163e+02 8.205 2.47e-16 *** ## bedrooms_per_room 2.970e+05 1.547e+04 19.197 &lt; 2e-16 *** ## population_per_household 5.979e+02 1.254e+02 4.766 1.89e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 67560 on 16320 degrees of freedom ## (176 observations deleted due to missingness) ## Multiple R-squared: 0.6584, Adjusted R-squared: 0.6581 ## F-statistic: 2097 on 15 and 16320 DF, p-value: &lt; 2.2e-16 For this case, the objective is to make a forecast, so is convenient to transform the categorical values into numeric. We apply the function asnum from the library dataclean. library(datapro) house_train&lt;-asnum(house_train) dep&lt;-&quot;median_house_value&quot; model&lt;-lm(median_house_value~.,data=house_train) summary(model) ## ## Call: ## lm(formula = median_house_value ~ ., data = house_train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -435692 -42148 -10821 29840 455544 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.584e+06 7.403e+04 -48.417 &lt; 2e-16 *** ## longitude -4.146e+04 8.458e+02 -49.023 &lt; 2e-16 *** ## latitude -4.100e+04 7.901e+02 -51.898 &lt; 2e-16 *** ## housing_median_age 1.137e+03 4.772e+01 23.835 &lt; 2e-16 *** ## total_rooms 2.336e+00 1.068e+00 2.188 0.0287 * ## total_bedrooms 1.412e+01 9.073e+00 1.556 0.1198 ## population -4.767e+01 1.363e+00 -34.978 &lt; 2e-16 *** ## households 1.221e+02 9.579e+00 12.743 &lt; 2e-16 *** ## median_income 4.319e+04 4.215e+02 102.454 &lt; 2e-16 *** ## ocean_proximity -5.589e+01 4.052e+02 -0.138 0.8903 ## rooms_per_household 2.920e+03 3.192e+02 9.149 &lt; 2e-16 *** ## bedrooms_per_room 3.335e+05 1.551e+04 21.504 &lt; 2e-16 *** ## population_per_household 5.758e+02 1.268e+02 4.542 5.62e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 68290 on 16323 degrees of freedom ## (176 observations deleted due to missingness) ## Multiple R-squared: 0.6509, Adjusted R-squared: 0.6506 ## F-statistic: 2536 on 12 and 16323 DF, p-value: &lt; 2.2e-16 Now we see that the ocean_proximity variable onpy have one coefficient. 4.4 Feature Scaling One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the target values (y, dependent variable) is generally not required. There are two common ways to get all attributes to have the same scale: min-max scaling and standardization. Min-max scaling (many people call this normalization) is the simplest: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min. Standardization. first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance. "],["select-and-train-and-evaluate-the-model.html", "Chapter 5 5 Select and Train and evaluate the Model 5.1 For continuous variables 5.2 Select the model for categorical variables 5.3 Resampling (cross-validation)", " Chapter 5 5 Select and Train and evaluate the Model 5.1 For continuous variables For continuous variables, the goal of a machine learning model such as linear regression by OLS is to predict a variable y. For example, the “median_house_value” from the house pricing data set. \\[y=\\beta_{0}+\\beta_{1}x_{1}+,..,+\\beta_{n}x_{n} +\\epsilon \\] where \\(x_{1}\\), \\(x_{n}\\) are the independent variables and \\(\\epsilon\\) is the error term. In this case, the predicted value of \\(\\hat{y}\\) : \\[\\hat{y}=\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n} \\] \\[\\hat{y}=\\hat{\\beta_{0}}+\\hat{\\beta_{1}}Z_{1}+,..,+\\hat{\\beta_{n}}Z_{m} \\] For the “median_house_value” from the house pricing data set, we use the function lm to make a Ordinary Least squares OLS estimation: dep&lt;-&quot;median_house_value&quot; house_model&lt;-lm(median_house_value~.,data=house_train) summary(house_model) ## ## Call: ## lm(formula = median_house_value ~ ., data = house_train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -435692 -42148 -10821 29840 455544 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.584e+06 7.403e+04 -48.417 &lt; 2e-16 *** ## longitude -4.146e+04 8.458e+02 -49.023 &lt; 2e-16 *** ## latitude -4.100e+04 7.901e+02 -51.898 &lt; 2e-16 *** ## housing_median_age 1.137e+03 4.772e+01 23.835 &lt; 2e-16 *** ## total_rooms 2.336e+00 1.068e+00 2.188 0.0287 * ## total_bedrooms 1.412e+01 9.073e+00 1.556 0.1198 ## population -4.767e+01 1.363e+00 -34.978 &lt; 2e-16 *** ## households 1.221e+02 9.579e+00 12.743 &lt; 2e-16 *** ## median_income 4.319e+04 4.215e+02 102.454 &lt; 2e-16 *** ## ocean_proximity -5.589e+01 4.052e+02 -0.138 0.8903 ## rooms_per_household 2.920e+03 3.192e+02 9.149 &lt; 2e-16 *** ## bedrooms_per_room 3.335e+05 1.551e+04 21.504 &lt; 2e-16 *** ## population_per_household 5.758e+02 1.268e+02 4.542 5.62e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 68290 on 16323 degrees of freedom ## (176 observations deleted due to missingness) ## Multiple R-squared: 0.6509, Adjusted R-squared: 0.6506 ## F-statistic: 2536 on 12 and 16323 DF, p-value: &lt; 2.2e-16 5.1.1 Linear Model Selection The spirit is the same we mention in a previous section, regarding the expected test of the MSE, for a given value of x(0), \\[E (y_{0}-\\hat{f(x_{0}))^{2}}= Var(\\hat{f(x_{0}))}+[Bias\\ \\hat{f(x_{0}))}]^2+Var[\\epsilon]\\] In other words, we look for a subset of independent variables or predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables, as as consequence reducing the variance of the \\(\\hat{f}\\), the test of the MSE, which improves the prediction accuracy. stepw&lt;-step(house_model, direction = &quot;both&quot;,trace = F) summary(stepw) ## ## Call: ## lm(formula = median_house_value ~ longitude + latitude + housing_median_age + ## total_rooms + total_bedrooms + population + households + ## median_income + rooms_per_household + bedrooms_per_room + ## population_per_household, data = house_train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -435760 -42172 -10800 29838 455571 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.581e+06 7.073e+04 -50.637 &lt; 2e-16 *** ## longitude -4.143e+04 8.111e+02 -51.080 &lt; 2e-16 *** ## latitude -4.098e+04 7.731e+02 -53.005 &lt; 2e-16 *** ## housing_median_age 1.137e+03 4.769e+01 23.845 &lt; 2e-16 *** ## total_rooms 2.329e+00 1.066e+00 2.184 0.029 * ## total_bedrooms 1.409e+01 9.071e+00 1.554 0.120 ## population -4.765e+01 1.355e+00 -35.157 &lt; 2e-16 *** ## households 1.221e+02 9.578e+00 12.744 &lt; 2e-16 *** ## median_income 4.319e+04 4.196e+02 102.950 &lt; 2e-16 *** ## rooms_per_household 2.919e+03 3.191e+02 9.148 &lt; 2e-16 *** ## bedrooms_per_room 3.335e+05 1.550e+04 21.509 &lt; 2e-16 *** ## population_per_household 5.754e+02 1.267e+02 4.540 5.67e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 68290 on 16324 degrees of freedom ## (176 observations deleted due to missingness) ## Multiple R-squared: 0.6509, Adjusted R-squared: 0.6506 ## F-statistic: 2767 on 11 and 16324 DF, p-value: &lt; 2.2e-16 5.2 Select the model for categorical variables 5.2.1 Clasification models The linear regression assumes that the response variable Y is quantitative. But in many situations, the response variable is instead qualitative. For example, eye color is qualitative, taking on values blue, brown, or green. Often qualitative variables are referred to as categorical ; we will use these terms interchangeably. In this chapter, we study approaches for predicting qualitative responses, a process that is known as classification. Predicting a qualitative response for an observation can be referred to as classifying that observation, since it involves assigning the observation to a category, or class. On the other hand, often the methods used for classification first predict the probability of each of the categories of a qualitative variable, as the basis for making the classification. In this sense they also behave like regression methods. We discuss three of the most widely-used classifiers: logistic regression, linear discriminant analysis, and logistic regression linear discriminant analysis K-nearest neighbors. Examples of classification: 1. A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have? An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history, and so forth. On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not 5.2.1.1 Logistic Regression In a binary response model, interest lies primarily in the response probability: \\[P(y=1/X)=P(y=1|x_{1},x_{2},..,x_{n}) \\] where we use X to denote the full set of explanatory variables. For example, when y is the default variable in the house pricing example or direction in the bit coin example. On the other hand, x is the set of independent variables. The previous equation reads: the probability that y=1, given X is equal to the probability that y=1, given \\(x_{1},x_{2},..,x_{n}\\). To simplify, we usually use a binary response c(0,1). One for the result of interest, the default for example, or zero for other case, not default. To ensures that the result predictions be a binary response c(0,1), also. First we estimate response probabilities that are strictly between zero and one. Specifying a model: \\[P(y=1/X)=G(\\beta_{0}+\\beta_{1}x_{1}+,..,+\\beta_{n}x_{n}) \\] where G is a function taking on values strictly between zero and one: 0 &lt; G &lt; 1. Various nonlinear functions have been suggested for the function G to make sure that the probabilities are between zero and one. The more common is the logit model, where G is the logistic function: \\[G(z)=\\frac{exp(z)}{1-(exp(z)}\\] which is between zero and one for all real numbers z. This is the cumulative distribution function for a standard logistic random variable. For the logit model z is the predicted model. \\[ z=\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n}\\] \\[G(z)=\\frac{exp(z)}{1-(exp(z)}\\] Then, \\[P(y=1/X)=\\frac{exp(\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n})}{1-(exp(\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n})}\\] Which is the equation we will use for making a prediction. However, we can not use the OLS to estimate the model, because it is a not linear binary response model. Then we apply the maximum likelihood estimation (MLE). For the credit data set we use the function glm to estimate the Logistic regression. Before that, we trasnform the categorical values into numerival and eliminate Nas. credit_train&lt;-asnum(credit_train) credit_train&lt;-na.omit(credit_train) credit_test&lt;-asnum(credit_test) credit_test&lt;-na.omit(credit_test) ## ## Call: ## glm(formula = Default ~ ., family = binomial(), data = credit_train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6715 -0.5833 -0.4045 -0.2269 2.7163 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.146e+00 2.695e+00 -1.538 0.12405 ## loan_amnt -1.659e-04 1.074e-04 -1.546 0.12222 ## term 2.136e+00 7.051e-01 3.030 0.00245 ** ## int_rate -6.764e-01 4.270e-01 -1.584 0.11317 ## installment 5.081e-03 3.390e-03 1.499 0.13397 ## grade -2.724e-01 4.041e-01 -0.674 0.50019 ## sub_grade 5.775e-01 3.035e-01 1.903 0.05708 . ## emp_title 5.335e-04 7.294e-04 0.731 0.46456 ## emp_length 6.027e-02 4.280e-02 1.408 0.15909 ## home_ownership 3.385e-01 1.383e-01 2.448 0.01437 * ## annual_inc -2.877e-06 4.169e-06 -0.690 0.49008 ## verification_status -2.748e-01 1.594e-01 -1.724 0.08479 . ## issue_d NA NA NA NA ## purpose -6.848e-03 8.858e-02 -0.077 0.93838 ## title 8.493e-02 8.682e-02 0.978 0.32796 ## dti 1.688e-02 1.542e-02 1.094 0.27381 ## earliest_cr_line -1.137e-03 1.318e-03 -0.863 0.38825 ## open_acc 5.082e-02 2.991e-02 1.699 0.08928 . ## pub_rec 4.611e-01 2.138e-01 2.157 0.03101 * ## revol_bal -1.050e-05 1.120e-05 -0.937 0.34851 ## revol_util 4.008e-03 5.821e-03 0.689 0.49111 ## total_acc -3.877e-03 1.334e-02 -0.291 0.77136 ## initial_list_status 8.969e-01 6.913e-01 1.297 0.19451 ## application_type -7.003e-01 1.230e+00 -0.569 0.56921 ## mort_acc -1.104e-01 7.737e-02 -1.427 0.15358 ## pub_rec_bankruptcies -4.767e-01 3.707e-01 -1.286 0.19838 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 631.12 on 697 degrees of freedom ## Residual deviance: 521.51 on 673 degrees of freedom ## AIC: 571.51 ## ## Number of Fisher Scoring iterations: 5 ## ## Call: ## glm(formula = Default ~ term + sub_grade + home_ownership + verification_status + ## open_acc + pub_rec + mort_acc + annual_inc, family = binomial(), ## data = credit_train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1902 -0.6038 -0.4188 -0.2584 2.6427 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.085e+00 5.892e-01 -6.933 4.11e-12 *** ## term 1.023e+00 2.731e-01 3.745 0.00018 *** ## sub_grade 8.677e-02 2.148e-02 4.040 5.35e-05 *** ## home_ownership 3.066e-01 1.323e-01 2.318 0.02048 * ## verification_status -2.955e-01 1.521e-01 -1.943 0.05207 . ## open_acc 3.959e-02 1.937e-02 2.043 0.04101 * ## pub_rec 3.551e-01 1.704e-01 2.084 0.03711 * ## mort_acc -1.327e-01 7.199e-02 -1.843 0.06531 . ## annual_inc -4.849e-06 3.321e-06 -1.460 0.14427 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 631.12 on 697 degrees of freedom ## Residual deviance: 536.64 on 689 degrees of freedom ## AIC: 554.64 ## ## Number of Fisher Scoring iterations: 5 #credit_test&lt;-asnum(credit_test) We apply the stepwise methodology to estimate a model using the argument “both” and trace = F: ## 15 19 24 26 32 34 ## -1.4750804 -2.2810764 -0.9013844 -3.4618845 -0.5504505 -3.1938712 Take the stepwise result to predict the final model but this time using the test dataset, print the head and tai The argument type = “response” transform automatically the results into a probability. In other words, applies the formula exp(x)/(1+exp(x)). Then if we use that argument, is no correct to transform it into a probability. predicted_test &lt;- predict(model, newdata=) Transform into a probabilistic model, applying the formula exp(x)/(1+exp(x)). Print the head and tail of the result: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.01140 0.05696 0.10391 0.15324 0.19847 0.67279 Create a conditional, if the prediction is bigger than 0.5, then 1, otherwise 0. Plot the results: Now measure the Accuracy applying the confusion Matrix: ## Loading required package: ggplot2 ## Loading required package: lattice ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 0 ## 1 6 1 ## 0 22 146 ## ## Accuracy : 0.8686 ## 95% CI : (0.8093, 0.9148) ## No Information Rate : 0.84 ## P-Value [Acc &gt; NIR] : 0.1775 ## ## Kappa : 0.2979 ## ## Mcnemar&#39;s Test P-Value : 3.042e-05 ## ## Sensitivity : 0.21429 ## Specificity : 0.99320 ## Pos Pred Value : 0.85714 ## Neg Pred Value : 0.86905 ## Prevalence : 0.16000 ## Detection Rate : 0.03429 ## Detection Prevalence : 0.04000 ## Balanced Accuracy : 0.60374 ## ## &#39;Positive&#39; Class : 1 ## 5.2.1.2 Linear Discriminant Analysis LDA Why do we need another method, when we have logistic regression? There are several reasons: • When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem. • If number of observations n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model. We apply the LDA model to the same credit data set. However, we takeout the variable issue_d, otherwise the model is not estimated. credit_train&lt;-credit_train[,-12] credit_test&lt;-credit_test[,-12] library (MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select lda.fit&lt;-lda(Default~.,data=credit_train) lda.fit ## Call: ## lda(Default ~ ., data = credit_train) ## ## Prior probabilities of groups: ## 0 1 ## 0.8323782 0.1676218 ## ## Group means: ## loan_amnt term int_rate installment grade sub_grade emp_title ## 0 14992.13 1.173838 11.30895 453.9710 2.401033 9.772806 262.6816 ## 1 15283.55 1.461538 14.55598 437.5981 3.316239 14.504274 272.5556 ## emp_length home_ownership annual_inc verification_status purpose title ## 0 5.117040 1.752151 81069.05 1.764200 3.258176 5.067126 ## 1 5.512821 2.094017 70985.66 1.786325 3.418803 5.188034 ## dti earliest_cr_line open_acc pub_rec revol_bal revol_util total_acc ## 0 18.90582 154.9449 12.38554 0.2271945 18348.62 49.06936 26.30465 ## 1 21.41171 151.0171 13.74359 0.3418803 15609.53 50.56667 27.13675 ## initial_list_status application_type mort_acc pub_rec_bankruptcies ## 0 1.955250 1.008606 1.975904 0.1376936 ## 1 1.974359 1.008547 1.316239 0.1794872 ## ## Coefficients of linear discriminants: ## LD1 ## loan_amnt -2.627499e-04 ## term 2.977285e+00 ## int_rate -4.457492e-01 ## installment 7.977948e-03 ## grade -1.942167e-01 ## sub_grade 3.744479e-01 ## emp_title 3.137385e-04 ## emp_length 5.217647e-02 ## home_ownership 3.159080e-01 ## annual_inc -1.086106e-06 ## verification_status -2.259384e-01 ## purpose -1.458813e-02 ## title 7.267585e-02 ## dti 1.467009e-02 ## earliest_cr_line -1.054073e-03 ## open_acc 5.127933e-02 ## pub_rec 4.644621e-01 ## revol_bal -4.349356e-06 ## revol_util 1.522495e-03 ## total_acc -6.584011e-03 ## initial_list_status 6.992175e-01 ## application_type -6.901451e-01 ## mort_acc -6.956526e-02 ## pub_rec_bankruptcies -4.437124e-01 lda.pred&lt;-predict(lda.fit, credit_test) #names(lda.pred) lda.pred$class ## [1] 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ## [38] 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 ## [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 ## Levels: 0 1 Defaultf_lda&lt;-factor(lda.pred$class,levels=c(1,0)) credit_testf&lt;-factor(credit_test$Default,levels=c(1,0)) confusionMatrix(Defaultf_lda,credit_testf) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 0 ## 1 5 4 ## 0 23 143 ## ## Accuracy : 0.8457 ## 95% CI : (0.7835, 0.8958) ## No Information Rate : 0.84 ## P-Value [Acc &gt; NIR] : 0.468215 ## ## Kappa : 0.2087 ## ## Mcnemar&#39;s Test P-Value : 0.000532 ## ## Sensitivity : 0.17857 ## Specificity : 0.97279 ## Pos Pred Value : 0.55556 ## Neg Pred Value : 0.86145 ## Prevalence : 0.16000 ## Detection Rate : 0.02857 ## Detection Prevalence : 0.05143 ## Balanced Accuracy : 0.57568 ## ## &#39;Positive&#39; Class : 1 ## 5.2.2 Training and Evaluating on the Training Set Estimating the RMSE #dep=&quot;median_house_value&quot; #rmse&lt;-cbind(test[,dep],predicted_test) #sqrt(mean((rmse[,2]-rmse[,1])^2,na.rm = T )) 5.3 Resampling (cross-validation) Resampling methods are an indispensable tool in modern statistics. They involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model. For example, in order to estimate the variability of a linear regression fit, we can repeatedly draw different samples from the training data, fit a linear regression to each new sample, and then examine the extent to which the resulting fits differ. Such an approach may allow us to obtain information that would not be available from fitting the model only once using the original training sample. One of the most common resampling method is K Fold Cross Validation. Cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility. The process of evaluating a model’s performance is known as model assessment, whereas model the process of selecting the proper level of flexibility for a model is known as assessment model selection. K Fold Cross Validation This approach involves randomly k-fold CV dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds. The mean squared error, MSE1, is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a validation set. \\[CV_{k} =\\frac{1}{k}\\ \\sum_{i=1}^{k} MSE_{i} \\] In practice, one typically performs k-fold CV using k = 5 or k = 10. "],["fine-tune-or-tune-the-ml-model.html", "Chapter 6 6 Fine-Tune or Tune the ML Model 6.1 Analyze the Best Models and Their Errors 6.2 Evaluate Your System on the Test Set", " Chapter 6 6 Fine-Tune or Tune the ML Model Tuning a machine learning model is an iterative process. Data scientists typically run numerous experiments to train and evaluate models, trying out different features, different loss functions, different AI/ML models, and adjusting model parameters and hyperparameters. Examples of steps involved in tuning and training a machine learning model include feature engineering, loss function formulation, model testing and selection, regularization, and selection of hyperparameters Krishnan (2022). Let’s assume that you now have a shortlist of promising models. You now need to fine-tune them. Let’s look at a few ways you can do that. Once we have retrieved optimum values of individual model parameters then we can use grid search to obtain combination of hyperparameter (The parameters are also known as hyperparameters) values of a model that can give us the highest accuracy. Grid Search evaluates all possible combinations of the parameter values. Grid Search is exhaustive and uses brute-force to evaluate the most accurate values. Therefore it is computationally intensive task. 6.1 Analyze the Best Models and Their Errors 6.2 Evaluate Your System on the Test Set "],["references.html", "Chapter 7 References", " Chapter 7 References Géron, A. (2019) Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition. O’Reilly, Sebastopol, CA. James, G, Witten, D, Hastie, T and Tibshirani, R (2017). An Introduction to Statistical Learning with Applications in R. Springer, NY. Krishnan, N. (2022). Enterprise AI and Machine Learning for Managers. Retrieved from c3.ai: https://c3.ai/publications/enterprise-ai-and-machine-learning-for-managers/ Lantz, B (2015). Machine Learning with R Second Edition. Packt Publishing, Birmingham, UK. Wooldridge (2013). Introductory econometrics. Cengage Learning. Mason, OH "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
