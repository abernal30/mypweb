[{"path":"index.html","id":"algorithms-and-financial-programing-in-r","chapter":"Algorithms and Financial Programing in R","heading":"Algorithms and Financial Programing in R","text":"book Algorithms Financial Programing R!work Aturo Bernal\nVisit GitHub repository site.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"started writing book guidance undergraduate courses related algorithms financial programming. extend content become book wish start learning program R want expand knowledge programming applications finance. codes book R written RStudio Markdowns worldwide known programming language.\nbook aims get introductory programming competencies, well topics data cleaning, data analysis, machine learning, employ concepts finance.R files stored GitHub repository site.","code":""},{"path":"preface.html","id":"outline","chapter":"Preface","heading":"Outline","text":"","code":""},{"path":"r-basics.html","id":"r-basics","chapter":"1 R Basics","heading":"1 R Basics","text":"","code":""},{"path":"r-basics.html","id":"r-markdown","chapter":"1 R Basics","heading":"1.1 1 R Markdown","text":"book work R Markdowns, document format embed code chunks (R languages) documents. important, allows print (knitr) authoring languages including LaTeX, HTML, Text, among others.See markdown content : (Yihui Xie Grolemund 2019).","code":""},{"path":"r-basics.html","id":"coding-basics","chapter":"1 R Basics","heading":"1.2 2 Coding basics","text":"entities can create manipulate R called objects. may variables, arrays numbers, character strings, functions, general structures. objects created applying assignment operator (‘<-’). consists two characters ‘<’ (“less ”) ‘-’ (“minus”) occurring strictly side--side ‘points’ object receiving value expression (Team 2022).also apply operator ‘=’, however, experience, functions use “=” operator inside, confused.example, create object “”, wich assigned value 4.delete object environment. can also use function rm(). However, suggest using R-studio. introduction RStudio suggest reviewing chapter 1 book (Ismay Kim 2019)","code":"\na<- 4 \nrm(a)"},{"path":"r-basics.html","id":"objet-types-atomic-structures","chapter":"1 R Basics","heading":"1.3 Objet types: “atomic” structures","text":"Types objects frequently used finance applications numeric, character, vectors logical. known “atomic” structures since components type. rest objects, like matrix Data frames, built atomic objects.Character strings entered using either matching double (“) single (’) quotes. example:print object use function print just typing object name.review object class use function “class”:Numeric objects.vector consist ordered collection numbers characters. build vector apply function concatenate “c()”. example,can see, object class numeric, vector taking class atomic objects, case numeric. example character vector:Selecting element vector. select element use brackets: “[]”. example, select first element vector “v2”:Also select sub sample:former example just select sub sample, object “v2” hasn’t changed. want change object need create new one. example, want delete element, use minus sign “-”. example 2nd element “v2”:case object “v2” changed. Also, change element vector, example, changing element “Amazon” “Meta”:like add new element, example “Amazon_new”, need apply “c” function:","code":"\nticker<-\"APPL\" \nticker\n#> [1] \"APPL\"\n# or \nprint(ticker)\n#> [1] \"APPL\"\nclass(ticker)\n#> [1] \"character\"\nnum<-4\nprint(num)\n#> [1] 4\nprint(num)\n#> [1] 4\nprint(class(num))\n#> [1] \"numeric\"\nv1<-c(160,165,167,145,145)\nprint(v1)\n#> [1] 160 165 167 145 145\nclass(v1)\n#> [1] \"numeric\"\nv2<-c(\"Apple\",\"Meta\",\"Amazon\")\nprint(v2)\n#> [1] \"Apple\"  \"Meta\"   \"Amazon\"\nclass(v2)\n#> [1] \"character\"\nv2[1]\n#> [1] \"Apple\"\nv2[1:2]\n#> [1] \"Apple\" \"Meta\"\nv2<-v2[-2]\nv2\n#> [1] \"Apple\"  \"Amazon\"\nv2[2]<-\"Meta\"\nv2\n#> [1] \"Apple\" \"Meta\"\nv2<-c(v2,\"Amazon_new\")\nv2\n#> [1] \"Apple\"      \"Meta\"       \"Amazon_new\""},{"path":"r-basics.html","id":"data-frames","chapter":"1 R Basics","heading":"1.4 Data frames","text":"Finance common practical using Data Frames, tabular-form data objects column can different form, , numeric character.example going use already created data frame, library Wooldridge. Get data frame k401k library WooldridgeRemember library set functions someone created. Wooldridge library many data sets econometrics book author (Wooldridge 2020).import library, apply function library()import databases library, library must imported, just calling data set name, case “k401k”.can see object class Data Frame.function “colnames” shows names column data frame. case, character vector:times convenient change column rows names data frame, example, change name first column, ““prate_1”. case use function “colnames” select, brackets, column number want change. changing column names vector, need establish assignment operator “<-”.show change row name, use “rownames” function. convenience, select first 5 rows.apply procedure made “colnames” function modify row data frame.Selecting rows columnsThere many ways select column row data frame.Selecting rows columns position, example selecting first row, column 5. data frame two dimensions, rows columns, selecting also use brackets, separating rows columns aSelecting columns $ symbolMerging two data frames columns.Suppose following Data Frame:Print dimension data frame, applying function paste, print dim:Applying function cbind merge two data frames call object df3Note method duplicating column age.takeoff one columns, select number position adding minus symbolCreate new variable, tot_part_age (totpart/age) variable row names index, call index, data frame. Insert object df3.Eliminate 2nd row object df3 call df4.Apply function cbind merge df3 df4It goig show debug “Error data.frame(…, check.names = FALSE) :\narguments imply differing number rows: 6, 5”, means number rows .Careful: number rows data frames multiple another, coincidence cbind function merge. However, R going fill missing values repeating values data frame.Try now function merge(x,y,.x=,.y=,=T F, .x=T F, .y=T F)merge function needs pivot reference variable make merge. case column index identification id (share variable). id must unique value row must present data frames. Also, need specify want keep data data frame x y.","code":"\nlibrary(wooldridge) \nk4<-k401k\nclass(k4)\n#> [1] \"data.frame\"\ncolnames(k4)\n#> [1] \"prate\"   \"mrate\"   \"totpart\" \"totelg\"  \"age\"     \"totemp\"  \"sole\"   \n#> [8] \"ltotemp\"\ncolnames(k4)[1]<-\"prate_1\" \ncolnames(k4)\n#> [1] \"prate_1\" \"mrate\"   \"totpart\" \"totelg\"  \"age\"     \"totemp\"  \"sole\"   \n#> [8] \"ltotemp\"\nrownames(k4)[1:5]\n#> [1] \"1\" \"2\" \"3\" \"4\" \"5\"\nk4[1,5]\n#> [1] 8\nk4$age[1:10]\n#>  [1]  8  6 10  7 28  7 31 13 21 10\ndf1<-k4[1:6,c(\"prate_1\",\"totpart\",\"age\")]\ndf2<-k4[1:6,c(\"age\",\"totemp\")]\n\ndf3<-cbind(df1,df2)\nhead(df3,10) \n#>   prate_1 totpart age age totemp\n#> 1    26.1    1653   8   8   8709\n#> 2   100.0     262   6   6    315\n#> 3    97.6     166  10  10    275\n#> 4   100.0     257   7   7    500\n#> 5    82.5     591  28  28    933\n#> 6   100.0      92   7   7    143\ndim<-dim(df3) \ndim\n#> [1] 6 5\ndf1<-k4[1:6,c(\"prate_1\",\"totpart\",\"age\")]\ndf2<-k4[1:6,c(\"age\",\"totemp\")]\ndf3<-cbind(df1,df2)\ndf3\n#>   prate_1 totpart age age totemp\n#> 1    26.1    1653   8   8   8709\n#> 2   100.0     262   6   6    315\n#> 3    97.6     166  10  10    275\n#> 4   100.0     257   7   7    500\n#> 5    82.5     591  28  28    933\n#> 6   100.0      92   7   7    143\ndf3<-df3[,-3]\ndf3\n#>   prate_1 totpart age totemp\n#> 1    26.1    1653   8   8709\n#> 2   100.0     262   6    315\n#> 3    97.6     166  10    275\n#> 4   100.0     257   7    500\n#> 5    82.5     591  28    933\n#> 6   100.0      92   7    143\ndf3[\" tot_part_age\"]<-(df3[,\"totpart\"]/df3[,\"age\"])\ndf3[\"index\"]<-rownames(df3)\ndf4<-df3[-2,]\n#df5<-cbind(df3,df4)\ndf5<-merge(df3,df4,by.x=\"age\",by.y=\"age\")\ndf5\n#>   age prate_1.x totpart.x totemp.x  tot_part_age.x index.x prate_1.y totpart.y\n#> 1   7     100.0       257      500        36.71429       4     100.0       257\n#> 2   7     100.0       257      500        36.71429       4     100.0        92\n#> 3   7     100.0        92      143        13.14286       6     100.0       257\n#> 4   7     100.0        92      143        13.14286       6     100.0        92\n#> 5   8      26.1      1653     8709       206.62500       1      26.1      1653\n#> 6  10      97.6       166      275        16.60000       3      97.6       166\n#> 7  28      82.5       591      933        21.10714       5      82.5       591\n#>   totemp.y  tot_part_age.y index.y\n#> 1      500        36.71429       4\n#> 2      143        13.14286       6\n#> 3      500        36.71429       4\n#> 4      143        13.14286       6\n#> 5     8709       206.62500       1\n#> 6      275        16.60000       3\n#> 7      933        21.10714       5"},{"path":"r-basics.html","id":"xts-objects","chapter":"1 R Basics","heading":"1.5 xts objects","text":"xts class object provide uniform handling R’s different time-based data classes. Also, API´s, quantmod, download data xts format. example, library xts write xlsx file data set “sample_matrix”.next section covered read “xlsx” file.default object class data frame. feature “xts” objects rownames dates objects. first replace numerical rownames dates inside object.useful functions can use xts objects, example transforming weekly, monthly, quarterly, yearly ,etc.Making sub sample:Note two examples, use apply.monthly object like data_df_2 work, rownames dates, can´t apply subset function object, generate empty object.","code":"\n#data(sample_matrix)\n#sm<-get(\"sample_matrix\")\n#data_df<-data.frame(sample_matrix)\n#date<-rownames(data_df)\n#data_df<-cbind(date,data_df)\n#write.xlsx(data_df,\"data/data_df.xlsx\")\nlibrary(openxlsx)\ndata_df<-read.xlsx(\"data/data_df.xlsx\")\ndate<-data_df[,1]\nrownames(data_df)<-date\n# Also I eliminate the dates in row one. \ndata_df_2<-data_df[,-1]\nlibrary(xts)\ndata_xts<- as.xts(data_df_2)\ndata_xt_m<-apply.monthly(data_xts,mean)\nsub_set<-subset(data_xts,\n  +index(data_xts)>=\"2007-05-01\" &\n  +index(data_xts)<=\"2007-06-30\")"},{"path":"r-basics.html","id":"reading-and-writing-csv-and-xlsx","chapter":"1 R Basics","heading":"1.6 Reading and writing csv and xlsx","text":"libraries write open xlsx csv files. suggest using “openxlsx”.open file use, File must directory need specify directory location; otherwise, error:","code":"\nwrite.xlsx(df5,\"data/df5.xlsx\")\nwrite.csv(df5,\"data/df5.csv\")\nlibrary(openxlsx)\nfdf5_x<-read.xlsx(\"data/df5.xlsx\")\nfdf5_c<-read.csv(\"data/df5.csv\")"},{"path":"clean.html","id":"clean","chapter":"2 Big data and data cleaning","heading":"2 Big data and data cleaning","text":"chapter use file credit_semioriginal.xlsx, historical information lendingclub, https://www.lendingclub.com/ fintech marketplace bank scale. original data set least 2 million observations 150 variables. find credit_semioriginal.xlsx first 1,000 observations 150 variables. using 2 million rows sample make processor low, challenge try original data set see big data .dataset source:\nhttps://www.kaggle.com/wordsforthewise/lending-clubReview data structure credit dataset descriptive statistics, first 10 columns.see numerical columns categorical. categorical mean elements characters.","code":"\nlibrary(openxlsx)\ndata<-read.xlsx(\"data/credit_semioriginal.xlsx\")\nstr(data[,1:10])\n#> 'data.frame':    1000 obs. of  10 variables:\n#>  $ loan_amnt      : num  3600 24700 20000 35000 10400 ...\n#>  $ funded_amnt    : num  3600 24700 20000 35000 10400 ...\n#>  $ funded_amnt_inv: num  3600 24700 20000 35000 10400 ...\n#>  $ term           : chr  \"36 months\" \"36 months\" \"60 months\" \"60 months\" ...\n#>  $ int_rate       : num  14 12 10.8 14.8 22.4 ...\n#>  $ installment    : num  123 820 433 830 290 ...\n#>  $ grade          : chr  \"C\" \"C\" \"B\" \"C\" ...\n#>  $ sub_grade      : chr  \"C4\" \"C1\" \"B4\" \"C5\" ...\n#>  $ emp_title      : chr  \"leadman\" \"Engineer\" \"truck driver\" \"Information Systems Officer\" ...\n#>  $ emp_length     : chr  \"10+ years\" \"10+ years\" \"10+ years\" \"10+ years\" ..."},{"path":"clean.html","id":"categorical-into-numerical-filtering-and-coditionals","chapter":"2 Big data and data cleaning","heading":"2.1 Categorical into numerical: filtering and coditionals","text":"several reasons transform numerical column variable categorical. detailed explanation suggest review chapter “Handling Text Categorical Attributes” book “Machine learning introductory guide R”. moment functions use chapter work variables categorical.see “loan_status” variable categorical. First review many categories column loan_status :data[,col][!duplicated(data[,“col”])]data name dataframe col column nameThere 5 categories, going transform column verification_status numeric:Create filter, way loan_status contains Fully Paid Charged .data %>%\nfilter(col== “categ1” |col== “categ2”)result, now 145 rows.Besides “loan_status” three several categorical columns, example term, winch 2 categories:method use transform simple, example “36 months” take value one “60 months” value 2. column 3 categories, 3rd categories take value 3 .former example easy 3 categories, however, otherI writted library data processing, “dataclean” install run following code chunk:remotes::install_github(“abernal30/dataclean”)\ndevtools::install_github(“abernal30/dataclean”)use charname function see many categorical variables . print first rows using head function.33 categorical columns. function “tonum” transform categorical column numeric, example transforming column “grade”, following categories:need specify data source column name.Finally, sure want transform data set numerical, function “asnum” reviews detect categorical columns transform numeric, result get data frame. apply function review now winch categorical columns, get .","code":"\ncol<-\"loan_status\"\ndata[,col][!duplicated(data[,col])]\n#> [1] \"Fully Paid\"         \"Current\"            \"Charged Off\"       \n#> [4] \"In Grace Period\"    \"Late (31-120 days)\"#> [1] 5\nlibrary(dplyr)\n\ndata1<-data %>%\n  filter(col== \"Fully Paid\" |loan_status== \"Charged Off\")#> [1] 145\ncol<-\"term\"\ncat<-data[,col][!duplicated(data[,col])]\ncat\n#> [1] \"36 months\" \"60 months\"\n\nncat<-c(1:length(cat))\nncat\n#> [1] 1 2\ncat[1]\n#> [1] \"36 months\"\ncol_cat<-ifelse(data1[, col] == cat[1],ncat[1],data1[, col])\nhead(col_cat)\n#> [1] \"60 months\" \"1\"         \"1\"         \"60 months\" \"60 months\" \"60 months\"\ncol_cat<-ifelse(data1[, col] == cat[1],ncat[1],ncat[2])\ncol_cat\n#>   [1] 2 1 1 2 2 2 2 2 2 2 2 2 2 1 1 2 1 1 2 2 1 2 1 1 1 1 1 1 1 1 2 2 2 2 1 1 2\n#>  [38] 1 2 2 2 1 1 2 2 1 2 2 1 1 1 1 2 1 1 2 1 1 1 1 2 2 2 1 2 2 2 1 1 2 2 1 2 1\n#>  [75] 1 1 1 1 2 1 2 1 2 2 1 1 1 1 2 2 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 2 1 2 1 1\n#> [112] 1 2 2 1 2 2 1 1 1 1 2 2 1 2 1 2 2 2 1 1 2 1 1 2 1 1 1 2 1 2 1 2 1 1\ndata1[1,\"mths_since_recent_bc\"]*2\n#> [1] 18\nlibrary(dataclean)\ncharname(data1)\n#>  [1] \"term\"                      \"grade\"                    \n#>  [3] \"sub_grade\"                 \"emp_title\"                \n#>  [5] \"emp_length\"                \"home_ownership\"           \n#>  [7] \"verification_status\"       \"issue_d\"                  \n#>  [9] \"loan_status\"               \"purpose\"                  \n#> [11] \"title\"                     \"zip_code\"                 \n#> [13] \"addr_state\"                \"earliest_cr_line\"         \n#> [15] \"initial_list_status\"       \"last_pymnt_d\"             \n#> [17] \"next_pymnt_d\"              \"last_credit_pull_d\"       \n#> [19] \"application_type\"          \"verification_status_joint\"\n#> [21] \"hardship_flag\"             \"hardship_type\"            \n#> [23] \"hardship_reason\"           \"hardship_status\"          \n#> [25] \"hardship_start_date\"       \"hardship_end_date\"        \n#> [27] \"payment_plan_start_date\"   \"hardship_loan_status\"     \n#> [29] \"disbursement_method\"       \"debt_settlement_flag\"     \n#> [31] \"debt_settlement_flag_date\" \"settlement_status\"        \n#> [33] \"settlement_date\"#> [1] 33\ncol<-\"grade\"\ncat<-data[,col][!duplicated(data[,col])]\ncat\n#> [1] \"C\" \"B\" \"F\" \"A\" \"E\" \"D\" \"G\"\ncol_cat2<-tonum(data1,col)\nhead(col_cat2)\n#> [1] 1 2 2 3 2 4\ndata2<-asnum(data1)\nhead(charname(data2))\n#> NULL"},{"path":"clean.html","id":"missing-values","chapter":"2 Big data and data cleaning","heading":"2.2 Missing values","text":"treat missing values, suggest taking one following alternatives combination : ) eliminating columns significant amount missing values; ii) eliminating row missing(s) value(s) () located; iii) replace missing values Na´s statistic.firs alternative, lets first apply function “sumna” detect columns 50 percent missing values:case 30 columns 50 percent missing values. like eliminate columns apply following:second alternative, eliminating rows missing(s) value(s) () located; applying na.omit function. However, careful, case row data frame least one missing value, cace delete rows data frame, like case:third alternative replacing missing values metric. us function “repnas”, object data3 wich already drop columns 50 percent missing values:","code":"\nsumna<- function(x,p) {\ndim<-dim(x)\nprov<-c()\nco<-c()\nfor (i in 1:dim[2]){\nsu<-sum(is.na(x[,i]))/dim[1]\nprov<-c(prov,su)\nind<-c()\nco<-c(co,colnames(x[,i]))\n}\nme<-data.frame(prov)\nrownames(me)<-colnames(x)\ndim<-dim(me)\nse<-c(1:dim[1])\nme<-cbind(me,se)\ncole<-c()\ncolem<-c()\ncole_name<-c()\nme\n#}\n#me<-sumna(data2,.5)\n\n#ifelse(is.na(me[1,1])==TRUE,0,me[1,1]) \n\nfor (i in 1:dim[1]){ \nme[i,1]<-ifelse(is.na(me[i,1])==TRUE,0,me[i,1]) \n}\n\nfor (i in 1:dim[1]){ \n\n  if (me[i,1] > p) {\n  cole<-c(cole,me[i,1])  \n  colem<-c(colem,me[i,2])\n  cole_name<-c(cole_name,rownames(me)[i])\n  }\n}\ncol2<-data.frame(cole)\ncol2<-cbind(col2,colem)\nrownames(col2)<-cole_name\n\n# This conditional is becausue when applying the na.omit, there is a error\n\nif (dim(col2)[1]!=0){\n  colnames(col2)<-c(\"% of NA´s\",\"Column number\")\n} else {col2<-\"There are no columns with missing values\"}\ncol2\n}\n\nna_perc<-sumna(data2,.5)\nhead(na_perc)\n#>                             % of NA´s Column number\n#> mths_since_last_record      0.7448276            27\n#> next_pymnt_d                1.0000000            45\n#> mths_since_last_major_derog 0.6275862            50\n#> annual_inc_joint            0.9931034            53\n#> dti_joint                   0.9931034            54\n#> mths_since_recent_bc_dlq    0.7103448            86\ndata3<-data2[,-na_perc[,2]]\ndata3_1<-na.omit(data2)\nhead(data3_1)\n#>   [1] loan_amnt                                 \n#>   [2] funded_amnt                               \n#>   [3] funded_amnt_inv                           \n#>   [4] term                                      \n#>   [5] int_rate                                  \n#>   [6] installment                               \n#>   [7] grade                                     \n#>   [8] sub_grade                                 \n#>   [9] emp_title                                 \n#>  [10] emp_length                                \n#>  [11] home_ownership                            \n#>  [12] annual_inc                                \n#>  [13] verification_status                       \n#>  [14] issue_d                                   \n#>  [15] loan_status                               \n#>  [16] purpose                                   \n#>  [17] title                                     \n#>  [18] zip_code                                  \n#>  [19] addr_state                                \n#>  [20] dti                                       \n#>  [21] delinq_2yrs                               \n#>  [22] earliest_cr_line                          \n#>  [23] fico_range_low                            \n#>  [24] fico_range_high                           \n#>  [25] inq_last_6mths                            \n#>  [26] mths_since_last_delinq                    \n#>  [27] mths_since_last_record                    \n#>  [28] open_acc                                  \n#>  [29] pub_rec                                   \n#>  [30] revol_bal                                 \n#>  [31] revol_util                                \n#>  [32] total_acc                                 \n#>  [33] initial_list_status                       \n#>  [34] out_prncp                                 \n#>  [35] out_prncp_inv                             \n#>  [36] total_pymnt                               \n#>  [37] total_pymnt_inv                           \n#>  [38] total_rec_prncp                           \n#>  [39] total_rec_int                             \n#>  [40] total_rec_late_fee                        \n#>  [41] recoveries                                \n#>  [42] collection_recovery_fee                   \n#>  [43] last_pymnt_d                              \n#>  [44] last_pymnt_amnt                           \n#>  [45] next_pymnt_d                              \n#>  [46] last_credit_pull_d                        \n#>  [47] last_fico_range_high                      \n#>  [48] last_fico_range_low                       \n#>  [49] collections_12_mths_ex_med                \n#>  [50] mths_since_last_major_derog               \n#>  [51] policy_code                               \n#>  [52] application_type                          \n#>  [53] annual_inc_joint                          \n#>  [54] dti_joint                                 \n#>  [55] verification_status_joint                 \n#>  [56] acc_now_delinq                            \n#>  [57] tot_coll_amt                              \n#>  [58] tot_cur_bal                               \n#>  [59] open_acc_6m                               \n#>  [60] open_act_il                               \n#>  [61] open_il_12m                               \n#>  [62] open_il_24m                               \n#>  [63] mths_since_rcnt_il                        \n#>  [64] total_bal_il                              \n#>  [65] il_util                                   \n#>  [66] open_rv_12m                               \n#>  [67] open_rv_24m                               \n#>  [68] max_bal_bc                                \n#>  [69] all_util                                  \n#>  [70] total_rev_hi_lim                          \n#>  [71] inq_fi                                    \n#>  [72] total_cu_tl                               \n#>  [73] inq_last_12m                              \n#>  [74] acc_open_past_24mths                      \n#>  [75] avg_cur_bal                               \n#>  [76] bc_open_to_buy                            \n#>  [77] bc_util                                   \n#>  [78] chargeoff_within_12_mths                  \n#>  [79] delinq_amnt                               \n#>  [80] mo_sin_old_il_acct                        \n#>  [81] mo_sin_old_rev_tl_op                      \n#>  [82] mo_sin_rcnt_rev_tl_op                     \n#>  [83] mo_sin_rcnt_tl                            \n#>  [84] mort_acc                                  \n#>  [85] mths_since_recent_bc                      \n#>  [86] mths_since_recent_bc_dlq                  \n#>  [87] mths_since_recent_inq                     \n#>  [88] mths_since_recent_revol_delinq            \n#>  [89] num_accts_ever_120_pd                     \n#>  [90] num_actv_bc_tl                            \n#>  [91] num_actv_rev_tl                           \n#>  [92] num_bc_sats                               \n#>  [93] num_bc_tl                                 \n#>  [94] num_il_tl                                 \n#>  [95] num_op_rev_tl                             \n#>  [96] num_rev_accts                             \n#>  [97] num_rev_tl_bal_gt_0                       \n#>  [98] num_sats                                  \n#>  [99] num_tl_120dpd_2m                          \n#> [100] num_tl_30dpd                              \n#> [101] num_tl_90g_dpd_24m                        \n#> [102] num_tl_op_past_12m                        \n#> [103] pct_tl_nvr_dlq                            \n#> [104] percent_bc_gt_75                          \n#> [105] pub_rec_bankruptcies                      \n#> [106] tax_liens                                 \n#> [107] tot_hi_cred_lim                           \n#> [108] total_bal_ex_mort                         \n#> [109] total_bc_limit                            \n#> [110] total_il_high_credit_limit                \n#> [111] revol_bal_joint                           \n#> [112] sec_app_fico_range_low                    \n#> [113] sec_app_fico_range_high                   \n#> [114] sec_app_earliest_cr_line                  \n#> [115] sec_app_inq_last_6mths                    \n#> [116] sec_app_mort_acc                          \n#> [117] sec_app_open_acc                          \n#> [118] sec_app_revol_util                        \n#> [119] sec_app_open_act_il                       \n#> [120] sec_app_num_rev_accts                     \n#> [121] sec_app_chargeoff_within_12_mths          \n#> [122] sec_app_collections_12_mths_ex_med        \n#> [123] sec_app_mths_since_last_major_derog       \n#> [124] hardship_flag                             \n#> [125] hardship_type                             \n#> [126] hardship_reason                           \n#> [127] hardship_status                           \n#> [128] deferral_term                             \n#> [129] hardship_amount                           \n#> [130] hardship_start_date                       \n#> [131] hardship_end_date                         \n#> [132] payment_plan_start_date                   \n#> [133] hardship_length                           \n#> [134] hardship_dpd                              \n#> [135] hardship_loan_status                      \n#> [136] orig_projected_additional_accrued_interest\n#> [137] hardship_payoff_balance_amount            \n#> [138] hardship_last_payment_amount              \n#> [139] disbursement_method                       \n#> [140] debt_settlement_flag                      \n#> [141] debt_settlement_flag_date                 \n#> [142] settlement_status                         \n#> [143] settlement_date                           \n#> [144] settlement_amount                         \n#> [145] settlement_percentage                     \n#> [146] settlement_term                           \n#> <0 rows> (or 0-length row.names)\nrepnas<-function(data,metric){\ndim<-dim(data)\nfor (i in 1:dim[2]){data[,i][is.na(data[,i])]<-\n  if(metric==\"median\"){\n  median(data[,i],na.rm = TRUE)} else{\n     if(metric==\"mean\"){\n  mean(data[,i],na.rm = TRUE)}\n    \n  } \n}\ndata\n}\ndata3<-repnas(data3,\"median\")"},{"path":"clean.html","id":"zero--and-near-zero-variance-predictors","chapter":"2 Big data and data cleaning","heading":"2.3 Zero- and Near Zero-Variance Predictors","text":"wil use library (Kuhn 2019) section. Zero- Near Zero-Variance Predictors variables columns single unique value, winch refereed “zero-variance predictor”. Also, variables might unique values occur low frequencies. cases may cause troubles estimating econometric machine learning model.function nearZeroVar shows columns number Zero- Near Zero-Variance Predictors.understand better “nearZeroVar” function , lets estimate metrics settlement_date columns, first apply function “table”, gives frequency per category:124 rows label 1, 2 rows label 2 .“frequency ratio” frequency prevalent value second frequent value. near one well-behaved predictors large highly-unbalanced, “grade” column :estimate “frequency ratio” e apply “.max” function gives position frequency prevalent value:get frequent value:second frequent value beThen, “frequency ratio” :default, threshold 19 (95/5), terms object “nzv” show column “frequency ratio” higher 19.Also, nearZeroVar function shows “percent unique values”, number unique values divided total number rows data frame (times 100). approaches zero granularity data increases.percent unique number categories, case 124 column estimated applying first function “length”:number rows data frame, obtain applying fucntio “dim”:“percent unique values” :object “nzv” shows “frequency ratio” “percent unique values”, however, apply filter get columns “frequency ratio” “percent unique values” higher respective threshold apply “nearZeroVar” time whitout argument “saveMetrics= TRUE”:object nzv_2 shows position colums tresholds higher, create object excluding columns.","code":"\nlibrary(caret)\nnzv <- nearZeroVar(data3,saveMetrics= TRUE)\nhead(nzv)\n#>                 freqRatio percentUnique zeroVar   nzv\n#> loan_amnt        1.444444      49.65517   FALSE FALSE\n#> funded_amnt      1.444444      49.65517   FALSE FALSE\n#> funded_amnt_inv  1.444444      49.65517   FALSE FALSE\n#> term             1.265625       1.37931   FALSE FALSE\n#> int_rate         1.076923      19.31034   FALSE FALSE\n#> installment      1.500000      93.79310   FALSE FALSE\ntail(nzv)\n#>                           freqRatio percentUnique zeroVar   nzv\n#> hardship_loan_status      71.500000     1.3793103   FALSE  TRUE\n#> disbursement_method        0.000000     0.6896552    TRUE  TRUE\n#> debt_settlement_flag       5.590909     1.3793103   FALSE FALSE\n#> debt_settlement_flag_date 24.800000     8.2758621   FALSE  TRUE\n#> settlement_status         11.818182     2.0689655   FALSE FALSE\n#> settlement_date           41.333333    10.3448276   FALSE FALSE\nt<-table(data3[,col])\nt\n#> \n#>   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15 \n#> 124   2   2   2   3   1   1   1   1   2   1   1   1   2   1\nw <- which.max(t)\nw\n#> 1 \n#> 1\nt[w]\n#>   1 \n#> 124\nmax(t[-w])\n#> [1] 3\nt[w]/max(t[-w])\n#>        1 \n#> 41.33333\nlength(table(data3[,col])) \n#> [1] 15\ndim(data3)[1]\n#> [1] 145\n(length(table(data3[,col]))/dim(data3)[1])*100\n#> [1] 10.34483\nnzv_2 <- nearZeroVar(data3)\nnzv_2 \n#>  [1]  14  15  32  33  34  47  48  49  50  51  73  74  92  93 104 105 106 107 108\n#> [20] 109 110 111 112 114\ndata4<-data3[,-nzv_2]"},{"path":"clean.html","id":"collinearity","chapter":"2 Big data and data cleaning","heading":"2.4 Collinearity","text":"Collinearity situation two variables closely related one another. presence collinearity can pose problems model esimatio, regression, becasue difficult separate individual effects collinear variables response (James Tibshirani 2017).open_acc, revol_bal,total_rev_hi_limtotal_acc","code":"#>  [1] \"total_rev_hi_lim\"           \"total_acc\"                 \n#>  [3] \"open_acc\"                   \"num_sats\"                  \n#>  [5] \"num_op_rev_tl\"              \"total_bal_ex_mort\"         \n#>  [7] \"total_bc_limit\"             \"num_rev_accts\"             \n#>  [9] \"loan_amnt\"                  \"funded_amnt\"               \n#> [11] \"funded_amnt_inv\"            \"num_bc_sats\"               \n#> [13] \"tot_hi_cred_lim\"            \"acc_open_past_24mths\"      \n#> [15] \"num_actv_rev_tl\"            \"tot_cur_bal\"               \n#> [17] \"num_tl_op_past_12m\"         \"total_pymnt\"               \n#> [19] \"total_pymnt_inv\"            \"total_il_high_credit_limit\"\n#> [21] \"recoveries\"                 \"bc_util\"                   \n#> [23] \"fico_range_low\"             \"debt_settlement_flag\"      \n#> [25] \"purpose\"\ncw<-c(\"open_acc\", \"total_rev_hi_lim\",\"total_acc\",\"num_sats\",\"total_bc_limit\",\"num_rev_accts\")\ndescrCor <-  data.frame(cor(data4[,cw]))\ndescrCor\n#>                   open_acc total_rev_hi_lim total_acc  num_sats total_bc_limit\n#> open_acc         1.0000000        0.5522515 0.7360423 0.9997421      0.3786225\n#> total_rev_hi_lim 0.5522515        1.0000000 0.4929105 0.5514165      0.8771811\n#> total_acc        0.7360423        0.4929105 1.0000000 0.7372437      0.3232249\n#> num_sats         0.9997421        0.5514165 0.7372437 1.0000000      0.3777843\n#> total_bc_limit   0.3786225        0.8771811 0.3232249 0.3777843      1.0000000\n#> num_rev_accts    0.6645154        0.5581800 0.8815264 0.6657604      0.3771416\n#>                  num_rev_accts\n#> open_acc             0.6645154\n#> total_rev_hi_lim     0.5581800\n#> total_acc            0.8815264\n#> num_sats             0.6657604\n#> total_bc_limit       0.3771416\n#> num_rev_accts        1.0000000\npanel.cor <- function(x, y, digits = 2, prefix = \"\", cex.cor, ...)\n{\n    usr <- par(\"usr\"); on.exit(par(usr))\n    par(usr = c(0, 1, 0, 1))\n    r <- abs(cor(x, y,use=\"pairwise.complete.obs\"))\n    txt <- format(c(r, 0.123456789), digits = digits)[1]\n    txt <- paste0(prefix, txt)\n    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)\n    text(0.5, 0.5, txt, cex = cex.cor * r)\n}\npairs(data4[,cw],upper.panel=panel.cor,na.action = na.omit)\ndescrCor <-  cor(data4)\nhighlyCorDescr <- findCorrelation(descrCor, cutoff = .75,names=T)\nhighlyCorDescr\n#>  [1] \"total_rev_hi_lim\"           \"total_acc\"                 \n#>  [3] \"open_acc\"                   \"num_sats\"                  \n#>  [5] \"num_op_rev_tl\"              \"total_bal_ex_mort\"         \n#>  [7] \"total_bc_limit\"             \"num_rev_accts\"             \n#>  [9] \"loan_amnt\"                  \"funded_amnt\"               \n#> [11] \"funded_amnt_inv\"            \"num_bc_sats\"               \n#> [13] \"tot_hi_cred_lim\"            \"acc_open_past_24mths\"      \n#> [15] \"num_actv_rev_tl\"            \"tot_cur_bal\"               \n#> [17] \"num_tl_op_past_12m\"         \"total_pymnt\"               \n#> [19] \"total_pymnt_inv\"            \"total_il_high_credit_limit\"\n#> [21] \"open_il_12m\"                \"recoveries\"                \n#> [23] \"bc_util\"                    \"fico_range_low\"            \n#> [25] \"debt_settlement_flag\"       \"zip_code\"                  \n#> [27] \"purpose\"                    \"tax_liens\"\nhighCorr <- sum(abs(descrCor[upper.tri(descrCor)]) > .999)\nhighCorr \n#> [1] 6"},{"path":"graphs.html","id":"graphs","chapter":"3 APIS and R graphs","heading":"3 APIS and R graphs","text":"","code":""},{"path":"graphs.html","id":"apis-apis-application-programming-interface","chapter":"3 APIS and R graphs","heading":"3.1 1. APIs API´s (Application Programming Interface)","text":"","code":""},{"path":"graphs.html","id":"quantmod-api","chapter":"3 APIS and R graphs","heading":"3.1.1 1.1 Quantmod API","text":"Quantitative Financial Modelling FrameworkThe quantmod package R designed assist quantitative trader development, testing, deployment statistically based trading models.function getSymbols wrapper load data various sources, local remote. One popular adn default yahoo fiannce, https://finance.yahoo.com/,\ngetSymbols(“Symbol”):can see, object class xts.eliminate warnings, add argument warnings = FGetting data specific date:getSymbols(“symbol”, =“YY/m/d”,=“YY/m/d”): YY= 4 digit year, m= 2 digit month, d= 2 digit day.previous way download data store information object environment, default assign name object, previous example “AMZN”.Another way store different name, form example name “data”, following:data <- new.env()\ngetSymbols(“ticker”,env=data)object class list. get information apply following code.previous method useful tickers like Bitcoin:apparently name object environment “BTC-USD”, want modify name, show debug like: “object ‘BTC’ found”even looks “BTC-USD” name kind brackets:Anhother alternative, wich helpful apply loops get onformation environment :like download intraday data, use Alphavantagehttps://www.alphavantage.co/getSymbols(“ticker”, src=“av”, api.key=“yourKey”, output.size=“full”,\nperiodicity=“intraday”)Dividends\ngetDividends(“ticker”), xts objetc ticker needs R environment","code":"\nlibrary(quantmod)\n\ngetSymbols(\"AAPL\")\n#> [1] \"AAPL\"\nclass(AAPL)\n#> [1] \"xts\" \"zoo\"\ngetSymbols(\"AMZN\", from=\"2020/04/01\",to=\"2022/04/04\")\n#> [1] \"AMZN\"\napple2 <- new.env()\ngetSymbols(\"AAPL\", env=apple2)\n#> [1] \"AAPL\"\nclass(apple2)\n#> [1] \"environment\"\napple3<-apple2[[\"AAPL\"]]\ngetSymbols(\"BTC-USD\")\n#> [1] \"BTC-USD\"\n# bit<-BTC-USD[,4]\n# `BTC-USD`\nbit<-get(\"BTC-USD\")\nhead(bit)\n#>            BTC-USD.Open BTC-USD.High BTC-USD.Low BTC-USD.Close BTC-USD.Volume\n#> 2014-09-17      465.864      468.174     452.422       457.334       21056800\n#> 2014-09-18      456.860      456.860     413.104       424.440       34483200\n#> 2014-09-19      424.103      427.835     384.532       394.796       37919700\n#> 2014-09-20      394.673      423.296     389.883       408.904       36863600\n#> 2014-09-21      408.085      412.426     393.181       398.821       26580100\n#> 2014-09-22      399.100      406.916     397.130       402.152       24127600\n#>            BTC-USD.Adjusted\n#> 2014-09-17          457.334\n#> 2014-09-18          424.440\n#> 2014-09-19          394.796\n#> 2014-09-20          408.904\n#> 2014-09-21          398.821\n#> 2014-09-22          402.152\ngetSymbols(\"AAPL\", src=\"av\", api.key=\"yourKey\", output.size=\"full\",\nperiodicity=\"intraday\",interval=\"5min\")\n#> [1] \"AAPL\"\n# Al parecer solo si ticker tiene iformación de divididendos\nap_div<-getDividends(\"AAPL\")"},{"path":"graphs.html","id":"in-house-api","chapter":"3 APIS and R graphs","heading":"3.1.2 1.2 In-house Api","text":"(covered final exam activities, exposition)Harvesting web rvest (get tickers)code read characters web pages. case, get tickers yahoo finance.","code":"\nlibrary(xml2)\nlibrary(rvest)\n# the page of of the criptocurrencies\n#yf <- \"https://finance.yahoo.com/cryptocurrencies/\"\n\n# for the IPC components\nyf <- \"https://finance.yahoo.com/quote/%5EMXX/components?p=%5EMXX\"\nhtml <- read_html(yf)\n# To get the node a, wich contains characters \nnode <- html_nodes(html,\"a\")\n\n# To read the text in the node\nnode<-html_text(node, trim=TRUE)\n\n# To get the elements that have USD (the tickers). For the IPC tickers, replace \"USD\" with \".MX\". For other tickers, print the node object and look for patterns or select by rows.\n#tickers<-grep(pattern = \"USD\", x = node, value = TRUE)\n\ntickers<-grep(pattern = \".MX\", x = node, value = TRUE)\n\n# to get only the first 5 tickers\n\ntickers1<-tickers\ntickers1\n#>  [1] \"IENOVA.MX\"     \"ALSEA.MX\"      \"MEGACPO.MX\"    \"ASURB.MX\"     \n#>  [5] \"ALPEKA.MX\"     \"FEMSAUBD.MX\"   \"LABB.MX\"       \"GCARSOA1.MX\"  \n#>  [9] \"BIMBOA.MX\"     \"AMXL.MX\"       \"GCC.MX\"        \"GMEXICOB.MX\"  \n#> [13] \"CEMEXCPO.MX\"   \"BOLSAA.MX\"     \"PINFRA.MX\"     \"CUERVO.MX\"    \n#> [17] \"TLEVISACPO.MX\" \"GENTERA.MX\"    \"LIVEPOLC1.MX\"  \"MEXCHEM.MX\"   \n#> [21] \"GAPB.MX\"       \"GRUMAB.MX\"     \"BBAJIOO.MX\"    \"AC.MX\"        \n#> [25] \"GFNORTEO.MX\"   \"KOFL.MX\"       \"KIMBERA.MX\"    \"PEOLES.MX\"    \n#> [29] \"OMAB.MX\"       \"SITESB1.MX\"\ngetSymbols(tickers1[1:2]) \n#> [1] \"IENOVA.MX\" \"ALSEA.MX\""},{"path":"graphs.html","id":"nasdaq-data-link-api","chapter":"3 APIS and R graphs","heading":"3.1.3 1.3 Nasdaq Data Link API","text":"need create account (suggest free academic account)Nasdaq Data Link API (eas Quandl) https://data.nasdaq.com/class objects Quandl Data Frames, e add argument: type=“xts”, xts object get.Emerging Markets High Grade Corporate Bond Index Yield","code":"\nlibrary(Quandl)\n# por default como data frame, pero podemos darle unargumento para que sea xts la descarga\neurex<-Quandl(\"ML/EMHGY\", api_key=\"e7Z7FD5siZ8mDi_yTVLD\",type=\"xts\")\n# api_key \nhead(eurex)\n#>            [,1]\n#> 1998-12-31 8.48\n#> 1999-01-04 8.48\n#> 1999-01-05 8.46\n#> 1999-01-06 8.37\n#> 1999-01-07 8.43\n#> 1999-01-08 8.40"},{"path":"graphs.html","id":"graphs-plots","chapter":"3 APIS and R graphs","heading":"3.2 3. Graphs (Plots)","text":"basic function ploting plot(x, type = “h”, col = “red”, lwd = 10, “xlab”,“ylab”).\nhelp plot.xy plot.default can see arguments.\nPlot HSI close priceNote: applying function plot, add another line, must data frame, otherwise may appear plot.Adding leggendThe function legends work data frames, xts., transform dataframe\n.data.frame(object).add legend, works data frames objects, xts. need transform data frame.legend(x= “topleft”, legend = c(“objst”,“objt2”),lty = 1,col=c(“green”,“red”))details legends look http://www.sthda.com/english, legend function arguments.","code":"\n#tickers<-c(\"AAPL\",\"AMZN\")\n# vector de caracteres\ngetSymbols(\"AAPL\")\n#> [1] \"AAPL\"\n#> [1] \"AAPL\" \"AMZN\"\napp<-AAPL[1:100,4]\n#am<-AMZN[1:110,4]\n#both<-cbind(app,am) \nplot(app,type=\"l\",col=\"green\", main = \"APPL\")\n\n# to add anhother time series, for example: apple*1.1\n\nlines(app[,1]*1.1,col=\"red\")\nappdf<-as.data.frame(app)\napp_1df<-as.data.frame(app[,1]*1.1)\n\nplot(appdf[,1],type=\"l\",col=\"green\", main = \"APPL\") \nlines(app_1df[,1],col=\"red\") \nlegend(x= \"topleft\", legend = c(\"app\",\"app*1.1\"),lty = 1,lwd=2,col=c(\"green\",\"red\"))"},{"path":"logit.html","id":"logit","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4 Machine learning with market direction prediction: Logit","text":"chapter covers machine learning market direction prediction. particular, forecast market moves either upward downward.logistic regression (Logit) Linear Discriminant Analysis (LDA) models help us fit model using binary behavior () forecast market direction. Logistic regression.","code":""},{"path":"logit.html","id":"data-preparation","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4.1 Data preparation","text":"following commands create variable direction either direction (1) direction (0). words, direction signal o buying signal buying. example, direction variable created short SMA greater long SMA zero otherwise.First make plot.Now create signal.machine learning example, predict BNB price, model :\\[ signal_{t}=\\alpha\\ +\\beta1\\ macd_{t-1}+\\beta2\\ rsi_{t-2} +\\beta3\\ bb +\\ e \\]difference independent variable, case signal.separate sample training testing. training data set used building model process, testing dataset used evaluation purposes.","code":"\nlibrary(\"quantmod\")\nticker<-\"BNB-USD\"\ndata<-getSymbols(ticker,from=\"2021-08-01\",to=\"2022-04-18\",warnings =FALSE,auto.assign=FALSE)\ndata<-data[,4]\ncolnames(data)<-\"bnb\"\nlag2<-12\nlag3<-9\nlag4<-26\navg<-SMA(data[,1],lag2) # var1\navg2<-SMA(data[,1],lag4) # var1\ndata2<-cbind(data,avg,avg2)\ndata2<-na.omit(data2)\n#par(mfrow=c(2,1))\nplo0<-as.data.frame(data2[,1])\nplo<-as.data.frame(data2[,2])\nplo1<-as.data.frame(data2[,3])\n\nplot(plo0[,1],col=\"blue\",type=\"l\")\nlines(plo[,1],col=\"red\",type=\"l\")\nlines(plo1[,1],col=\"green\",type=\"l\")\nlegend(x= \"topleft\", legend = c(\"actual\",\"short-sma\",\"long-sma\"),lty = 1,lwd=2,col=c(\"blue\",\"red\",\"green\"))\nsignal<-ifelse(data2[,\"SMA\"]>data2[,\"SMA.1\"],1,0)\nplot(signal)\nstd<- rollapply(data[,1],lag2,sd) # var2\ncolnames(std)<-\"std\"\n\nmacd<- MACD(data[,1], lag2,lag3,lag4, \"SMA\") # var2\nmacd2<- MACD(data[,1], 11,25,8, \"SMA\") \ncolnames(macd)[2]<-\"macd_signal\"\n\n\nrsi<-  RSI(data[,1],lag2,\"SMA\")# var3\nrsi2<-  RSI(data[,1],13,\"SMA\")# var3\n\nbb <- BBands(data2[,1], n = 10, maType=\"SMA\", sd = 2) \n\n# Agregar el nombre de signal en lugar de sig\ndata2<-cbind(signal,std,macd,rsi,bb)\ncolnames(data2)[1]<-\"signal\"\ndata2<-na.omit(data2)\nN<-dim(data2)[1]\nn_train<-round(N*.8,0)\npart<-index(data2)[n_train]\n#This is the test data set.\ntrain<-subset(data2,\n  +index(data2)>=index(data2)[1] &\n  +index(data2)<=part)\n\n# The subset of the training data set.\ntest<-subset(data2,\n  +index(data2)>=part+1 &\n  +index(data2)<=\"2022-04-18\")\ny1<-test[,1] "},{"path":"logit.html","id":"logistic-regression","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4.2 Logistic Regression","text":"linear regression assumes response variable Y quantitative. many situations, response variable instead qualitative. example, eye color qualitative, taking values blue, brown, green. Often qualitative variables referred categorical ; use terms interchangeably.binary response model, interest lies primarily response probability. However, can use OLS estimate model, linear binary response model. apply Logistic regression.glm(y ~.,data= ,family=binomial())expect forecast 0,1 result, signal. transform probabilistic model.\nexp(x)/(1+exp(x))Even result probability, 0 1, require result 0,1. transform , creating binary variable, takes value 1 probability higher 0.5, zero lower 0.5.","code":"\nmodel<-glm(signal~.,data= train,family=binomial())\npred<-predict(model,test)\npred\n#> 2022-03-05 2022-03-06 2022-03-07 2022-03-08 2022-03-09 2022-03-10 2022-03-11 \n#>  1.4071871  0.7838451  1.1688270  1.5835724  0.6400374  1.1018749  1.5094064 \n#> 2022-03-12 2022-03-13 2022-03-14 2022-03-15 2022-03-16 2022-03-17 2022-03-18 \n#> -1.2026024 -2.0134675 -2.4429877 -1.9810255 -0.7260440 -1.5145946 -0.3496281 \n#> 2022-03-19 2022-03-20 2022-03-21 2022-03-22 2022-03-23 2022-03-24 2022-03-25 \n#> -0.7131193 -0.6484040 -1.3564305  1.4355937  1.6560035  2.0060744  3.8742966 \n#> 2022-03-26 2022-03-27 2022-03-28 2022-03-29 2022-03-30 2022-03-31 2022-04-01 \n#>  3.5987035  3.9839923  4.1817522  4.1877168  3.9090570  2.9184529  3.7917209 \n#> 2022-04-02 2022-04-03 2022-04-04 2022-04-05 2022-04-06 2022-04-07 2022-04-08 \n#>  3.4630973  2.7885380  2.6969620  2.6106934  4.1880525  3.2083981  3.0614675 \n#> 2022-04-09 2022-04-10 2022-04-11 2022-04-12 2022-04-13 2022-04-14 2022-04-15 \n#>  2.6747753  2.6183627  1.9073456  2.0025636  0.6854552  1.0431508 -0.2146935 \n#> 2022-04-16 2022-04-17 2022-04-18 \n#> -0.1493334  0.3593882  0.6847538\nprob<-exp(pred)/(1+exp(pred))\npredf<-ifelse(prob>.5,1,0)\nplot(predf)\n# comparar vs el dato real que estaé en y1"},{"path":"logit.html","id":"confusion-matrix","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4.3 Confusion matrix","text":"measure accuracy prediction, categorical variables, 0,1, confusion matrix table indicates possible categories predicted values, actual values.True Positive (TP): Correctly classified class interest. True Negative (TN) Correctly classified class interest. False Positive (FP) Incorrectly classified class interest. False Negative (FN): Incorrectly classified class interest.confusion matrix, one mesures interest accuracy, defined :\\[ accuracy =\\frac{TP+TN}{TP+TN+FP+FN}\\]formula, terms TP, TN, FP, FN refer number times model’s predictions fell categories. accuracy therefore proportion represents number true positives true negatives, divided total number predictions.factor(x,levels=c(1,0))\nconfusionMatrix(pred,real)SensitivityFinding useful classifier often involves balance predictions overly conservative overly aggressive. example, e-mail filter guarantee eliminate every spam message aggressively eliminating nearly every ham message time. hand, guaranteeing ham message inadvertently filtered might require us allow unacceptable amount spam pass filter. pair performance measures captures trade : sensitivity specificity.sensitivity model (also called true positive rate) measures proportion positive examples correctly classified. Therefore, shown following formula, calculated number true positives divided total number positives, correctly classified (true positives) well incorrectly classified (false negatives):\\[sensitivity =\\frac{TP}{TP+FN}\\]","code":"\nlibrary(caret)\npredf2<-as.data.frame(predf)\npredf3<-factor(predf2[,1],levels=c(1,0))   \nreal<-factor(y1,levels=c(1,0))\nconfusionMatrix(predf3,real) \n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction  1  0\n#>          1 25  8\n#>          0  0 12\n#>                                         \n#>                Accuracy : 0.8222        \n#>                  95% CI : (0.6795, 0.92)\n#>     No Information Rate : 0.5556        \n#>     P-Value [Acc > NIR] : 0.0001572     \n#>                                         \n#>                   Kappa : 0.625         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.0133283     \n#>                                         \n#>             Sensitivity : 1.0000        \n#>             Specificity : 0.6000        \n#>          Pos Pred Value : 0.7576        \n#>          Neg Pred Value : 1.0000        \n#>              Prevalence : 0.5556        \n#>          Detection Rate : 0.5556        \n#>    Detection Prevalence : 0.7333        \n#>       Balanced Accuracy : 0.8000        \n#>                                         \n#>        'Positive' Class : 1             \n#> "},{"path":"logit.html","id":"linear-discriminant-analysis-lda","chapter":"4 Machine learning with market direction prediction: Logit","heading":"4.4 Linear Discriminant Analysis LDA","text":"need another method, logistic regression?\nseveral reasons:• classes well-separated, parameter estimates \nlogistic regression model surprisingly unstable. Linear discriminant\nanalysis suffer problem.• number observations n small distribution predictors X approximately normal classes, linear discriminant model stable logistic regression model.• Finally, 2 categories, example, c(-1,0,1).Signal creation, now 3 categories c(-1,0,1)1 es señal de compra, -1 de venta (o venta en corto), y cero es hacer nada (ni comprar ni vender).Combinig data2 signalTraining test partitionLDA model prediction\nlda(x~.,data= , prior = c(1,1,1)/3)","code":"\nlibrary(\"quantmod\")\nticker<-\"BNB-USD\"\ndata<-getSymbols(ticker,from=\"2018-08-01\",to=\"2022-04-18\",warnings =FALSE,auto.assign=FALSE)\ndata<-data[,4]\ncolnames(data)<-\"bnb\"\nlag2<-12\nlag3<-18\nlag4<-26\navg<-SMA(data[,1],lag2) # var1\navg2<-SMA(data[,1],lag3) # var1\navg3<-SMA(data[,1],lag4) # var1\ndata2<-cbind(data,avg,avg2,avg3)\ndata2<-na.omit(data2)\n\nstd<- rollapply(data[,1],lag2,sd) # var2\ncolnames(std)<-\"std\"\nmacd<- MACD(data[,1], lag2,lag3,lag4, \"SMA\") # var2\ncolnames(macd)[2]<-\"macd_signal\"\nrsi<-  RSI(data[,1],lag2,\"SMA\")# var3\nbb <- BBands(data2[,1], n = 10, maType=\"SMA\", sd = 2) \n\ndata2<-cbind(data,std,macd,rsi,bb)\n#colnames(data2)[1]<-\"signal\"\ndata2<-na.omit(data2)\nsignal <- ifelse(data2[,1]> data2[,'up'] & data2[,'macd']> data2[,'macd_signal'],1,ifelse(data2[,1]< data2[,'dn'] & data2[,'macd'] <data2[,'macd_signal'],-1,0))\nplot(signal)\n#We first replace bnp by signal \ndata2<-data2[,-1]\n\n# Eliminate up, because is causing issues (correlated with mavg, and does not allow estimate the model)\ndata2<-data2[,-6]\ndata2<-cbind(signal,data2)\ncolnames(data2)[1]<-\"signal\"\nN<-dim(data2)[1]\nn_train<-round(N*.8,0)\npart<-index(data2)[n_train]\n#This is the test data set.\ntrain<-subset(data2,\n  +index(data2)>=index(data2)[1] &\n  +index(data2)<=part)\n\n# The subset of the training data set.\ntest<-subset(data2,\n  +index(data2)>=part+1 &\n  +index(data2)<=\"2022-04-18\")\n\ntrain<-train[,-6]\ntest<-test[,-6]\n\ny1<-test[,1] # contiene la varaible que voy a pronosticar\n#test<-test[,-1] # las variables independientes, que voy a usar para haver mi pronóstico\nlibrary(MASS)\nmodellda<-lda(signal~.,data= train, prior = c(1,1,1)/3)\npred<-predict(modellda,test)\npred<-pred[[\"class\"]]\nclass(pred)\n#> [1] \"factor\"\n\nlibrary(caret) \nreal<-factor(y1,levels=c(-1,0,1))\nconfusionMatrix(pred,real) \n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction  -1   0   1\n#>         -1   8  18   0\n#>         0    0 199   0\n#>         1    0  27  11\n#> \n#> Overall Statistics\n#>                                           \n#>                Accuracy : 0.8289          \n#>                  95% CI : (0.7778, 0.8724)\n#>     No Information Rate : 0.9278          \n#>     P-Value [Acc > NIR] : 1               \n#>                                           \n#>                   Kappa : 0.4079          \n#>                                           \n#>  Mcnemar's Test P-Value : NA              \n#> \n#> Statistics by Class:\n#> \n#>                      Class: -1 Class: 0 Class: 1\n#> Sensitivity            1.00000   0.8156  1.00000\n#> Specificity            0.92941   1.0000  0.89286\n#> Pos Pred Value         0.30769   1.0000  0.28947\n#> Neg Pred Value         1.00000   0.2969  1.00000\n#> Prevalence             0.03042   0.9278  0.04183\n#> Detection Rate         0.03042   0.7567  0.04183\n#> Detection Prevalence   0.09886   0.7567  0.14449\n#> Balanced Accuracy      0.96471   0.9078  0.94643"},{"path":"big-data-and-machine-learning.html","id":"big-data-and-machine-learning","chapter":"5 Big data and machine learning","heading":"5 Big data and machine learning","text":"Predicting market direction price quite challenging task market data involves lots noise. market moves either upward downward, nature market movement binary (Jeet Vat, 2017).chapter, use OLS, predict 1 day open price, price 19 April 2022, cryptocurency, case Bianance, “BNB-USD”. ideas code adapted Jeet Vat (2017).Binance Launched July 2017, Binance biggest cryptocurrency exchange globally based daily trading volume. Binance aims bring cryptocurrency exchanges forefront financial activity globally. idea behind Binance’s name show new paradigm global finance — Binary Finance, Binance.","code":""},{"path":"big-data-and-machine-learning.html","id":"data-preparation-1","chapter":"5 Big data and machine learning","heading":"5.1 Data preparation","text":"going start OLS (Ordinary least square) model. independent variables using lags independent variable.simplicity, suppose predict 19 April 2022 BNB price, make regression, OLS, following model:\\[bnb_{t}=\\alpha\\ +\\beta1\\ bnb_{t-1}+\\beta2\\ bnb_{t-2} + e \\]\\(\\alpha\\) intercept, \\(beta\\) parameters estimated, \\(bnb_{t-1}\\) bnb price traiding previouse day, case 18 April 2022, \\(bnb_{t-2}\\) bnb price day , 17 April 2022, “e” error term regression. words, price today explained price yesterday day yesterday.data<-stats::lag(y,lag)Para realizar la regresión por OLS\nlm(bnb~.,data=data)Also, suppose found following result regression:\\[bnb_{t}=\\ 52.42583\\ +\\ 0.88235\\ bnb_{t-1}\\ -0.01226\\ bnb_{t-2} \\]case, forecast 19 April 2022 :Given last price 406.3009.However, last example exposition purposes. reality, need test independent variables besides lags dependent variable. session, besides lags values close prices dependent variable, going add variables used technical analysis, moving average, standard deviation, RSI (see appendix detailed explanation), MACD, , predictive power market direction. indicators can constructed using following commands:","code":"\nlibrary(\"quantmod\")\nticker<-\"LALAB.MX\"\ny<-getSymbols(ticker,from=\"2021-01-01\",to=\"2022-04-18\",warnings =FALSE,auto.assign=FALSE)\n\ny<-y[,1]\ncolnames(y)<-\"bnb\"\nhead(y)\n#>              bnb\n#> 2021-01-04 15.54\n#> 2021-01-05 15.23\n#> 2021-01-06 15.75\n#> 2021-01-07 15.81\n#> 2021-01-08 15.90\n#> 2021-01-11 15.64\nlag<-1\nlag1<-2\ndata<-stats::lag(y,lag)\ndata2<-stats::lag(y,lag1)\ndata<-cbind(y,data,data2)\ncolnames(data)[2:3]<-c(\"bnb_1\",\"bnb_2\")\nmodel<-lm(bnb~.,data=data)\nsummary(model)\n#> \n#> Call:\n#> lm(formula = bnb ~ ., data = data)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.25123 -0.13298  0.00724  0.08428  1.92931 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.40151    0.20665   1.943   0.0529 .  \n#> bnb_1        0.88236    0.05571  15.837   <2e-16 ***\n#> bnb_2        0.09303    0.05572   1.670   0.0960 .  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.2664 on 319 degrees of freedom\n#>   (2 observations deleted due to missingness)\n#> Multiple R-squared:  0.9486, Adjusted R-squared:  0.9482 \n#> F-statistic:  2941 on 2 and 319 DF,  p-value: < 2.2e-16\ntail(y)\n#>              bnb\n#> 2022-04-06 17.25\n#> 2022-04-07 17.03\n#> 2022-04-08 17.20\n#> 2022-04-11 17.00\n#> 2022-04-12 16.80\n#> 2022-04-13 16.50\nbnb_t1<-406.3009 # price 18 April 2022\nbnb_t2<-417.4115 # price 17 April 2022\n\n# The prediction manually would be:\n47.13630+0.90113*bnb_t1-0.01765*bnb_t1\n#> [1] 406.095"},{"path":"big-data-and-machine-learning.html","id":"variable-creation","chapter":"5 Big data and machine learning","heading":"5.2 Variable creation","text":"SMA Calculate moving averages\nSMA(x, n = 10, …), x time serie, n Number periods average overSMA Calculate moving averages\nSMA(x, n = 10, …), x time serie, n Number periods average overThe rollapply function applying function rolling margins array, case used make moving standard deviation.\nrollapply(x,n,sd), sd standard deviationThe rollapply function applying function rolling margins array, case used make moving standard deviation.\nrollapply(x,n,sd), sd standard deviationThe MACD moving average converge diverge (see Appendix)MACD moving average converge diverge (see Appendix)MACD(x, nFast = 12, nSlow = 26, nSig = 9, maType=SMA EMA)MACD(x, nFast = 12, nSlow = 26, nSig = 9, maType=SMA EMA)RSI relative strength index\nRSI(x, n = 14, maType=SMA EMA)RSI relative strength index\nRSI(x, n = 14, maType=SMA EMA)model :\n\\[bnb_{t}=\\alpha\\ +\\beta1\\ bnb_{t-1}+\\beta2\\ bnb_{t-2} +\\beta3\\ sma +\\\\ \\beta4\\ std\\ +\\beta5\\ macd\\ + \\beta6\\ rsi +\\ e\\]see, lags new variables, many missing values early dates, apply na.omit, eliminate rows nas.","code":"\nlag2<-6\nlag3<-9\nlag4<-26\navg<-SMA(data[,1],lag2) # var1\nstd<- rollapply(data[,1],lag2,sd) # var2\ncolnames(std)<-\"std\"\n\nmacd<- MACD(data[,1], lag2,lag3,lag4, \"SMA\") # var2\ncolnames(macd)[2]<-\"macd_signal\"\n  \nrsi<-  RSI(data[,1],lag2,\"SMA\")# var3\n\n\ndata2<-cbind(data,avg,std,macd,rsi)\nhead(data2)\n#>              bnb bnb_1 bnb_2    SMA       std macd macd_signal rsi\n#> 2021-01-04 15.54    NA    NA     NA        NA   NA          NA  NA\n#> 2021-01-05 15.23 15.54    NA     NA        NA   NA          NA  NA\n#> 2021-01-06 15.75 15.23 15.54     NA        NA   NA          NA  NA\n#> 2021-01-07 15.81 15.75 15.23     NA        NA   NA          NA  NA\n#> 2021-01-08 15.90 15.81 15.75     NA        NA   NA          NA  NA\n#> 2021-01-11 15.64 15.90 15.81 15.645 0.2393951   NA          NA  NA\ndata2<-na.omit(data2)"},{"path":"big-data-and-machine-learning.html","id":"sub-samples","chapter":"5 Big data and machine learning","heading":"5.3 Sub samples","text":"separate sample training testing. training data set used building model process, testing dataset used evaluation purposes.code automate sub-sample creation, usually split 80% training set 20% test set.case, “2022-01-31” date represents 80% observations, starting date “2022-01-01”.use function subset:name<-subset(object,\n+index(object)>=“YY-mm-dd” &\n+index(object)<=“YY-mm-dd”)forecast vs real data, going takeout real data BNB prices test set, store object call y1.","code":"\nN<-dim(data2)[1]\nn_train<-round(N*.8,0)\npart<-index(data2)[n_train]\npart\n#> [1] \"2022-01-19\"\n\n#This is the test data set.\ntrain<-subset(data2,\n  +index(data2)>=index(data2)[1] &\n  +index(data2)<=part)\n\n# The subset of the training data set.\ntest<-subset(data2,\n  +index(data2)>=part+1 &\n  +index(data2)<=\"2022-04-18\")\ny1<-test[,1] \nhead(test)\n#>              bnb bnb_1 bnb_2      SMA       std      macd macd_signal       rsi\n#> 2022-01-20 16.79 16.80 16.91 17.19167 0.4656785 -1.077298  -0.1888210  3.816726\n#> 2022-01-21 17.36 16.79 16.80 17.12667 0.3938356 -1.109900  -0.2228045 38.036818\n#> 2022-01-24 16.03 17.36 16.79 16.83167 0.4478131 -1.976833  -0.2798782 19.587642\n#> 2022-01-25 16.17 16.03 17.36 16.67667 0.4948401 -1.715669  -0.3048719 30.212749\n#> 2022-01-26 16.03 16.17 16.03 16.53000 0.5401851 -1.470294  -0.3000546 30.869562\n#> 2022-01-27 16.20 16.03 16.17 16.43000 0.5357239 -1.017469  -0.2740690 37.288156"},{"path":"big-data-and-machine-learning.html","id":"making-the-model","chapter":"5 Big data and machine learning","heading":"5.4 Making the model","text":"estimate OLS model aplying function lm,lm(bnb~.,data=train)make prediction, need apply function predict, test set.first prediction, 2022-04-04, 433.6627, \\[ bnb_{t}=\\ -20.28047\\ +\\ 0.06146\\ bnb_{t-1}\\ -0.27979\\ bnb_{t-2} + \\\\1.15805\\ sma +\\ 0.33934206\\ std -3.76813\\ macd\\\\ -6.68031\\ macd\\_ signal\\ + 0.74661\\ rsi \\]\n.","code":"\nmodel1<-lm(bnb~.,data=train)\nsummary(model1)\n#> \n#> Call:\n#> lm(formula = bnb ~ ., data = train)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.69108 -0.09940  0.02329  0.10403  1.10734 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.0910104  0.1643070  -0.554   0.5802    \n#> bnb_1        0.3459223  0.0643449   5.376 1.90e-07 ***\n#> bnb_2       -0.2987632  0.0692119  -4.317 2.37e-05 ***\n#> SMA          0.9308127  0.0896208  10.386  < 2e-16 ***\n#> std          0.4042500  0.0749164   5.396 1.72e-07 ***\n#> macd        -0.0430484  0.0225301  -1.911   0.0573 .  \n#> macd_signal  0.0607983  0.0542453   1.121   0.2636    \n#> rsi          0.0075120  0.0007548   9.952  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.1813 on 225 degrees of freedom\n#> Multiple R-squared:  0.9802, Adjusted R-squared:  0.9796 \n#> F-statistic:  1594 on 7 and 225 DF,  p-value: < 2.2e-16\npred<-predict(model1,test)\nhead(pred)\n#> 2022-01-20 2022-01-21 2022-01-24 2022-01-25 2022-01-26 2022-01-27 \n#>   16.92244   17.11869   16.96135   16.27277   16.59502   16.44019\npred\n#> 2022-01-20 2022-01-21 2022-01-24 2022-01-25 2022-01-26 2022-01-27 2022-01-28 \n#>   16.92244   17.11869   16.96135   16.27277   16.59502   16.44019   16.35616 \n#> 2022-01-31 2022-02-01 2022-02-02 2022-02-03 2022-02-04 2022-02-08 2022-02-09 \n#>   15.66042   15.86946   15.64015   15.54924   15.46544   15.57716   15.43428 \n#> 2022-02-10 2022-02-11 2022-02-14 2022-02-15 2022-02-16 2022-02-17 2022-02-18 \n#>   15.59519   15.64140   15.68066   15.65542   15.82226   16.17687   16.51849 \n#> 2022-02-21 2022-02-22 2022-02-23 2022-02-24 2022-02-25 2022-02-28 2022-03-01 \n#>   16.47065   16.31094   16.30532   16.60728   16.31808   16.30070   16.22510 \n#> 2022-03-02 2022-03-03 2022-03-04 2022-03-07 2022-03-08 2022-03-09 2022-03-10 \n#>   16.41384   15.92332   15.85047   15.72339   15.66347   15.35515   15.42859 \n#> 2022-03-11 2022-03-14 2022-03-15 2022-03-16 2022-03-17 2022-03-18 2022-03-22 \n#>   15.37003   15.35250   15.19145   15.87888   16.21718   16.00074   16.10055 \n#> 2022-03-23 2022-03-24 2022-03-25 2022-03-28 2022-03-29 2022-03-30 2022-03-31 \n#>   16.37870   16.38080   15.99185   15.87494   16.45360   16.65301   16.40199 \n#> 2022-04-01 2022-04-04 2022-04-05 2022-04-06 2022-04-07 2022-04-08 2022-04-11 \n#>   16.64081   16.76660   16.68875   16.92925   17.12510   17.01967   17.08268 \n#> 2022-04-12 2022-04-13 \n#>   16.98242   16.84476"},{"path":"big-data-and-machine-learning.html","id":"accuracy-of-the-prediction","chapter":"5 Big data and machine learning","heading":"5.5 Accuracy of the prediction","text":"Lest make plot forecast vs real value BNB.Finaly, measure accuracy prediction, apply Root Mean Square Error (RMSE). gives idea much error system typically makes predictions. formula RMSE :\\[RMSE =\\frac{1}{n}\\ \\sum_{=1}^{n} (y_{}-\\hat{f(x_{}))^{2}} \\]\n$ $ prediction ith observation (actual), $ y_{} $ observation ith independent variable, n number observations.\\[\\hat{f(x_{})}=\\hat{\\beta_{0}}+\\hat{\\beta_{1}}x_{1}+,..,+\\hat{\\beta_{n}}x_{n}\\]RMSE computed using training data used fit model, accurately referred training RMSE., RMSE close zero, better.\nsqrt(mean((real-forecast)^2,na.rm = T ))","code":"\npred2<-as.data.frame(pred)\ny2<-as.data.frame(y1)\nall<-cbind(y2,pred2)\nplot(all[,1],type = \"l\",col=\"blue\",ylab=\"x\")\nlines(all[,2],col=\"green\")\nlegend(x= \"topleft\", legend = c(\"real\",\"prediction\"),lty = 1,lwd=2,col=c(\"blue\",\"green\"))\nsqrt(mean((all[,1]-all[,2])^2,na.rm = T ))\n#> [1] 0.242318"},{"path":"big-data-and-machine-learning.html","id":"appendix","chapter":"5 Big data and machine learning","heading":"5.6 Appendix","text":"MACD signals (investopedia).Moving Average Convergence Divergence (MACD) trend-following momentum indicator shows relationship two moving averages security’s price. MACD calculated subtracting 26-period Exponential Moving Average (EMA) 12-period EMA.result calculation MACD line. nine-day EMA MACD called “signal line,” plotted MACD line, can signal buy sell. Traders may buy security MACD crosses signal line sell - short - security MACD crosses signal line.exponential moving average (EMA) type moving average (MA) places greater weight significance recent data points. exponential moving average also referred exponentially weighted moving average. exponentially weighted moving average reacts significantly recent price changes simple moving average (SMA), applies equal weight observations period.next example, default, function MACD creates 12 days EMA 26-days EMA.relative strength index (RSI)momentum indicator measures magnitude recent price changes evaluate overbought oversold conditions price stock asset. RSI displayed oscillator (line graph moves two extremes) can reading 0 100. indicator originally developed J. Welles Wilder Jr. introduced seminal 1978 book, New Concepts Technical Trading Systems.Relative Strength Index (RSI) calculates ratio recent upward price movements absolute price movement. Developed J. Welles Wilder. RSI calculation RSI = 100 - 100 / ( 1 + RS ), RS smoothed ratio ‘average’ gains ‘average’ losses. ‘averages’ aren’t true averages, since ’re divided value n number periods gains/losses.Traditional interpretation usage RSI values 70 indicate security becoming overbought overvalued may primed trend reversal corrective pullback (drop stock) price. RSI reading 30 indicates oversold undervalued condition (Investopedia).","code":""},{"path":"big-data-and-machine-learning.html","id":"bibliography","chapter":"5 Big data and machine learning","heading":"5.7 Bibliography","text":"Jeet, P Vat, P. (2017). Learning Quantitative finance R (2017), Packt Publishing, Birmingham, UK.","code":""},{"path":"credit-analysis.html","id":"credit-analysis","chapter":"6 Credit analysis","heading":"6 Credit analysis","text":"","code":"\nlibrary(openxlsx)\nlibrary(dplyr)\nlibrary(caret)"},{"path":"credit-analysis.html","id":"example","chapter":"6 Credit analysis","heading":"6.1 Example","text":"database credit_short.xlsx historical information lendingclub, https://www.lendingclub.com/ fintech marketplace bank scale. One spreadsheets variable description. original data set least 2 million observations 150 variables. find credit_semioriginal.xlsx first 1,000 observations.following example, find example “prediction model” credit_short.xlsx, 26 variables.dataset source:\nhttps://www.kaggle.com/wordsforthewise/lending-club","code":""},{"path":"credit-analysis.html","id":"a-independent-variable-creation","chapter":"6 Credit analysis","heading":"6.1.1 a) Independent variable creation","text":"Function count(df,col,sort = T) give us number observations per category variable loan_status:example, evidence, create binary variable, based Fully Paid Charged categories, equivalent -default default respectively.Charge ” means credit grantor wrote account receivables loss, closed future charges. account displays status “charge ,” means account closed future use, although debt still owed.barplot(df\\(colsum , names.arg=df\\)colnames , las=2,\ncol=c(“red”,“blue”,“green”,“purple”,“black”),\nylim=c(0,700))df data frame name, colsum column data frame number categorie colnames column data frame names categories.make filter, way loan_status contains Fully Paid Charged :library dplyr%>% pipe todf %>%\nfilter(col== “r1” |col== “r2”)\ndf data frame, col column name, r1 r2 category 1 2 respectively, case Fully Paid Charged OffFor logit model run, need transform loan_status (0,1), “Fully Paid” 0, “Charged ”, 1. relevant variable “Charged ”, concerned expected losses (customers pay loan).apply ifelse function cbind combine object.","code":"\ndf<-read.xlsx(\"data/credit_short.xlsx\")\nco<-count(df,loan_status,sort =T )\nco\n#>          loan_status   n\n#> 1         Fully Paid 728\n#> 2        Charged Off 145\n#> 3            Current 121\n#> 4 Late (31-120 days)   5\n#> 5    In Grace Period   1\n# para saber cuantas cztegorias tiene term\nco<-count(df,loan_status,sort =T )\nbarplot(co$n , names.arg=co$loan_status , las=2 , \n  col=c(\"red\",\"blue\",\"green\",\"purple\",\"black\"),\n                  ylim=c(0,700))\ndf2<-df %>%\n  filter(loan_status == \"Fully Paid\" | loan_status== \"Charged Off\") \ncount(df2,loan_status)\n#>   loan_status   n\n#> 1 Charged Off 145\n#> 2  Fully Paid 728\nDefault<-ifelse(df2$loan_status==\"Fully Paid\",0,1)\n#Default<-factor(Default ,levels =c(1,0))\n# combining default and df2\ndf3<-cbind(Default,df2)\n# delete loan status, is the second colum\ndf4<-df3[,-2]\nhead(df4)\n#>   Default loan_amnt      term int_rate installment grade sub_grade\n#> 1       0      3600 36 months    13.99      123.03     C        C4\n#> 2       0     24700 36 months    11.99      820.28     C        C1\n#> 3       0     20000 60 months    10.78      432.66     B        B4\n#> 4       0     10400 60 months    22.45      289.91     F        F1\n#> 5       0     11950 36 months    13.44      405.18     C        C3\n#> 6       0     20000 36 months     9.17      637.58     B        B2\n#>                                 emp_title emp_length home_ownership annual_inc\n#> 1                                 leadman  10+ years       MORTGAGE      55000\n#> 2                                Engineer  10+ years       MORTGAGE      65000\n#> 3                            truck driver  10+ years       MORTGAGE      63000\n#> 4                     Contract Specialist    3 years       MORTGAGE     104433\n#> 5                    Veterinary Tecnician    4 years           RENT      34000\n#> 6 Vice President of Recruiting Operations  10+ years       MORTGAGE     180000\n#>   verification_status            purpose              title   dti\n#> 1        Not Verified debt_consolidation Debt consolidation  5.91\n#> 2        Not Verified     small_business           Business 16.06\n#> 3        Not Verified   home_improvement               <NA> 10.78\n#> 4     Source Verified     major_purchase     Major purchase 25.37\n#> 5     Source Verified debt_consolidation Debt consolidation 10.20\n#> 6        Not Verified debt_consolidation Debt consolidation 14.67\n#>   earliest_cr_line open_acc pub_rec revol_bal revol_util total_acc\n#> 1         Aug-2003        7       0      2765       29.7        13\n#> 2         Dec-1999       22       0     21470       19.2        38\n#> 3         Aug-2000        6       0      7869       56.2        18\n#> 4         Jun-1998       12       0     21929       64.5        35\n#> 5         Oct-1987        5       0      8822       68.4         6\n#> 6         Jun-1990       12       0     87329       84.5        27\n#>   initial_list_status application_type mort_acc pub_rec_bankruptcies\n#> 1                   w       Individual        1                    0\n#> 2                   w       Individual        4                    0\n#> 3                   w        Joint App        5                    0\n#> 4                   w       Individual        6                    0\n#> 5                   w       Individual        0                    0\n#> 6                   f       Individual        4                    0\n# Don´t forget to eliminate the column loan_status, because it would be duplicated with Default"},{"path":"credit-analysis.html","id":"b-prediction-model","chapter":"6 Credit analysis","heading":"6.1.2 b) “Prediction model”","text":"run model like , variables categorical, example, term, grade, many others, model accuracy low. importantly, predict function may work. need transform variables numeric.example, column term following categories:following code transforms categorical variable numerical. code level course shown exposition purposes (covered final exam)., created package makes procedure us entire data set one click.Download file: Art_0.1.0.tar.gz install package archive file. Apply function asnum(df). Also, simplicity, going apply na.omit() function, eliminates rows missing values.install library :Split data set training test 80% training 20% test data set.data set time series, use function sample. randomly generates dim[1]*n numbers full data set. dim[1] number rows full data set n %, case 80%.set.seed (1)\ntrain_sample<-sample(dim[1],dim[1]*n)train <- df[train_sample, ]\ntest <- df[-train_sample, ]\nset.seed (13)result running logit model variables using train set :\nglm(y ~x1+x2+x3,data=,family=binomial())y dependent variable, x1, x2, x3 independent variables model:\\[y=\\alpha_{0}\\ +\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\beta_{3}x_{3}+e\\]\ne error term.want appli model variables\nglm(y ~.,data=,family=binomial())\\[y=\\alpha_{0}\\ +\\beta_{1}x_{1}+\\beta_{2}x_{2}+...+\\beta_{n}x_{n}+e\\]case, y Default variable.\nglm(y ~x1+x2+x3,data=,family=binomial())prediction:\npredict(model,newdata = test,type = “response”)type = “response” argument get transformation logit model probability. Also need transform probability c(0,1).","code":"\n#library(devtools)\n#doremotes::install_github(\"abernal30/dataclean\")\n\n#library(devtools)\n#devtools::install_github(\"abernal30/dataclean\")\nlibrary(dataclean) \ndf5<-asnum(df4)\nhead(df5)\n#>   Default loan_amnt term int_rate installment grade sub_grade emp_title\n#> 1       0      3600    1    13.99      123.03     1         1         1\n#> 2       0     24700    1    11.99      820.28     1         2         2\n#> 3       0     20000    2    10.78      432.66     2         3         3\n#> 4       0     10400    2    22.45      289.91     3         4         4\n#> 5       0     11950    1    13.44      405.18     1         5         5\n#> 6       0     20000    1     9.17      637.58     2         6         6\n#>   emp_length home_ownership annual_inc verification_status purpose title   dti\n#> 1          1              1      55000                   1       1     1  5.91\n#> 2          1              1      65000                   1       2     2 16.06\n#> 3          1              1      63000                   1       3     3 10.78\n#> 4          2              1     104433                   2       4     4 25.37\n#> 5          3              2      34000                   2       1     1 10.20\n#> 6          1              1     180000                   1       1     1 14.67\n#>   earliest_cr_line open_acc pub_rec revol_bal revol_util total_acc\n#> 1                1        7       0      2765       29.7        13\n#> 2                2       22       0     21470       19.2        38\n#> 3                3        6       0      7869       56.2        18\n#> 4                4       12       0     21929       64.5        35\n#> 5                5        5       0      8822       68.4         6\n#> 6                6       12       0     87329       84.5        27\n#>   initial_list_status application_type mort_acc pub_rec_bankruptcies\n#> 1                   1                1        1                    0\n#> 2                   1                1        4                    0\n#> 3                   1                2        5                    0\n#> 4                   1                1        6                    0\n#> 5                   1                1        0                    0\n#> 6                   2                1        4                    0#>     Default loan_amnt term int_rate installment grade sub_grade emp_title\n#> 773       0     13050    2    26.06      391.19     3        27        39\n#> 698       0     15000    1     9.80      482.61     2        19       533\n#> 652       0      2000    1    11.99       66.42     1         2       500\n#> 548       0     20000    1    11.48      659.33     2         9       426\n#> 872       0      5500    1    11.99      182.66     1         2       648\n#> 392       0     22600    1    15.77      791.99     6        18        39\n#>     emp_length home_ownership annual_inc verification_status purpose title\n#> 773          1              1      43500                   3       1     1\n#> 698          7              2      89000                   1       1     1\n#> 652          3              2      35000                   2       1     1\n#> 548         11              2     185000                   3       1     1\n#> 872          2              2      37000                   2       1     1\n#> 392          2              1      53867                   1       1     1\n#>       dti earliest_cr_line open_acc pub_rec revol_bal revol_util total_acc\n#> 773 22.79              305        8       0     12799       88.3        21\n#> 698 19.50               29       11       0      6273       26.4        24\n#> 652  4.01              256        7       1      1024       17.1        16\n#> 548 25.70              242       17       0     31201       90.2        28\n#> 872 21.15               42        7       0      5914       81.0        21\n#> 392 25.31               40       33       0     23959       53.5        46\n#>     initial_list_status application_type mort_acc pub_rec_bankruptcies\n#> 773                   1                1        3                    0\n#> 698                   1                1        1                    0\n#> 652                   1                1        0                    0\n#> 548                   1                1        0                    0\n#> 872                   1                1        0                    0\n#> 392                   1                1        0                    0\nmodel<-glm(Default~. ,data=train,family=binomial())\npredict<-predict(model,newdata = test,type = \"response\")\n#trasnform that probability into a 0,1\npredictp<-ifelse(predict>.5,1,0)\npredictp\n#>   7  21  29  31  34  35  38  39  41  43  46  55  57  61  66  68  83  90  91  97 \n#>   0   0   0   0   0   0   0   0   1   0   0   1   0  NA   0   1   0   0   0   0 \n#> 106 107 112 113 119 124 126 141 150 152 156 160 162 167 173 174 179 196 199 200 \n#>   0   0   1   0   0  NA   1   0   0   0   0   0  NA   0   0   0   1   0   0   0 \n#> 203 207 210 211 216 218 221 230 232 235 239 240 249 254 259 264 267 268 279 281 \n#>   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1 \n#> 288 290 301 302 308 313 315 323 329 335 341 342 348 350 351 359 361 363 365 368 \n#>   0  NA   0   0   0   1   0   0   0   0   0   0   1   0   0  NA   0   0   0   0 \n#> 384 396 402 404 408 409 414 419 423 425 434 435 445 447 451 452 453 454 458 459 \n#>   0   0   0   0   0   0   0   0   0  NA   0   0   0   0   0   1   0   0   0   1 \n#> 464 467 480 491 499 501 506 511 523 525 526 530 536 537 543 544 545 546 568 569 \n#>   0   0   0   0  NA   0   0   0   0   1   1   0   0   0  NA   0   0   0   0   0 \n#> 573 585 588 595 599 600 609 622 627 632 638 655 657 661 665 687 692 699 700 706 \n#>   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n#> 708 719 724 726 735 740 748 751 754 755 758 775 777 781 783 784 789 794 799 803 \n#>   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  NA   0   0 \n#> 804 807 810 811 812 814 819 825 829 835 836 843 844 851 854 \n#>   0   0   0  NA   0   0   0   0   0   0   0   0   0   1   0"},{"path":"credit-analysis.html","id":"measuring-model-performance","chapter":"6 Credit analysis","heading":"6.1.3 Measuring model performance","text":"confusion Matrix. need transform variables factor.confusionMatrix(prediction,real)Cross validation.Resampling methods indispensable tool modern statistics. involve repeatedly drawing samples training set refitting model interest sample order obtain additional information model. approach may allow us obtain information available fitting model using original training sample.Also, help us get best variables improves accuracy. First need transform dependent variable factor.K Fold Cross ValidationThis approach involves randomly k-fold CV dividing set observations k groups, folds, approximately equal size. first fold treated validation set, method fit remaining k − 1 folds.glm(Default ~ ., data = train)model glmStepAIC makes selection variables. case, improve Accuracy.Making prediction.Suponiendo que este es mi modelo final, voy hacer la predicción real.\nUna persona pide crédito y tiene los siguietes datos","code":"#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction   1   0\n#>          1   7   8\n#>          0  16 134\n#>                                           \n#>                Accuracy : 0.8545          \n#>                  95% CI : (0.7913, 0.9045)\n#>     No Information Rate : 0.8606          \n#>     P-Value [Acc > NIR] : 0.6409          \n#>                                           \n#>                   Kappa : 0.2903          \n#>                                           \n#>  Mcnemar's Test P-Value : 0.1530          \n#>                                           \n#>             Sensitivity : 0.30435         \n#>             Specificity : 0.94366         \n#>          Pos Pred Value : 0.46667         \n#>          Neg Pred Value : 0.89333         \n#>              Prevalence : 0.13939         \n#>          Detection Rate : 0.04242         \n#>    Detection Prevalence : 0.09091         \n#>       Balanced Accuracy : 0.62400         \n#>                                           \n#>        'Positive' Class : 1               \n#> \ndef_train_f<-factor(train$Default,levels=c(1,0))\ntrainf<-train\ntrainf[,\"Default\"]<-def_train_f\ntestf<-test\ntestf[,\"Default\"]<-real # antes una f \n#set.seed(1)\ntrainf<-na.omit(trainf)# delete the rows with nas or missing values\ngbmFit1 <- train(Default ~ ., data = trainf,\n                 method = \"glmStepAIC\", \n                            trControl = trainControl(method = \"cv\", number = 10),\n                        trace=0,   metric=\"Accuracy\")\ngbmFit1 \n#> Generalized Linear Model with Stepwise Feature Selection \n#> \n#> 665 samples\n#>  24 predictor\n#>   2 classes: '1', '0' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 598, 598, 598, 598, 599, 599, ... \n#> Resampling results:\n#> \n#>   Accuracy  Kappa    \n#>   0.818227  0.1285474\ngbmFit1$finalModel$formula\n#> .outcome ~ loan_amnt + term + int_rate + emp_length + home_ownership + \n#>     open_acc + pub_rec + mort_acc\n#> <environment: 0x000000003e1d1030>\ntest[1,]\n#>   Default loan_amnt term int_rate installment grade sub_grade emp_title\n#> 7       0     20000    1     8.49      631.26     2         7         7\n#>   emp_length home_ownership annual_inc verification_status purpose title   dti\n#> 7          1              1      85000                   1       4     4 17.61\n#>   earliest_cr_line open_acc pub_rec revol_bal revol_util total_acc\n#> 7                7        8       0       826        5.7        15\n#>   initial_list_status application_type mort_acc pub_rec_bankruptcies\n#> 7                   1                1        3                    0\nmodel<-glm(Default~loan_amnt + term + int_rate + installment + grade + \n    sub_grade + home_ownership + open_acc + pub_rec + mort_acc + \n    pub_rec_bankruptcies ,data=train,family=binomial())\npredict<-predict(model,newdata = test[1,],type = \"response\")\n#trasnform that probability into a 0,1\npredictp<-ifelse(predict>.5,1,0)\npredictp\n#> 7 \n#> 0"},{"path":"rational-agent-and-behavioral-finance-in-investment.html","id":"rational-agent-and-behavioral-finance-in-investment","chapter":"7 Rational agent and behavioral finance in investment","heading":"7 Rational agent and behavioral finance in investment","text":"test chapter text based Wooldridge (2012) codes.","code":""},{"path":"rational-agent-and-behavioral-finance-in-investment.html","id":"shorts-samples-test-of-efficient-markets-hypothesis-emh-for-one-asset","chapter":"7 Rational agent and behavioral finance in investment","heading":"7.1 Shorts samples test of efficient markets hypothesis (EMH) for one asset","text":"Let yt daily price S&P500. strict form efficient markets hypothesis states information observable market prior day t help predict price. use past information y, EMH stated :\\[E(y_t/ y_{t-1} ,y_{t-1},.... )=E(y_t) \\]previous equation false, use information past predict current price.EMH presumes investment opportunities noticed disappear almost instantaneously.One simple way equation specify AR(1) model alternative model.\\[y_t= \\beta_0 +\\beta_1\\ y_{t-1}+u_t, \\]significant beta1 coefficient reject EMH; , use information past predict current price. However, common practice make test using lags.\\[y_t= \\beta_0 +\\beta_1\\ y_{t-1},...,y_{t-n}+u_t\\]Now, make lags price, make Loop , creating lag storing created lags xts object S&P500 close price stored.Note. Compute lagged version time series, shifting time base back given number observations.Run regressionRemember, significant beta1 coefficient reject EMH; , use past information predict price, case DJI.","code":"\ndji<-all[,4]\ndji<-Delt(all[,4])\nla2<-stats::lag(dji,2)\nla3<-stats::lag(dji,3)\ndji<-cbind(dji,la2,la3)\ndji<-na.omit(dji)\ncolnames(dji)<-c(\"SP500\",\"SP500_lag2\",\"SP500_lag3\")\nhead(dji)\n#>                  SP500   SP500_lag2   SP500_lag3\n#> 2007-01-08  0.01412499 -0.014953248 -0.012687405\n#> 2007-01-09 -0.01652505 -0.009250474 -0.014953248\n#> 2007-01-10  0.01032165  0.014124994 -0.009250474\n#> 2007-01-11  0.04110240 -0.016525047  0.014124994\n#> 2007-01-12  0.01437702  0.010321651 -0.016525047\n#> 2007-01-15  0.02137235  0.041102400  0.010321651\nsummary(lm(SP500~.,data =dji))\n#> \n#> Call:\n#> lm(formula = SP500 ~ ., data = dji)\n#> \n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -0.170951 -0.009607 -0.000202  0.009729  0.136626 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)  0.0007028  0.0003145   2.235   0.0255 *\n#> SP500_lag2   0.0272148  0.0160414   1.697   0.0899 .\n#> SP500_lag3  -0.0120675  0.0160421  -0.752   0.4520  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.0196 on 3890 degrees of freedom\n#> Multiple R-squared:  0.0008625,  Adjusted R-squared:  0.0003488 \n#> F-statistic: 1.679 on 2 and 3890 DF,  p-value: 0.1867"},{"path":"rational-agent-and-behavioral-finance-in-investment.html","id":"long-samples-test-of-efficient-markets-hypothesis-emh-for-one-asset","chapter":"7 Rational agent and behavioral finance in investment","heading":"7.2 Long samples test of efficient markets hypothesis (EMH) for one asset","text":"Although EMH states expected return given past observable information constant, says nothing conditional variance. tested using \nheteroskedasticity, Breusch-Pagan, example. However, heteroskedasticity better characterized ARCH model.Suppose dependent variable, y(t), contemporary exogenous variable, z(t).\\[E(y_t/z_t,y_{t-1},z_{t-1},y_{t-2},..)= \\beta_0 +\\beta_1\\ z_t+\\beta_2\\ y_{t-1}+\\beta_3\\ z_{t-1}. \\]typical approach assume :\\[Var(y_t/z_t,y_{t-1},z_{t-1},y_{t-2},..)= \\sigma, \\]\nconstant. variance follow ARCH model:\\[Var(y_t/z_t,y_{t-1},z_{t-1},y_{t-2},..)=Var(u_t/z_t,y_{t-1},z_{t-1},y_{t-2},..)=\\\\ \\alpha_0 +\\alpha_1\\ u^2_{t-2}.\\]\n\\[u_t=y_t-E(y_t/z_t,y_{t-1},z_{t-1},y_{t-2},..)\\]can check ARCH effects using ArchTest() function FinTS package. Lagrange Multiplier (LM) test autoregressive conditional heteroscedasticity (ARCH). Computes Lagrange multiplier test conditional heteroscedasticity. Equivalent test OLS:\\[u_t=\\alpha_0 +\\alpha_1\\ u^2_{t-2}\\]\nlook verify significance alpha 1. alpha 1 significant, reject null hypothesis conclude presence ARCH(1) effects. use past information predict future.\nArchTest(object,lags=n), usually lags=1If p-value <10%, case, conclude presence ARCH(1) effects, make forecast time series using past information.","code":"\nticker<-\"^GSPC\"\ngetSymbols(ticker,from=\"2021-05-01\",to=\"2022-05-01\")\n#> [1] \"^GSPC\"\ndji_long<-GSPC[,4]\nar<-ArchTest(dji_long,lags=1)\ndata.frame(ar$p.value)\n#>               ar.p.value\n#> Chi-squared 9.801502e-53\nlibrary(quantmod)\n#library(xml2) # this are for the  code \n#library(rvest) # in 154 to 177 lines, just instaal it if you are running that code\nlibrary(openxlsx)\nlibrary(FinTS)\nlibrary(tseries)\nlibrary(rugarch)"},{"path":"rational-agent-and-behavioral-finance-in-investment.html","id":"long-samples-test-of-efficient-markets-hypothesis-emh-applyied-to-portfolio","chapter":"7 Rational agent and behavioral finance in investment","heading":"7.3 Long samples test of efficient markets hypothesis (EMH) applyied to portfolio","text":"next session, cover subject market anomalies. cover momentum anomalies. However, strategy constructing portfolio stocks. However, first, need filter stocks can make prediction. assume going long-term horizon portfolio. , need apply long samples test efficient markets hypothesis.following code makes long samples test efficient markets hypothesis (EMH) applied many assets dfx object.start estimating returns time series deleteing missig valies.next loops knowledge level course contents requires, application covered final exam, topic loops covered.following code counts number tickers use make prediction, applyinf ifelse function combing object contains EMH test.Finally, filter get tickers category predict.arf<- df %>%\nfilter(coll== “category”)Also take historical information filtered stocks","code":"\ndf<-read.xlsx(\"data/df_dates.xlsx\", detectDates = T)\n\ndate<-df[,1]\n\ndim<-dim(df)\n\n# important to takeout the data before transforming to xts, other wise does not transform into numeric. \ndata<-df[,2:dim[2]]\n\ndatax<- xts(data,\n         order.by = as.Date(date))\n\ndfx<-na.omit(datax)\nreturn<-Delt(dfx[,1])\n\n# we are going to apply the Delt function to the 100 stocks\n\n# function apply()\nreturn_all<-apply(dfx, 2, Delt)\n# es 1 for rows o 2for columns\ndfr<-return_all\nm<-26\n#---de aqui es la creación del Loop for, esto rebasa el nivel de este curso\nar<-c()\nn<-dim(dfr)[2]\nfor (i in 1:n){\nar1<-ArchTest(dfr[,i],lags=m)$p.value\nar<-c(ar,ar1)\n}\n#----- \nar<-data.frame(ar)\n# it has the p value of the EMH test, if the p-value is lees than 10%, then we could make predictions \n\n# add the name of the ticker\ncol_name<-colnames(return_all)\ncol_name\n#>   [1] \"AAPL.Close\"   \"MSFT.Close\"   \"GOOG.Close\"   \"GOOGL.Close\"  \"AMZN.Close\"  \n#>   [6] \"TSLA.Close\"   \"BRK.A.Close\"  \"BRK.B.Close\"  \"FB.Close\"     \"TSM.Close\"   \n#>  [11] \"JNJ.Close\"    \"UNH.Close\"    \"NVDA.Close\"   \"V.Close\"      \"JPM.Close\"   \n#>  [16] \"TCEHY.Close\"  \"TCTZF.Close\"  \"XOM.Close\"    \"BAC.Close\"    \"PG.Close\"    \n#>  [21] \"WMT.Close\"    \"MA.Close\"     \"CVX.Close\"    \"JPM.PC.Close\" \"JPM.PD.Close\"\n#>  [26] \"NSRGY.Close\"  \"NSRGF.Close\"  \"LVMUY.Close\"  \"BAC.PK.Close\" \"HD.Close\"    \n#>  [31] \"LVMHF.Close\"  \"BAC.PL.Close\" \"LLY.Close\"    \"PFE.Close\"    \"RHHBF.Close\" \n#>  [36] \"RHHBY.Close\"  \"RHHVF.Close\"  \"KO.Close\"     \"BML.PL.Close\" \"BAC.PE.Close\"\n#>  [41] \"ABBV.Close\"   \"BML.PG.Close\" \"BAC.PB.Close\" \"BML.PH.Close\" \"BML.PJ.Close\"\n#>  [46] \"BABA.Close\"   \"BABAF.Close\"  \"AVGO.Close\"   \"NVO.Close\"    \"NONOF.Close\" \n#>  [51] \"IDCBF.Close\"  \"IDCBY.Close\"  \"PEP.Close\"    \"MRK.Close\"    \"ASML.Close\"  \n#>  [56] \"ASMLF.Close\"  \"TM.Close\"     \"TOYOF.Close\"  \"RYDAF.Close\"  \"SHEL.Close\"  \n#>  [61] \"TMO.Close\"    \"BHP.Close\"    \"BHPLF.Close\"  \"VZ.Close\"     \"AZN.Close\"   \n#>  [66] \"COST.Close\"   \"AZNCF.Close\"  \"ABT.Close\"    \"NVS.Close\"    \"WFC.PY.Close\"\n#>  [71] \"ADBE.Close\"   \"NVSEF.Close\"  \"DIS.Close\"    \"WFC.PL.Close\" \"CMCSA.Close\" \n#>  [76] \"ORCL.Close\"   \"DHR.Close\"    \"WFC.PR.Close\" \"WFC.PQ.Close\" \"ACN.Close\"   \n#>  [81] \"CSCO.Close\"   \"LRLCF.Close\"  \"LRLCY.Close\"  \"CICHF.Close\"  \"CICHY.Close\" \n#>  [86] \"MCD.Close\"    \"NKE.Close\"    \"INTC.Close\"   \"WFC.Close\"    \"C.PJ.Close\"  \n#>  [91] \"TMUS.Close\"   \"PM.Close\"     \"AMD.Close\"    \"LIN.Close\"    \"TXN.Close\"   \n#>  [96] \"CRM.Close\"    \"BMY.Close\"    \"UPS.Close\"    \"RLLCF.Close\"  \"QCOM.Close\"\n# add the ticker na,me\nrownames(ar)<-col_name\nar\n#>                        ar\n#> AAPL.Close   3.731438e-17\n#> MSFT.Close   6.033001e-39\n#> GOOG.Close   3.224007e-16\n#> GOOGL.Close  2.570860e-16\n#> AMZN.Close   5.093709e-04\n#> TSLA.Close   2.275599e-05\n#> BRK.A.Close  9.677118e-32\n#> BRK.B.Close  1.239809e-42\n#> FB.Close     9.999469e-01\n#> TSM.Close    1.023778e-17\n#> JNJ.Close    3.822835e-55\n#> UNH.Close    4.686889e-41\n#> NVDA.Close   5.391420e-16\n#> V.Close      4.215814e-25\n#> JPM.Close    2.665692e-25\n#> TCEHY.Close  9.959273e-01\n#> TCTZF.Close  9.621425e-01\n#> XOM.Close    1.416110e-15\n#> BAC.Close    2.022251e-25\n#> PG.Close     3.813798e-46\n#> WMT.Close    6.208759e-18\n#> MA.Close     1.311943e-25\n#> CVX.Close    2.928436e-36\n#> JPM.PC.Close 1.177236e-31\n#> JPM.PD.Close 1.802175e-39\n#> NSRGY.Close  3.838131e-34\n#> NSRGF.Close  9.786318e-17\n#> LVMUY.Close  3.131397e-30\n#> BAC.PK.Close 1.334235e-44\n#> HD.Close     1.407075e-28\n#> LVMHF.Close  4.802603e-23\n#> BAC.PL.Close 5.073425e-39\n#> LLY.Close    4.523789e-01\n#> PFE.Close    4.057538e-10\n#> RHHBF.Close  3.191309e-10\n#> RHHBY.Close  4.145604e-21\n#> RHHVF.Close  4.923693e-22\n#> KO.Close     7.387011e-37\n#> BML.PL.Close 3.373921e-38\n#> BAC.PE.Close 4.353408e-40\n#> ABBV.Close   8.142840e-15\n#> BML.PG.Close 5.972559e-27\n#> BAC.PB.Close 1.512724e-39\n#> BML.PH.Close 1.148495e-37\n#> BML.PJ.Close 5.720375e-32\n#> BABA.Close   9.584327e-01\n#> BABAF.Close  9.915286e-01\n#> AVGO.Close   6.211754e-36\n#> NVO.Close    2.608217e-17\n#> NONOF.Close  6.222725e-03\n#> IDCBF.Close  6.026186e-06\n#> IDCBY.Close  5.635984e-05\n#> PEP.Close    9.403916e-42\n#> MRK.Close    4.164260e-16\n#> ASML.Close   2.102782e-21\n#> ASMLF.Close  7.137022e-18\n#> TM.Close     4.283555e-10\n#> TOYOF.Close  1.000000e+00\n#> RYDAF.Close  2.468392e-29\n#> SHEL.Close   6.660111e-28\n#> TMO.Close    1.081827e-17\n#> BHP.Close    2.567945e-27\n#> BHPLF.Close  4.454357e-06\n#> VZ.Close     2.605012e-23\n#> AZN.Close    1.370948e-12\n#> COST.Close   6.450225e-13\n#> AZNCF.Close  3.414503e-05\n#> ABT.Close    2.771004e-30\n#> NVS.Close    2.446595e-34\n#> WFC.PY.Close 6.843455e-20\n#> ADBE.Close   3.030550e-27\n#> NVSEF.Close  2.347245e-28\n#> DIS.Close    3.796324e-12\n#> WFC.PL.Close 1.137063e-29\n#> CMCSA.Close  1.099444e-25\n#> ORCL.Close   1.339600e-13\n#> DHR.Close    4.817773e-25\n#> WFC.PR.Close 3.355589e-40\n#> WFC.PQ.Close 1.902654e-26\n#> ACN.Close    5.024203e-30\n#> CSCO.Close   5.216193e-16\n#> LRLCF.Close  2.238256e-27\n#> LRLCY.Close  2.921135e-35\n#> CICHF.Close  4.538511e-03\n#> CICHY.Close  7.367629e-03\n#> MCD.Close    4.305474e-27\n#> NKE.Close    4.117376e-07\n#> INTC.Close   4.792427e-23\n#> WFC.Close    1.886590e-30\n#> C.PJ.Close   7.192938e-31\n#> TMUS.Close   4.916386e-08\n#> PM.Close     1.502492e-46\n#> AMD.Close    3.990568e-08\n#> LIN.Close    1.617731e-22\n#> TXN.Close    2.126171e-36\n#> CRM.Close    9.968976e-01\n#> BMY.Close    2.116336e-17\n#> UPS.Close    2.309662e-02\n#> RLLCF.Close  3.103137e-01\n#> QCOM.Close   8.319828e-08\nlibrary(dplyr)\n\npred<-ifelse(ar[,1]<0.1,\"Predict\",\"No Predict\")\n# merge with the ar object \nar<-cbind(ar,pred)\nar\n#>                        ar       pred\n#> AAPL.Close   3.731438e-17    Predict\n#> MSFT.Close   6.033001e-39    Predict\n#> GOOG.Close   3.224007e-16    Predict\n#> GOOGL.Close  2.570860e-16    Predict\n#> AMZN.Close   5.093709e-04    Predict\n#> TSLA.Close   2.275599e-05    Predict\n#> BRK.A.Close  9.677118e-32    Predict\n#> BRK.B.Close  1.239809e-42    Predict\n#> FB.Close     9.999469e-01 No Predict\n#> TSM.Close    1.023778e-17    Predict\n#> JNJ.Close    3.822835e-55    Predict\n#> UNH.Close    4.686889e-41    Predict\n#> NVDA.Close   5.391420e-16    Predict\n#> V.Close      4.215814e-25    Predict\n#> JPM.Close    2.665692e-25    Predict\n#> TCEHY.Close  9.959273e-01 No Predict\n#> TCTZF.Close  9.621425e-01 No Predict\n#> XOM.Close    1.416110e-15    Predict\n#> BAC.Close    2.022251e-25    Predict\n#> PG.Close     3.813798e-46    Predict\n#> WMT.Close    6.208759e-18    Predict\n#> MA.Close     1.311943e-25    Predict\n#> CVX.Close    2.928436e-36    Predict\n#> JPM.PC.Close 1.177236e-31    Predict\n#> JPM.PD.Close 1.802175e-39    Predict\n#> NSRGY.Close  3.838131e-34    Predict\n#> NSRGF.Close  9.786318e-17    Predict\n#> LVMUY.Close  3.131397e-30    Predict\n#> BAC.PK.Close 1.334235e-44    Predict\n#> HD.Close     1.407075e-28    Predict\n#> LVMHF.Close  4.802603e-23    Predict\n#> BAC.PL.Close 5.073425e-39    Predict\n#> LLY.Close    4.523789e-01 No Predict\n#> PFE.Close    4.057538e-10    Predict\n#> RHHBF.Close  3.191309e-10    Predict\n#> RHHBY.Close  4.145604e-21    Predict\n#> RHHVF.Close  4.923693e-22    Predict\n#> KO.Close     7.387011e-37    Predict\n#> BML.PL.Close 3.373921e-38    Predict\n#> BAC.PE.Close 4.353408e-40    Predict\n#> ABBV.Close   8.142840e-15    Predict\n#> BML.PG.Close 5.972559e-27    Predict\n#> BAC.PB.Close 1.512724e-39    Predict\n#> BML.PH.Close 1.148495e-37    Predict\n#> BML.PJ.Close 5.720375e-32    Predict\n#> BABA.Close   9.584327e-01 No Predict\n#> BABAF.Close  9.915286e-01 No Predict\n#> AVGO.Close   6.211754e-36    Predict\n#> NVO.Close    2.608217e-17    Predict\n#> NONOF.Close  6.222725e-03    Predict\n#> IDCBF.Close  6.026186e-06    Predict\n#> IDCBY.Close  5.635984e-05    Predict\n#> PEP.Close    9.403916e-42    Predict\n#> MRK.Close    4.164260e-16    Predict\n#> ASML.Close   2.102782e-21    Predict\n#> ASMLF.Close  7.137022e-18    Predict\n#> TM.Close     4.283555e-10    Predict\n#> TOYOF.Close  1.000000e+00 No Predict\n#> RYDAF.Close  2.468392e-29    Predict\n#> SHEL.Close   6.660111e-28    Predict\n#> TMO.Close    1.081827e-17    Predict\n#> BHP.Close    2.567945e-27    Predict\n#> BHPLF.Close  4.454357e-06    Predict\n#> VZ.Close     2.605012e-23    Predict\n#> AZN.Close    1.370948e-12    Predict\n#> COST.Close   6.450225e-13    Predict\n#> AZNCF.Close  3.414503e-05    Predict\n#> ABT.Close    2.771004e-30    Predict\n#> NVS.Close    2.446595e-34    Predict\n#> WFC.PY.Close 6.843455e-20    Predict\n#> ADBE.Close   3.030550e-27    Predict\n#> NVSEF.Close  2.347245e-28    Predict\n#> DIS.Close    3.796324e-12    Predict\n#> WFC.PL.Close 1.137063e-29    Predict\n#> CMCSA.Close  1.099444e-25    Predict\n#> ORCL.Close   1.339600e-13    Predict\n#> DHR.Close    4.817773e-25    Predict\n#> WFC.PR.Close 3.355589e-40    Predict\n#> WFC.PQ.Close 1.902654e-26    Predict\n#> ACN.Close    5.024203e-30    Predict\n#> CSCO.Close   5.216193e-16    Predict\n#> LRLCF.Close  2.238256e-27    Predict\n#> LRLCY.Close  2.921135e-35    Predict\n#> CICHF.Close  4.538511e-03    Predict\n#> CICHY.Close  7.367629e-03    Predict\n#> MCD.Close    4.305474e-27    Predict\n#> NKE.Close    4.117376e-07    Predict\n#> INTC.Close   4.792427e-23    Predict\n#> WFC.Close    1.886590e-30    Predict\n#> C.PJ.Close   7.192938e-31    Predict\n#> TMUS.Close   4.916386e-08    Predict\n#> PM.Close     1.502492e-46    Predict\n#> AMD.Close    3.990568e-08    Predict\n#> LIN.Close    1.617731e-22    Predict\n#> TXN.Close    2.126171e-36    Predict\n#> CRM.Close    9.968976e-01 No Predict\n#> BMY.Close    2.116336e-17    Predict\n#> UPS.Close    2.309662e-02    Predict\n#> RLLCF.Close  3.103137e-01 No Predict\n#> QCOM.Close   8.319828e-08    Predict\nlibrary(dplyr) \narf<- ar %>%\n  filter(pred == \"Predict\")\n# this code takes the names of the filtered tickers\ncol_filterd<-rownames(arf)\ndfx_2<-dfx[,col_filterd]\ndfx_3<-data.frame(dfx_2)\ndate<-rownames(dfx_3)\ndfx_4<-cbind(date,dfx_3)"},{"path":"rational-agent-and-behavioral-finance-in-investment.html","id":"bibliography-1","chapter":"7 Rational agent and behavioral finance in investment","heading":"7.4 Bibliography","text":"Wooldridge, J. M. (2012). Introductory econometrics: modern approach. Mason, OH: Thomson/South-Western.","code":""},{"path":"momentum-or-directional-trading.html","id":"momentum-or-directional-trading","chapter":"8 Momentum or directional trading","heading":"8 Momentum or directional trading","text":"previous chapter, covered “Long samples test efficient markets hypothesis (EMH) applied portfolio”. remember, filter get stocks EMH fails, test suggests inefficient market, can make prediction based past information.final goal (expect cover final session) create portfolio filtered stocks submit trading platform, interactive brokers.chapter, make another filter get stocks high expected performance. apply trading strategy called Momentum. strategy consists buying stocks instrument trending selling . idea historical winners expected winners historical losers expected lose short run. momentum effect considered market anomaly (see Cervantes, M., Montoya, M. Á., & Bernal, L.. (2016))Note. also apply machine learning technique predict stocks make filter get ones better risk-reward expected performance, however, probably covered topic UF Algorithms Data Analysis, 2nd period.create signal buy sell, based variables created past information (reason made EMH test). Finally, cover make back-testing using historical data.code methodology based Based : Jeet Vats Learning (2017). code authorship.First read dataTo back-test momentum strategy, divide data set two smaller data sets called -sample -sample data sets. Something similar training test machine learning.define four dates. in_sd defines date -sample data starts, in_ed -sample end date. Similarly, out_sd out_ed defined -sample start end dates. dates defined order data time series format interested building model historical data used real-time data, , data set dates later historical data:sample 1-3 years sample 1 year.back-test momentum strategy, divide data set two: -sample -sample. Something like training test machine learning. back-testing proving strategy performance implementing , without wait days, months see works. Also, helps control human bias towards parameter estimation. use -sample data back-test strategy, estimate optimal set parameters, evaluate performance.-sample data estimating optimal set parameters, evaluate performance. optimal set parameters must applied -sample data understand generalization capacity rules parameters. performance -sample data pretty similar -sample data, assume parameters rule set good generalization power can used live trading.create subsample, price return, datax retx.subset(data,\n+index(data)>= initial date &\n+index(data)<= final date)use moving average convergence divergence (MACD) Bollinger band indicators generate automated trading signals.MACD Bollinger band indicators calculated using following two lines code. used parameter values functions; however, can use parameters think best dataset. output variable macd contains MACD indicator signal value; however, output variable bb contains lower band, average, upper band, percentage Bollinger band:use 26 days lags ArchTest function. generating MACD 26 lags, implying use information past 26 days predict signal buying selling stock.going star single stock, one first column, understand . apply procedure stocks.apply first stock price, in_sample[,1]\nMACD(data, nFast = , nSlow = , nSig = ,maType=“SMA”)usually nfast 12 nSlow 26 nsig 9.BBands(data, n = , maType=“SMA”, sd = )\nusually n=20 se=2Now create variable signal initializes NULL. second line, generated buy signal (1) dji upper Bollinger band macd value macd-signal value; sell signal (-1) dji lower Bollinger band macd less macd-signal value; market signal 0:signal <- ifelse(in_data[,col]> bb[,‘’] &macd[,‘macd’] >macd[,‘signal’],1,ifelse(in_sample[,col]< bb[,‘dn’] &macd[,‘macd’] <macd[,‘signal’],-1,0))\n“col” column numberWe can modify signal generation mechanism use criterion. haven’t included transaction cost slippage cost calculate performance none strategies directly trading.estimate strategy return, use return previous day signal. like assuming , buying signal, buy today sell tomorrow, making one day profit.in_return[,col]*(stats::lag(signal[,col]))\n“col” column numberWe use package PerformanceAnalytics calculate strategy performance annual return.\nEstimating ReturnsAnnualized geometric return\\[ geometric\\ return =\\prod_{=1}^{n} (1+HPR)^{scale/n}\\]\n\\(\\prod_{=1}^{n} (1+HPR)\\) product (1+HPR). Also, n number observations, scale number periods year (daily scale = 252, monthly scale = 12, quarterly scale = 4) HPR Holding Period Return:\\[HPR=  \\frac{Price_{t} - Price_{t-1}}{Price_{t-1}}\\]Return.annualized(data,geometric = T,scale= )Annualized SD\\[ Std\\ Dev.annualized  = (variance(HPR)*252)^{0.5}\\]StdDev.annualized(x,scale=)Assuming risk free rate zero, estimate Sharp Ratio.","code":"\nlibrary(openxlsx)\nlibrary(quantmod)\n\ndata<-read.xlsx(\"data/dfx_2.xlsx\")\n\n# The following codes are to transform the data frame date into xts (as we did last session)\ndate<-data[,1]\n\n# liminating the fits column\ndata<-data[,-1]\n\n#Applying the xts function\ndatax<- xts(data,\n         order.by = as.Date(date))\n\n# Eliminating rows with missing values\ndatax<-na.omit(datax)\n\n#Also we estimate the returns for each stock, as we did in last session\nret<-apply(datax,2,Delt)\n\n# we lost the xts \nretx<- xts(ret,\n         order.by = as.Date(date))\n# Eliminating rows with missing values\nretx<-na.omit(retx)\nin_sd<- \"2018-05-26\"\nin_ed<- \"2021-05-26\" \n\nout_sd<- \"2021-05-27\"\nout_ed<- \"2022-05-27\"\nin_sd<- \"2018-05-26\"  # están en la 59\nin_ed<- \"2021-05-26\" \n# in_sample of the price of the stocks\nin_datax<- subset(datax,\n  +index(datax)>= in_sd &\n  +index(datax)<= in_ed)\n# in_sample of the returns\nin_ret<- subset(retx,\n  +index(retx)>= in_sd &\n  +index(retx)<= in_ed)\n\nout_datax<- subset(datax,\n  +index(datax)>= out_sd &\n  +index(datax)<= out_ed)\n# in_sample of the returns\nout_ret<- subset(retx,\n  +index(retx)>= out_sd &\n  +index(retx)<= out_ed)\nmacd<-MACD(in_datax[,3] , nFast =12 , nSlow =26  , nSig =9 ,maType=\"SMA\")\n\nbb<-BBands(in_datax[,3], n = 20, maType=\"SMA\", sd = 2)\nsignal <- ifelse(in_datax[,1]> bb[,'up'] & macd[,'macd'] >macd[,'signal'],1,ifelse(in_datax[,1]< bb[,'dn'] &macd[,'macd'] <macd[,'signal'],-1,0))\nplot(signal[,1])\nstrat_ret<-in_ret[,1]*(stats::lag(signal[,1]))\nlibrary(PerformanceAnalytics)\n# The strategy return signal \nret_annual<-Return.annualized(strat_ret,geometric = T,scale= 252)\nret_annual\n#>                   AAPL.Close\n#> Annualized Return -0.3863758\n\nsd<-StdDev.annualized(strat_ret,scale=252)\n\n# Sharpe ratio\nret_annual/sd\n#>                   AAPL.Close\n#> Annualized Return  -1.077952"},{"path":"momentum-or-directional-trading.html","id":"bibliography-2","chapter":"8 Momentum or directional trading","heading":"8.1 Bibliography","text":"Cervantes, M., Montoya, M. Á., & Bernal, L.. (2016). Effect Business Cycle Investment Strategies: Evidence Mexico. Revista Mexicana de Economía y Finanzas, 11(2).Jeet, P VatsLearning, P (2017). Quantitative Finance R. Packt Publishing.","code":""},{"path":"portfolio-management-algorithms.html","id":"portfolio-management-algorithms","chapter":"9 Portfolio management algorithms","heading":"9 Portfolio management algorithms","text":"chapter, take filtered stocks “Rational agents theory behavioral finance theories”. remember, previous chapter applied momentum strategy.file df_merge.xlsx find estimations in_sample out_sample.mention previous chapter, performance -sample data pretty like -sample data, assume parameters rule set good generalization power can used live trading. session, filter stocks similar -sample -sample data. purpose, took difference Sharpe ratios in_sample out_sample. also need define threshold tolerance difference. example, take stocks difference less 20% absolute value.\ndf %>%\nfilter(Sharpe_diff < n & Sharpe_diff > -n)\nn thresholdThe momentum strategy consists buying stocks instrument trending selling . case, order sample in_Sharpe, split sample 3 tranches. first stocks taking long positions 3rd one shorts positions.df %>% arrange(desc(col))next code make split winnersThe next code make split losersFinally, combine tranches 1 3 one single objectWe generate thousands simulations portfolio weights, need generate aleatory numbers weights. long position, weights must positive short position, weight must negative. , first trancheThe function runif create random numbers apply function first tranche, runif(n, 0, 1), n number simulations want. need generate number random weights rend_win object.Regarding set.seed(42), runif generate aleatory numbers. useful take # set.seed(42) get result. everyone gets results, insert # .simplicity, generate one portfolio weights simulation, late generate .\nrunif.short position weights must negative. , 3rd tranche:Finally, combine weights.portfolio standard deviation result covariance multiplied portfolio weights.estimate covariance matrix, tickers tranches 1 3. covariance need returs tickers .filtered stocks, get returns stocks, taking returns estimated last session, :Also, filter get filtered stocks, co_allportfolio covariance\ncov(df,use=“complete.obs”)portfolio_std =cov%% weigths\n%% para multiplicar matricesBut need annualized portfolio_std\ntwe<-t(weigths)\nportfolio_std_1=(twe%%portfolio_std252)^.5The following code annualized returns in_sample data, momentum portfolio.","code":"\ndf_merge<-read.xlsx(\"data/df_merge.xlsx\",rowNames=T)\ntreh<-0.2\n\n  df_merge2<- df_merge   %>%\n  filter(Sharpe_diff < treh & Sharpe_diff > -treh)\ndf_merge2\n#>                 in_return     in_sd in_Sharpe   out_return     out_sd\n#> AAPL.Close   -0.054771548 0.2747365   -0.1994 -0.082373325 0.23299812\n#> AMZN.Close   -0.016509915 0.2788159   -0.0592 -0.035770361 0.29049652\n#> TSLA.Close    0.564098218 0.5666686    0.9955  0.532664333 0.45951725\n#> TSM.Close    -0.135791198 0.3021663   -0.4494 -0.121677232 0.22938977\n#> BAC.Close     0.027169757 0.3355819    0.0810 -0.011570730 0.20220638\n#> NSRGF.Close  -0.223354678 0.1760606   -1.2686 -0.210133138 0.15186225\n#> LVMUY.Close  -0.108547037 0.2736889   -0.3966 -0.101687079 0.24380399\n#> BAC.PK.Close  0.017871194 0.1272181    0.1405  0.001189315 0.06644728\n#> RHHBF.Close  -0.667692583 0.3264243   -2.0455 -0.660057680 0.33908716\n#> KO.Close     -0.038464295 0.2080766   -0.1849 -0.023579715 0.12021278\n#> TM.Close     -0.091717493 0.2144692   -0.4276 -0.118928664 0.21121348\n#> RYDAF.Close   0.017644987 0.3660373    0.0482  0.001445751 0.20941036\n#> SHEL.Close   -0.010895257 0.3851020   -0.0283 -0.003778604 0.17708269\n#> BHP.Close    -0.028648261 0.3246614   -0.0882 -0.029134810 0.25596044\n#> VZ.Close     -0.119001044 0.1588481   -0.7491 -0.102977092 0.12531984\n#> AZN.Close    -0.207450467 0.2140546   -0.9691 -0.193951418 0.17725594\n#> AZNCF.Close  -0.372264173 0.2180492   -1.7072 -0.346987471 0.21398241\n#> ADBE.Close   -0.044939517 0.2979994   -0.1508 -0.049712045 0.24744804\n#> NVSEF.Close  -0.095764445 0.2272564   -0.4214 -0.034737903 0.11387070\n#> DIS.Close    -0.045685926 0.2775004   -0.1646 -0.079449603 0.21852916\n#> ORCL.Close   -0.121939597 0.2788517   -0.4373 -0.095171013 0.25138906\n#> WFC.PR.Close -0.002421536 0.1492000   -0.0162  0.002284474 0.05670749\n#> TMUS.Close   -0.216877202 0.2408982   -0.9003 -0.195021363 0.18670398\n#>              out_Sharpe Sharpe_diff\n#> AAPL.Close      -0.3535      0.1541\n#> AMZN.Close      -0.1231      0.0639\n#> TSLA.Close       1.1592     -0.1637\n#> TSM.Close       -0.5304      0.0810\n#> BAC.Close       -0.0572      0.1382\n#> NSRGF.Close     -1.3837      0.1151\n#> LVMUY.Close     -0.4171      0.0205\n#> BAC.PK.Close     0.0179      0.1226\n#> RHHBF.Close     -1.9466     -0.0989\n#> KO.Close        -0.1961      0.0112\n#> TM.Close        -0.5631      0.1355\n#> RYDAF.Close      0.0069      0.0413\n#> SHEL.Close      -0.0213     -0.0070\n#> BHP.Close       -0.1138      0.0256\n#> VZ.Close        -0.8217      0.0726\n#> AZN.Close       -1.0942      0.1251\n#> AZNCF.Close     -1.6216     -0.0856\n#> ADBE.Close      -0.2009      0.0501\n#> NVSEF.Close     -0.3051     -0.1163\n#> DIS.Close       -0.3636      0.1990\n#> ORCL.Close      -0.3786     -0.0587\n#> WFC.PR.Close     0.0403     -0.0565\n#> TMUS.Close      -1.0445      0.1442\ndf_filtered<- df_merge2 %>% arrange(desc(in_Sharpe))\n# to gt the names of those stocks\nco<-rownames(df_filtered)\nle<-length(co)\nn<-round(le/3,0)  \nwin<-co[1:n] # long positions\nn\n#> [1] 8\nloss<-co[(le-n):le]\nloss # short positions\n#> [1] \"TM.Close\"    \"ORCL.Close\"  \"TSM.Close\"   \"VZ.Close\"    \"TMUS.Close\" \n#> [6] \"AZN.Close\"   \"NSRGF.Close\" \"AZNCF.Close\" \"RHHBF.Close\"\nco_all<-c(win,loss)\nco_all\n#>  [1] \"TSLA.Close\"   \"BAC.PK.Close\" \"BAC.Close\"    \"RYDAF.Close\"  \"WFC.PR.Close\"\n#>  [6] \"SHEL.Close\"   \"AMZN.Close\"   \"BHP.Close\"    \"TM.Close\"     \"ORCL.Close\"  \n#> [11] \"TSM.Close\"    \"VZ.Close\"     \"TMUS.Close\"   \"AZN.Close\"    \"NSRGF.Close\" \n#> [16] \"AZNCF.Close\"  \"RHHBF.Close\"\nw<- 1.2 # long position weight\nw_short<- 1-w \nset.seed(42)\n#runif \nru<-runif(n , 0, 1)\n# weigths sum\nsu<-sum(ru)\n# runif/sum and trasnsforming into data frame\nwe_win<-data.frame(ru*w/su)\n#colnanmes weigth\ncolnames(we_win)<-\"we\"\n# row names from win\nrownames(we_win)<-win\nru<-runif(length(loss), 0, 1)\nset.seed(42)\nsu<-sum(ru)\n# runif/sum and trasnsforming into data frame\nwe_loss<-data.frame(ru*w_short/su) \n#colnanmes weigth\ncolnames(we_loss)<-\"we\"\n# row names from loss\nrownames(we_loss)<-loss\nsum(we_loss) # set.seed(42)\n#> [1] -0.2\nwe_loss\n#>                      we\n#> TM.Close    -0.02150707\n#> ORCL.Close  -0.02308076\n#> TSM.Close   -0.01498448\n#> VZ.Close    -0.02354061\n#> TMUS.Close  -0.03059711\n#> AZN.Close   -0.00836163\n#> NSRGF.Close -0.01513346\n#> AZNCF.Close -0.03077199\n#> RHHBF.Close -0.03202288\nwe_all<-rbind(we_win,we_loss)\nwe_all\n#>                       we\n#> TSLA.Close    0.21952864\n#> BAC.PK.Close  0.22487269\n#> BAC.Close     0.06866573\n#> RYDAF.Close   0.19928491\n#> WFC.PR.Close  0.15400152\n#> SHEL.Close    0.12456895\n#> AMZN.Close    0.17676122\n#> BHP.Close     0.03231633\n#> TM.Close     -0.02150707\n#> ORCL.Close   -0.02308076\n#> TSM.Close    -0.01498448\n#> VZ.Close     -0.02354061\n#> TMUS.Close   -0.03059711\n#> AZN.Close    -0.00836163\n#> NSRGF.Close  -0.01513346\n#> AZNCF.Close  -0.03077199\n#> RHHBF.Close  -0.03202288\ndata<-read.xlsx(\"data/dfx_2.xlsx\")\ndate<-data[,1]\ndata<-data[,-1]\ndatax<- xts(data,\n         order.by = as.Date(date))\ndatax<-na.omit(datax)\nret<-apply(datax,2,Delt)\nretx<- xts(ret,\n         order.by = as.Date(date))\nretx<-na.omit(retx)\nhead(retx[,1:5])\n#>              AAPL.Close   MSFT.Close    GOOG.Close  GOOGL.Close   AMZN.Close\n#> 2020-01-03 -0.009722044 -0.012451750 -0.0049072022 -0.005231342 -0.012139050\n#> 2020-01-06  0.007968248  0.002584819  0.0246570974  0.026654062  0.014885590\n#> 2020-01-07 -0.004703042 -0.009117758 -0.0006240057 -0.001931646  0.002091556\n#> 2020-01-08  0.016086289  0.015928379  0.0078803309  0.007117757 -0.007808656\n#> 2020-01-09  0.021240806  0.012492973  0.0110444988  0.010497921  0.004799272\n#> 2020-01-10  0.002260711 -0.004627059  0.0069726829  0.006458647 -0.009410597\nretx_all<-retx[,co_all]\ncovar<-cov(retx_all,use=\"complete.obs\")\nportfolio_std =covar %*% we_all[,1]\nportfolio_std\n#>                      [,1]\n#> TSLA.Close   7.294191e-04\n#> BAC.PK.Close 1.071041e-04\n#> BAC.Close    3.251108e-04\n#> RYDAF.Close  4.231321e-04\n#> WFC.PR.Close 1.213717e-04\n#> SHEL.Close   4.451261e-04\n#> AMZN.Close   2.318799e-04\n#> BHP.Close    3.398317e-04\n#> TM.Close     1.541582e-04\n#> ORCL.Close   1.497664e-04\n#> TSM.Close    2.662127e-04\n#> VZ.Close     6.751342e-05\n#> TMUS.Close   2.029555e-04\n#> AZN.Close    1.142982e-04\n#> NSRGF.Close  9.421821e-05\n#> AZNCF.Close  7.306948e-05\n#> RHHBF.Close  8.447179e-05\ntwe<-t(we_all[,1])\nportfolio_std_1=(twe%*%portfolio_std*252)^.5\nret_a<-df_merge2[co_all,1]\n\nret_a_f<-twe %*%ret_a \nret_a_f\n#>           [,1]\n#> [1,] 0.1818727"},{"path":"portfolio-management-algorithms.html","id":"graphs-of-the-results","chapter":"9 Portfolio management algorithms","heading":"9.0.1 Graphs of the results","text":"Pending","code":""},{"path":"portfolio-management-algorithms.html","id":"bibliography-3","chapter":"9 Portfolio management algorithms","heading":"9.1 Bibliography","text":"Cervantes, M., Montoya, M. Á., & Bernal Ponce, L. . (2016). Effect Business Cycle Investment Strategies: Evidence Mexico. Revista Mexicana de Economía y Finanzas, 11(2).Hilpisch (2019). Python Finance, 2nd Edition. O’Reilly Media, Inc. Sebastopol, CA.","code":""}]
