[{"path":"index.html","id":"algorithms-and-financial-programing-in-r","chapter":"Algorithms and Financial programing in R","heading":"Algorithms and Financial programing in R","text":"website Algorithms Financial programing R! Visit GitHub repository site, find book O’Reilly, buy Amazon.work Aturo Bernal licensed Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States License. ","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"work analytics data science, like , familiar fact data generated time ever faster rates. (may even little weary people pontificating fact.) Analysts often trained handle tabular rectangular data mostly numeric, much data proliferating today unstructured text-heavy. Many us work analytical fields trained even simple interpretation natural language.developed tidytext (Silge Robinson 2016) R package familiar many methods data wrangling visualization, couldn’t easily apply methods text. found using tidy data principles can make many text mining tasks easier, effective, consistent tools already wide use. Treating text data frames individual words allows us manipulate, summarize, visualize characteristics text easily integrate natural language processing effective workflows already using.book serves introduction text mining using tidytext package tidy tools R. functions provided tidytext package relatively simple; important possible applications. Thus, book provides compelling examples real text mining problems.","code":""},{"path":"preface.html","id":"outline","chapter":"Preface","heading":"Outline","text":"start introducing tidy text format, ways dplyr, tidyr, tidytext allow informative analyses structure.Chapter 1 outlines tidy text format unnest_tokens() function. also introduces gutenbergr janeaustenr packages, provide useful literary text datasets ’ll use throughout book.Chapter 2 shows perform sentiment analysis tidy text dataset, using sentiments dataset tidytext inner_join() dplyr.Chapter 3 describes tf-idf statistic (term frequency times inverse document frequency), quantity used identifying terms especially important particular document.Chapter 4 introduces n-grams analyze word networks text using widyr ggraph packages.Text won’t tidy stages analysis, important able convert back forth tidy non-tidy formats.Chapter 5 introduces methods tidying document-term matrices corpus objects tm quanteda packages, well casting tidy text datasets formats.Chapter 6 explores concept topic modeling, uses tidy() method interpret visualize output topicmodels package.conclude several case studies bring together multiple tidy text mining approaches ’ve learned.Chapter 7 demonstrates application tidy text analysis analyzing authors’ Twitter archives. Dave’s Julia’s tweeting habits compare?Chapter 8 explores metadata 32,000 NASA datasets (available JSON) looking keywords datasets connected title description fields.Chapter 9 analyzes dataset Usenet messages diverse set newsgroups (focused topics like politics, hockey, technology, atheism, ) understand patterns across groups.","code":""},{"path":"preface.html","id":"topics-this-book-does-not-cover","chapter":"Preface","heading":"Topics this book does not cover","text":"book serves introduction tidy text mining framework along collection examples, far complete exploration natural language processing. CRAN Task View Natural Language Processing provides details ways use R computational linguistics. several areas may want explore detail according needs.Clustering, classification, prediction: Machine learning text vast topic easily fill volume. introduce one method unsupervised clustering (topic modeling) Chapter 6 many machine learning algorithms can used dealing text.Word embedding: One popular modern approach text analysis map words vector representations, can used examine linguistic relationships words classify text. representations words tidy sense consider , found powerful applications machine learning algorithms.complex tokenization: tidytext package trusts tokenizers package (Mullen 2016) perform tokenization, wraps variety tokenizers consistent interface, many others exist specific applications.Languages English: users success applying tidytext text mining needs languages English, don’t cover examples book.","code":""},{"path":"preface.html","id":"about-this-book","chapter":"Preface","heading":"About this book","text":"book focused practical software examples data explorations. equations, great deal code. especially focus generating real insights literature, news, social media analyze.don’t assume previous knowledge text mining. Professional linguists text analysts likely find examples elementary, though confident can build framework analyses.assume reader least slightly familiar dplyr, ggplot2, %>% “pipe” operator R, interested applying tools text data. users don’t background, recommend books R Data Science. believe basic background interest tidy data, even user early R career can understand apply examples.","code":""},{"path":"preface.html","id":"using-code-examples","chapter":"Preface","heading":"Using code examples","text":"book written RStudio using bookdown. website hosted via Netlify, automatically built every push GitHub Actions. show code behind vast majority analyses, interest space sometimes choose show code generating particular visualization ’ve already provided code several similar graphs. trust reader can learn build examples, code used generate book can found public GitHub repository. generated plots book using ggplot2 light theme (theme_light()).version book built R version 4.2.0 (2022-04-22 ucrt) following packages:","code":""},{"path":"preface.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"thankful contributions, help, perspectives people moved us forward project. several people organizations like thank particular.like thank Os Keyes Gabriela de Queiroz contributions tidytext package, Lincoln Mullen work tokenizers package, Kenneth Benoit work quanteda package, Thomas Pedersen work ggraph package, Hadley Wickham work framing tidy data principles building tidy tools. also like thank rOpenSci, hosted us unconference began work, NASA Datanauts program, opportunities support provided Julia time .received thoughtful, thorough technical reviews improved quality book significantly. like thank Mara Averick, Carolyn Clayton, Simon Jackson, Sean Kross, Lincoln Mullen investment time energy technical reviews.book written open, several people contributed via pull requests issues. Special thanks goes contributed via GitHub pull requests (alphabetical order username): Alper Yilmaz (@alperyilmaz), Alison Presmanes Hill (@apreshill), Christoph Molnar (@christophM), Denis Maciel (@denismaciel), Greg Botwin (@greg-botwin), Halian Vilela (@halian-vilela), Ilari Scheinin (@ilarischeinin), J.J. Allaire (@jjallaire), Jamie Lendrum (@jl5000), Jon Calder (@jonmcalder), Kanishka (@kanishkamisra), Luis de Sousa (@luisdza), Mark Beveridge (@mbeveridge), Matthew Henderson (@MHenderson), Michael Chirico (@MichaelChirico), Nina Jakobsen (@nmjakobsen), Timothy James Dobbins (@tmthyjames), Yihui Xie (@yihui), Yu-Wen Pu (@yuwen41200).Finally, want dedicate book spouses, Robert Dana. produce great deal sentimental text subject restrict heartfelt thanks.","code":""},{"path":"tidytext.html","id":"tidytext","chapter":"1 The tidy text format","heading":"1 The tidy text format","text":"Using tidy data principles powerful way make handling data easier effective, less true comes dealing text. described Hadley Wickham (Wickham 2014), tidy data specific structure:variable columnEach observation rowEach type observational unit tableWe thus define tidy text format table one-token-per-row. token meaningful unit text, word, interested using analysis, tokenization process splitting text tokens. one-token-per-row structure contrast ways text often stored current analyses, perhaps strings document-term matrix. tidy text mining, token stored row often single word, can also n-gram, sentence, paragraph. tidytext package, provide functionality tokenize commonly used units text like convert one-term-per-row format.Tidy data sets allow manipulation standard set “tidy” tools, including popular packages dplyr (Wickham Francois 2016), tidyr (Wickham 2016), ggplot2 (Wickham 2009), broom (Robinson 2017). keeping input output tidy tables, users can transition fluidly packages. ’ve found tidy tools extend naturally many text analyses explorations.time, tidytext package doesn’t expect user keep text data tidy form times analysis. package includes functions tidy() objects (see broom package [Robinson et al cited ]) popular text mining R packages tm (Feinerer, Hornik, Meyer 2008) quanteda (Benoit Nulty 2016). allows, example, workflow importing, filtering, processing done using dplyr tidy tools, data converted document-term matrix machine learning applications. models can re-converted tidy form interpretation visualization ggplot2.","code":""},{"path":"tidytext.html","id":"contrasting-tidy-text-with-other-data-structures","chapter":"1 The tidy text format","heading":"1.1 Contrasting tidy text with other data structures","text":"stated , define tidy text format table one-token-per-row. Structuring text data way means conforms tidy data principles can manipulated set consistent tools. worth contrasting ways text often stored text mining approaches.String: Text can, course, stored strings, .e., character vectors, within R, often text data first read memory form.Corpus: types objects typically contain raw strings annotated additional metadata details.Document-term matrix: sparse matrix describing collection (.e., corpus) documents one row document one column term. value matrix typically word count tf-idf (see Chapter 3).Let’s hold exploring corpus document-term matrix objects Chapter 5, get basics converting text tidy format.","code":""},{"path":"tidytext.html","id":"the-unnest_tokens-function","chapter":"1 The tidy text format","heading":"1.2 The unnest_tokens function","text":"Emily Dickinson wrote lovely text time.typical character vector might want analyze. order turn tidy text dataset, first need put data frame.mean data frame printed “tibble”? tibble modern class data frame within R, available dplyr tibble packages, convenient print method, convert strings factors, use row names. Tibbles great use tidy tools.Notice data frame containing text isn’t yet compatible tidy text analysis, though. can’t filter words count occur frequently, since row made multiple combined words. need convert one-token-per-document-per-row.\ntoken meaningful unit text, often word, \ninterested using analysis, tokenization \nprocess splitting text tokens.\nfirst example, one document (poem), explore examples multiple documents soon.Within tidy text framework, need break text individual tokens (process called tokenization) transform tidy data structure. , use tidytext’s unnest_tokens() function.two basic arguments unnest_tokens used column names. First output column name created text unnested (word, case), input column text comes (text, case). Remember text_df column called text contains data interest.using unnest_tokens, ’ve split row one token (word) row new data frame; default tokenization unnest_tokens() single words, shown . Also notice:columns, line number word came , retained.Punctuation stripped.default, unnest_tokens() converts tokens lowercase, makes easier compare combine datasets. (Use to_lower = FALSE argument turn behavior).text data format lets us manipulate, process, visualize text using standard set tidy tools, namely dplyr, tidyr, ggplot2, shown Figure 1.1.\nFigure 1.1: flowchart typical text analysis using tidy data principles. chapter shows summarize visualize text using tools.\n","code":"\ntext <- c(\"Because I could not stop for Death -\",\n          \"He kindly stopped for me -\",\n          \"The Carriage held but just Ourselves -\",\n          \"and Immortality\")\n\ntext\n#> [1] \"Because I could not stop for Death -\"  \n#> [2] \"He kindly stopped for me -\"            \n#> [3] \"The Carriage held but just Ourselves -\"\n#> [4] \"and Immortality\"\nlibrary(dplyr)\ntext_df <- tibble(line = 1:4, text = text)\n\ntext_df\n#> # A tibble: 4 × 2\n#>    line text                                  \n#>   <int> <chr>                                 \n#> 1     1 Because I could not stop for Death -  \n#> 2     2 He kindly stopped for me -            \n#> 3     3 The Carriage held but just Ourselves -\n#> 4     4 and Immortality\nlibrary(tidytext)\n\ntext_df %>%\n  unnest_tokens(word, text)\n#> # A tibble: 20 × 2\n#>     line word   \n#>    <int> <chr>  \n#>  1     1 because\n#>  2     1 i      \n#>  3     1 could  \n#>  4     1 not    \n#>  5     1 stop   \n#>  6     1 for    \n#>  7     1 death  \n#>  8     2 he     \n#>  9     2 kindly \n#> 10     2 stopped\n#> # … with 10 more rows"},{"path":"tidytext.html","id":"tidyausten","chapter":"1 The tidy text format","heading":"1.3 Tidying the works of Jane Austen","text":"Let’s use text Jane Austen’s 6 completed, published novels janeaustenr package (Silge 2016), transform tidy format. janeaustenr package provides texts one-row-per-line format, line context analogous literal printed line physical book. Let’s start , also use mutate() annotate linenumber quantity keep track lines original format chapter (using regex) find chapters .work tidy dataset, need restructure one-token-per-row format, saw earlier done unnest_tokens() function.function uses tokenizers package separate line text original data frame tokens. default tokenizing words, options include characters, n-grams, sentences, lines, paragraphs, separation around regex pattern.Now data one-word-per-row format, can manipulate tidy tools like dplyr. Often text analysis, want remove stop words; stop words words useful analysis, typically extremely common words “”, “”, “”, forth English. can remove stop words (kept tidytext dataset stop_words) anti_join().stop_words dataset tidytext package contains stop words three lexicons. can use together, , filter() use one set stop words appropriate certain analysis.can also use dplyr’s count() find common words books whole.’ve using tidy tools, word counts stored tidy data frame. allows us pipe directly ggplot2 package, example create visualization common words (Figure 1.2).\nFigure 1.2: common words Jane Austen’s novels\nNote austen_books() function started us exactly text wanted analyze, cases may need perform cleaning text data, removing copyright headers formatting. ’ll see examples kind pre-processing case study chapters, particularly Chapter 9.1.1.","code":"\nlibrary(janeaustenr)\nlibrary(dplyr)\nlibrary(stringr)\n\noriginal_books <- austen_books() %>%\n  group_by(book) %>%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, \n                                     regex(\"^chapter [\\\\divxlc]\",\n                                           ignore_case = TRUE)))) %>%\n  ungroup()\n\noriginal_books\n#> # A tibble: 73,422 × 4\n#>    text                    book                linenumber chapter\n#>    <chr>                   <fct>                    <int>   <int>\n#>  1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n#>  2 \"\"                      Sense & Sensibility          2       0\n#>  3 \"by Jane Austen\"        Sense & Sensibility          3       0\n#>  4 \"\"                      Sense & Sensibility          4       0\n#>  5 \"(1811)\"                Sense & Sensibility          5       0\n#>  6 \"\"                      Sense & Sensibility          6       0\n#>  7 \"\"                      Sense & Sensibility          7       0\n#>  8 \"\"                      Sense & Sensibility          8       0\n#>  9 \"\"                      Sense & Sensibility          9       0\n#> 10 \"CHAPTER 1\"             Sense & Sensibility         10       1\n#> # … with 73,412 more rows\nlibrary(tidytext)\ntidy_books <- original_books %>%\n  unnest_tokens(word, text)\n\ntidy_books\n#> # A tibble: 725,055 × 4\n#>    book                linenumber chapter word       \n#>    <fct>                    <int>   <int> <chr>      \n#>  1 Sense & Sensibility          1       0 sense      \n#>  2 Sense & Sensibility          1       0 and        \n#>  3 Sense & Sensibility          1       0 sensibility\n#>  4 Sense & Sensibility          3       0 by         \n#>  5 Sense & Sensibility          3       0 jane       \n#>  6 Sense & Sensibility          3       0 austen     \n#>  7 Sense & Sensibility          5       0 1811       \n#>  8 Sense & Sensibility         10       1 chapter    \n#>  9 Sense & Sensibility         10       1 1          \n#> 10 Sense & Sensibility         13       1 the        \n#> # … with 725,045 more rows\ndata(stop_words)\n\ntidy_books <- tidy_books %>%\n  anti_join(stop_words)\ntidy_books %>%\n  count(word, sort = TRUE) \n#> # A tibble: 13,914 × 2\n#>    word       n\n#>    <chr>  <int>\n#>  1 miss    1855\n#>  2 time    1337\n#>  3 fanny    862\n#>  4 dear     822\n#>  5 lady     817\n#>  6 sir      806\n#>  7 day      797\n#>  8 emma     787\n#>  9 sister   727\n#> 10 house    699\n#> # … with 13,904 more rows\nlibrary(ggplot2)\n\ntidy_books %>%\n  count(word, sort = TRUE) %>%\n  filter(n > 600) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)"},{"path":"tidytext.html","id":"the-gutenbergr-package","chapter":"1 The tidy text format","heading":"1.4 The gutenbergr package","text":"Now ’ve used janeaustenr package explore tidying text, let’s introduce gutenbergr package (Robinson 2016). gutenbergr package provides access public domain works Project Gutenberg collection. package includes tools downloading books (stripping unhelpful header/footer information), complete dataset Project Gutenberg metadata can used find works interest. book, mostly use function gutenberg_download() downloads one works Project Gutenberg ID, can also use functions explore metadata, pair Gutenberg ID title, author, language, etc., gather information authors.\nlearn gutenbergr, check package’s documentation \nrOpenSci, one rOpenSci’s packages data\naccess.\n","code":""},{"path":"tidytext.html","id":"word-frequencies","chapter":"1 The tidy text format","heading":"1.5 Word frequencies","text":"common task text mining look word frequencies, just like done Jane Austen’s novels, compare frequencies across different texts. can intuitively smoothly using tidy data principles. already Jane Austen’s works; let’s get two sets texts compare . First, let’s look science fiction fantasy novels H.G. Wells, lived late 19th early 20th centuries. Let’s get Time Machine, War Worlds, Invisible Man, Island Doctor Moreau. can access works using gutenberg_download() Project Gutenberg ID numbers novel.Just kicks, common words novels H.G. Wells?Now let’s get well-known works Brontë sisters, whose lives overlapped Jane Austen’s somewhat wrote rather different style. Let’s get Jane Eyre, Wuthering Heights, Tenant Wildfell Hall, Villette, Agnes Grey. use Project Gutenberg ID numbers novel access texts using gutenberg_download().common words novels Brontë sisters?Interesting “time”, “eyes”, “hand” top 10 H.G. Wells Brontë sisters.Now, let’s calculate frequency word works Jane Austen, Brontë sisters, H.G. Wells binding data frames together. can use pivot_wider() pivot_longer() tidyr reshape dataframe just need plotting comparing three sets novels.use str_extract() UTF-8 encoded texts Project Gutenberg examples words underscores around indicate emphasis (like italics). tokenizer treated words, don’t want count “_any_” separately “” saw initial data exploration choosing use str_extract().Now let’s plot (Figure 1.3).\nFigure 1.3: Comparing word frequencies Jane Austen, Brontë sisters, H.G. Wells\nWords close line plots similar frequencies sets texts, example, Austen Brontë texts (“miss”, “time”, “day” upper frequency end) Austen Wells texts (“time”, “day”, “brother” high frequency end). Words far line words found one set texts another. example, Austen-Brontë panel, words like “elizabeth”, “emma”, “fanny” (proper nouns) found Austen’s texts much Brontë texts, words like “arthur” “dog” found Brontë texts Austen texts. comparing H.G. Wells Jane Austen, Wells uses words like “beast”, “guns”, “feet”, “black” Austen , Austen uses words like “family”, “friend”, “letter”, “dear” Wells .Overall, notice Figure 1.3 words Austen-Brontë panel closer zero-slope line Austen-Wells panel. Also notice words extend lower frequencies Austen-Brontë panel; empty space Austen-Wells panel low frequency. characteristics indicate Austen Brontë sisters use similar words Austen H.G. Wells. Also, see words found three sets texts fewer data points panel Austen H.G. Wells.Let’s quantify similar different sets word frequencies using correlation test. correlated word frequencies Austen Brontë sisters, Austen Wells?Just saw plots, word frequencies correlated Austen Brontë novels Austen H.G. Wells.","code":"\nlibrary(gutenbergr)\n\nhgwells <- gutenberg_download(c(35, 36, 5230, 159))\ntidy_hgwells <- hgwells %>%\n  unnest_tokens(word, text) %>%\n  anti_join(stop_words)\ntidy_hgwells %>%\n  count(word, sort = TRUE)\n#> # A tibble: 11,769 × 2\n#>    word       n\n#>    <chr>  <int>\n#>  1 time     454\n#>  2 people   302\n#>  3 door     260\n#>  4 heard    249\n#>  5 black    232\n#>  6 stood    229\n#>  7 white    222\n#>  8 hand     218\n#>  9 kemp     213\n#> 10 eyes     210\n#> # … with 11,759 more rows\nbronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))\ntidy_bronte <- bronte %>%\n  unnest_tokens(word, text) %>%\n  anti_join(stop_words)\ntidy_bronte %>%\n  count(word, sort = TRUE)\n#> # A tibble: 23,051 × 2\n#>    word       n\n#>    <chr>  <int>\n#>  1 time    1065\n#>  2 miss     855\n#>  3 day      827\n#>  4 hand     768\n#>  5 eyes     713\n#>  6 night    647\n#>  7 heart    638\n#>  8 looked   602\n#>  9 door     592\n#> 10 half     586\n#> # … with 23,041 more rows\nlibrary(tidyr)\n\nfrequency <- bind_rows(mutate(tidy_bronte, author = \"Brontë Sisters\"),\n                       mutate(tidy_hgwells, author = \"H.G. Wells\"), \n                       mutate(tidy_books, author = \"Jane Austen\")) %>% \n  mutate(word = str_extract(word, \"[a-z']+\")) %>%\n  count(author, word) %>%\n  group_by(author) %>%\n  mutate(proportion = n / sum(n)) %>% \n  select(-n) %>% \n  pivot_wider(names_from = author, values_from = proportion) %>%\n  pivot_longer(`Brontë Sisters`:`H.G. Wells`,\n               names_to = \"author\", values_to = \"proportion\")\n\nfrequency\n#> # A tibble: 57,820 × 4\n#>    word    `Jane Austen` author          proportion\n#>    <chr>           <dbl> <chr>                <dbl>\n#>  1 a          0.00000919 Brontë Sisters  0.0000319 \n#>  2 a          0.00000919 H.G. Wells      0.0000150 \n#>  3 a'most    NA          Brontë Sisters  0.0000159 \n#>  4 a'most    NA          H.G. Wells     NA         \n#>  5 aback     NA          Brontë Sisters  0.00000398\n#>  6 aback     NA          H.G. Wells      0.0000150 \n#>  7 abaht     NA          Brontë Sisters  0.00000398\n#>  8 abaht     NA          H.G. Wells     NA         \n#>  9 abandon   NA          Brontë Sisters  0.0000319 \n#> 10 abandon   NA          H.G. Wells      0.0000150 \n#> # … with 57,810 more rows\nlibrary(scales)\n\n# expect a warning about rows with missing values being removed\nggplot(frequency, aes(x = proportion, y = `Jane Austen`, \n                      color = abs(`Jane Austen` - proportion))) +\n  geom_abline(color = \"gray40\", lty = 2) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  scale_color_gradient(limits = c(0, 0.001), \n                       low = \"darkslategray4\", high = \"gray75\") +\n  facet_wrap(~author, ncol = 2) +\n  theme(legend.position=\"none\") +\n  labs(y = \"Jane Austen\", x = NULL)\ncor.test(data = frequency[frequency$author == \"Brontë Sisters\",],\n         ~ proportion + `Jane Austen`)\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  proportion and Jane Austen\n#> t = 119.64, df = 10404, p-value < 2.2e-16\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.7527837 0.7689611\n#> sample estimates:\n#>       cor \n#> 0.7609907\ncor.test(data = frequency[frequency$author == \"H.G. Wells\",], \n         ~ proportion + `Jane Austen`)\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  proportion and Jane Austen\n#> t = 36.441, df = 6053, p-value < 2.2e-16\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.4032820 0.4446006\n#> sample estimates:\n#>      cor \n#> 0.424162"},{"path":"tidytext.html","id":"summary","chapter":"1 The tidy text format","heading":"1.6 Summary","text":"chapter, explored mean tidy data comes text, tidy data principles can applied natural language processing. text organized format one token per row, tasks like removing stop words calculating word frequencies natural applications familiar operations within tidy tool ecosystem. one-token-per-row framework can extended single words n-grams meaningful units text, well many analysis priorities consider book.","code":""},{"path":"sentiment.html","id":"sentiment","chapter":"2 Sentiment analysis with tidy data","heading":"2 Sentiment analysis with tidy data","text":"previous chapter, explored depth mean tidy text format showed format can used approach questions word frequency. allowed us analyze words used frequently documents compare documents, now let’s investigate different topic. Let’s address topic opinion mining sentiment analysis. human readers approach text, use understanding emotional intent words infer whether section text positive negative, perhaps characterized nuanced emotion like surprise disgust. can use tools text mining approach emotional content text programmatically, shown Figure 2.1.\nFigure 2.1: flowchart typical text analysis uses tidytext sentiment analysis. chapter shows implement sentiment analysis using tidy data principles.\nOne way analyze sentiment text consider text combination individual words sentiment content whole text sum sentiment content individual words. isn’t way approach sentiment analysis, often-used approach, approach naturally takes advantage tidy tool ecosystem.","code":""},{"path":"sentiment.html","id":"the-sentiments-datasets","chapter":"2 Sentiment analysis with tidy data","heading":"2.1 The sentiments datasets","text":"discussed , variety methods dictionaries exist evaluating opinion emotion text. tidytext package provides access several sentiment lexicons. Three general-purpose lexicons areAFINN Finn Årup Nielsen,bing Bing Liu collaborators, andnrc Saif Mohammad Peter Turney.three lexicons based unigrams, .e., single words. lexicons contain many English words words assigned scores positive/negative sentiment, also possibly emotions like joy, anger, sadness, forth. nrc lexicon categorizes words binary fashion (“yes”/“”) categories positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, trust. bing lexicon categorizes words binary fashion positive negative categories. AFINN lexicon assigns words score runs -5 5, negative scores indicating negative sentiment positive scores indicating positive sentiment.\nlexicons available different licenses, sure\nlicense lexicon want use appropriate \nproject. may asked agree license downloading\ndata.\nfunction get_sentiments() allows us get specific sentiment lexicons appropriate measures one.sentiment lexicons put together validated? constructed via either crowdsourcing (using, example, Amazon Mechanical Turk) labor one authors, validated using combination crowdsourcing , restaurant movie reviews, Twitter data. Given information, may hesitate apply sentiment lexicons styles text dramatically different validated , narrative fiction 200 years ago. true using sentiment lexicons , example, Jane Austen’s novels may give us less accurate results tweets sent contemporary writer, still can measure sentiment content words shared across lexicon text.also domain-specific sentiment lexicons available, constructed used text specific content area. Section 5.3.1 explores analysis using sentiment lexicon specifically finance.\nDictionary-based methods like ones discussing find \ntotal sentiment piece text adding individual sentiment\nscores word text.\nevery English word lexicons many English words pretty neutral. important keep mind methods take account qualifiers word, “good” “true”; lexicon-based method like based unigrams . many kinds text (like narrative examples ), sustained sections sarcasm negated text, important effect. Also, can use tidy text approach begin understand kinds negation words important given text; see Chapter 9 extended example analysis.One last caveat size chunk text use add unigram sentiment scores can effect analysis. text size many paragraphs can often positive negative sentiment averaged zero, sentence-sized paragraph-sized text often works better.","code":"\nlibrary(tidytext)\n\nget_sentiments(\"afinn\")#> # A tibble: 2,477 × 2\n#>    word       value\n#>    <chr>      <dbl>\n#>  1 abandon       -2\n#>  2 abandoned     -2\n#>  3 abandons      -2\n#>  4 abducted      -2\n#>  5 abduction     -2\n#>  6 abductions    -2\n#>  7 abhor         -3\n#>  8 abhorred      -3\n#>  9 abhorrent     -3\n#> 10 abhors        -3\n#> # … with 2,467 more rows\nget_sentiments(\"bing\")\n#> # A tibble: 6,786 × 2\n#>    word        sentiment\n#>    <chr>       <chr>    \n#>  1 2-faces     negative \n#>  2 abnormal    negative \n#>  3 abolish     negative \n#>  4 abominable  negative \n#>  5 abominably  negative \n#>  6 abominate   negative \n#>  7 abomination negative \n#>  8 abort       negative \n#>  9 aborted     negative \n#> 10 aborts      negative \n#> # … with 6,776 more rows\nget_sentiments(\"nrc\")#> # A tibble: 13,901 × 2\n#>    word        sentiment\n#>    <chr>       <chr>    \n#>  1 abacus      trust    \n#>  2 abandon     fear     \n#>  3 abandon     negative \n#>  4 abandon     sadness  \n#>  5 abandoned   anger    \n#>  6 abandoned   fear     \n#>  7 abandoned   negative \n#>  8 abandoned   sadness  \n#>  9 abandonment anger    \n#> 10 abandonment fear     \n#> # … with 13,891 more rows"},{"path":"sentiment.html","id":"sentiment-analysis-with-inner-join","chapter":"2 Sentiment analysis with tidy data","heading":"2.2 Sentiment analysis with inner join","text":"data tidy format, sentiment analysis can done inner join. another great successes viewing text mining tidy data analysis task; much removing stop words antijoin operation, performing sentiment analysis inner join operation.Let’s look words joy score NRC lexicon. common joy words Emma? First, need take text novels convert text tidy format using unnest_tokens(), just Section 1.3. Let’s also set columns keep track line chapter book word comes ; use group_by mutate construct columns.Notice chose name word output column unnest_tokens(). convenient choice sentiment lexicons stop word datasets columns named word; performing inner joins anti-joins thus easier.Now text tidy format one word per row, ready sentiment analysis. First, let’s use NRC lexicon filter() joy words. Next, let’s filter() data frame text books words Emma use inner_join() perform sentiment analysis. common joy words Emma? Let’s use count() dplyr.see mostly positive, happy words hope, friendship, love . also see words may used joyfully Austen (“found”, “present”); discuss detail Section 2.4.can also examine sentiment changes throughout novel. can just handful lines mostly dplyr functions. First, find sentiment score word using Bing lexicon inner_join().Next, count many positive negative words defined sections book. define index keep track narrative; index (using integer division) counts sections 80 lines text.\n%/% operator integer division\n(x %/% y equivalent floor(x/y)) \nindex keeps track 80-line section text counting \nnegative positive sentiment .\nSmall sections text may enough words get good estimate sentiment really large sections can wash narrative structure. books, using 80 lines works well, can vary depending individual texts, long lines start , etc. use pivot_wider() negative positive sentiment separate columns, lastly calculate net sentiment (positive - negative).Now can plot sentiment scores across plot trajectory novel. Notice plotting index x-axis keeps track narrative time sections text.\nFigure 2.2: Sentiment narratives Jane Austen’s novels\ncan see Figure 2.2 plot novel changes toward positive negative sentiment trajectory story.","code":"\nlibrary(janeaustenr)\nlibrary(dplyr)\nlibrary(stringr)\n\ntidy_books <- austen_books() %>%\n  group_by(book) %>%\n  mutate(\n    linenumber = row_number(),\n    chapter = cumsum(str_detect(text, \n                                regex(\"^chapter [\\\\divxlc]\", \n                                      ignore_case = TRUE)))) %>%\n  ungroup() %>%\n  unnest_tokens(word, text)\nnrc_joy <- get_sentiments(\"nrc\") %>% \n  filter(sentiment == \"joy\")\n\ntidy_books %>%\n  filter(book == \"Emma\") %>%\n  inner_join(nrc_joy) %>%\n  count(word, sort = TRUE)#> # A tibble: 303 × 2\n#>    word        n\n#>    <chr>   <int>\n#>  1 good      359\n#>  2 young     192\n#>  3 friend    166\n#>  4 hope      143\n#>  5 happy     125\n#>  6 love      117\n#>  7 deal       92\n#>  8 found      92\n#>  9 present    89\n#> 10 kind       82\n#> # … with 293 more rows\nlibrary(tidyr)\n\njane_austen_sentiment <- tidy_books %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(book, index = linenumber %/% 80, sentiment) %>%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% \n  mutate(sentiment = positive - negative)\nlibrary(ggplot2)\n\nggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~book, ncol = 2, scales = \"free_x\")"},{"path":"sentiment.html","id":"comparing-the-three-sentiment-dictionaries","chapter":"2 Sentiment analysis with tidy data","heading":"2.3 Comparing the three sentiment dictionaries","text":"several options sentiment lexicons, might want information one appropriate purposes. Let’s use three sentiment lexicons examine sentiment changes across narrative arc Pride Prejudice. First, let’s use filter() choose words one novel interested .Now, can use inner_join() calculate sentiment different ways.\nRemember AFINN lexicon measures sentiment \nnumeric score -5 5, two lexicons categorize\nwords binary fashion, either positive negative. find \nsentiment score chunks text throughout novel, need \nuse different pattern AFINN lexicon \ntwo.\nLet’s use integer division (%/%) define larger sections text span multiple lines, can use pattern count(), pivot_wider(), mutate() find net sentiment sections text.now estimate net sentiment (positive - negative) chunk novel text sentiment lexicon. Let’s bind together visualize Figure 2.3.\nFigure 2.3: Comparing three sentiment lexicons using Pride Prejudice\nthree different lexicons calculating sentiment give results different absolute sense similar relative trajectories novel. see similar dips peaks sentiment places novel, absolute values significantly different. AFINN lexicon\ngives largest absolute values, high positive values. lexicon Bing et al. lower absolute values seems label larger blocks contiguous positive negative text. NRC results shifted higher relative two, labeling text positively, detects similar relative changes text. find similar differences methods looking novels; NRC sentiment high, AFINN sentiment variance, Bing et al. sentiment appears find longer stretches similar text, three agree roughly overall trends sentiment narrative arc., example, result NRC lexicon biased high sentiment compared Bing et al. result? Let’s look briefly many positive negative words lexicons.lexicons negative positive words, ratio negative positive words higher Bing lexicon NRC lexicon. contribute effect see plot , systematic difference word matches, e.g. negative words NRC lexicon match words Jane Austen uses well. Whatever source differences, see similar relative trajectories across narrative arc, similar changes slope, marked differences absolute sentiment lexicon lexicon. important context keep mind choosing sentiment lexicon analysis.","code":"\npride_prejudice <- tidy_books %>% \n  filter(book == \"Pride & Prejudice\")\n\npride_prejudice\n#> # A tibble: 122,204 × 4\n#>    book              linenumber chapter word     \n#>    <fct>                  <int>   <int> <chr>    \n#>  1 Pride & Prejudice          1       0 pride    \n#>  2 Pride & Prejudice          1       0 and      \n#>  3 Pride & Prejudice          1       0 prejudice\n#>  4 Pride & Prejudice          3       0 by       \n#>  5 Pride & Prejudice          3       0 jane     \n#>  6 Pride & Prejudice          3       0 austen   \n#>  7 Pride & Prejudice          7       1 chapter  \n#>  8 Pride & Prejudice          7       1 1        \n#>  9 Pride & Prejudice         10       1 it       \n#> 10 Pride & Prejudice         10       1 is       \n#> # … with 122,194 more rows\nafinn <- pride_prejudice %>% \n  inner_join(get_sentiments(\"afinn\")) %>% \n  group_by(index = linenumber %/% 80) %>% \n  summarise(sentiment = sum(value)) %>% \n  mutate(method = \"AFINN\")\n\nbing_and_nrc <- bind_rows(\n  pride_prejudice %>% \n    inner_join(get_sentiments(\"bing\")) %>%\n    mutate(method = \"Bing et al.\"),\n  pride_prejudice %>% \n    inner_join(get_sentiments(\"nrc\") %>% \n                 filter(sentiment %in% c(\"positive\", \n                                         \"negative\"))\n    ) %>%\n    mutate(method = \"NRC\")) %>%\n  count(method, index = linenumber %/% 80, sentiment) %>%\n  pivot_wider(names_from = sentiment,\n              values_from = n,\n              values_fill = 0) %>% \n  mutate(sentiment = positive - negative)\nbind_rows(afinn, \n          bing_and_nrc) %>%\n  ggplot(aes(index, sentiment, fill = method)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~method, ncol = 1, scales = \"free_y\")\nget_sentiments(\"nrc\") %>% \n  filter(sentiment %in% c(\"positive\", \"negative\")) %>% \n  count(sentiment)#> # A tibble: 2 × 2\n#>   sentiment     n\n#>   <chr>     <int>\n#> 1 negative   3324\n#> 2 positive   2312\nget_sentiments(\"bing\") %>% \n  count(sentiment)\n#> # A tibble: 2 × 2\n#>   sentiment     n\n#>   <chr>     <int>\n#> 1 negative   4781\n#> 2 positive   2005"},{"path":"sentiment.html","id":"most-positive-negative","chapter":"2 Sentiment analysis with tidy data","heading":"2.4 Most common positive and negative words","text":"One advantage data frame sentiment word can analyze word counts contribute sentiment. implementing count() arguments word sentiment, find much word contributed sentiment.can shown visually, can pipe straight ggplot2, like, way consistently using tools built handling tidy data frames.\nFigure 2.4: Words contribute positive negative sentiment Jane Austen’s novels\nFigure 2.4 lets us spot anomaly sentiment analysis; word “miss” coded negative used title young, unmarried women Jane Austen’s works. appropriate purposes, easily add “miss” custom stop-words list using bind_rows(). implement strategy .","code":"\nbing_word_counts <- tidy_books %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n\nbing_word_counts\n#> # A tibble: 2,585 × 3\n#>    word     sentiment     n\n#>    <chr>    <chr>     <int>\n#>  1 miss     negative   1855\n#>  2 well     positive   1523\n#>  3 good     positive   1380\n#>  4 great    positive    981\n#>  5 like     positive    725\n#>  6 better   positive    639\n#>  7 enough   positive    613\n#>  8 happy    positive    534\n#>  9 love     positive    495\n#> 10 pleasure positive    462\n#> # … with 2,575 more rows\nbing_word_counts %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 10) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL)\ncustom_stop_words <- bind_rows(tibble(word = c(\"miss\"),  \n                                      lexicon = c(\"custom\")), \n                               stop_words)\n\ncustom_stop_words\n#> # A tibble: 1,150 × 2\n#>    word        lexicon\n#>    <chr>       <chr>  \n#>  1 miss        custom \n#>  2 a           SMART  \n#>  3 a's         SMART  \n#>  4 able        SMART  \n#>  5 about       SMART  \n#>  6 above       SMART  \n#>  7 according   SMART  \n#>  8 accordingly SMART  \n#>  9 across      SMART  \n#> 10 actually    SMART  \n#> # … with 1,140 more rows"},{"path":"sentiment.html","id":"wordclouds","chapter":"2 Sentiment analysis with tidy data","heading":"2.5 Wordclouds","text":"’ve seen tidy text mining approach works well ggplot2, data tidy format useful plots well.example, consider wordcloud package, uses base R graphics. Let’s look common words Jane Austen’s works whole , time wordcloud Figure 2.5.\nFigure 2.5: common words Jane Austen’s novels\nfunctions, comparison.cloud(), may need turn data frame matrix reshape2’s acast(). Let’s sentiment analysis tag positive negative words using inner join, find common positive negative words. step need send data comparison.cloud(), can done joins, piping, dplyr data tidy format.\nFigure 2.6: common positive negative words Jane Austen’s novels\nsize word’s text Figure 2.6 proportion frequency within sentiment. can use visualization see important positive negative words, sizes words comparable across sentiments.","code":"\nlibrary(wordcloud)\n\ntidy_books %>%\n  anti_join(stop_words) %>%\n  count(word) %>%\n  with(wordcloud(word, n, max.words = 100))\nlibrary(reshape2)\n\ntidy_books %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %>%\n  comparison.cloud(colors = c(\"gray20\", \"gray80\"),\n                   max.words = 100)"},{"path":"sentiment.html","id":"looking-at-units-beyond-just-words","chapter":"2 Sentiment analysis with tidy data","heading":"2.6 Looking at units beyond just words","text":"Lots useful work can done tokenizing word level, sometimes useful necessary look different units text. example, sentiment analysis algorithms look beyond unigrams (.e. single words) try understand sentiment sentence whole. algorithms try understand thatI good day.sad sentence, happy one, negation. R packages included coreNLP (T. Arnold Tilton 2016), cleanNLP (T. B. Arnold 2016), sentimentr (Rinker 2017) examples sentiment analysis algorithms. , may want tokenize text sentences, makes sense use new name output column case.Let’s look just one.sentence tokenizing seem bit trouble UTF-8 encoded text, especially sections dialogue; much better punctuation ASCII. One possibility, important, try using iconv(), something like iconv(text, = 'latin1') mutate statement unnesting.Another option unnest_tokens() split tokens using regex pattern. use , example, split text Jane Austen’s novels data frame chapter.recovered correct number chapters novel (plus “extra” row novel title). austen_chapters data frame, row corresponds one chapter.Near beginning chapter, used similar regex find chapters Austen’s novels tidy data frame organized one-word-per-row. can use tidy text analysis ask questions negative chapters Jane Austen’s novels? First, let’s get list negative words Bing lexicon. Second, let’s make data frame many words chapter can normalize length chapters. , let’s find number negative words chapter divide total words chapter. book, chapter highest proportion negative words?chapters sad words book, normalized number words chapter. happening chapters? Chapter 43 Sense Sensibility Marianne seriously ill, near death, Chapter 34 Pride Prejudice Mr. Darcy proposes first time (badly!). Chapter 46 Mansfield Park almost end, everyone learns Henry’s scandalous adultery, Chapter 15 Emma horrifying Mr. Elton proposes, Chapter 21 Northanger Abbey Catherine deep Gothic faux fantasy murder, etc. Chapter 4 Persuasion reader gets full flashback Anne refusing Captain Wentworth sad terrible mistake realized .","code":"\np_and_p_sentences <- tibble(text = prideprejudice) %>% \n  unnest_tokens(sentence, text, token = \"sentences\")\np_and_p_sentences$sentence[2]\n#> [1] \"by jane austen\"\nausten_chapters <- austen_books() %>%\n  group_by(book) %>%\n  unnest_tokens(chapter, text, token = \"regex\", \n                pattern = \"Chapter|CHAPTER [\\\\dIVXLC]\") %>%\n  ungroup()\n\nausten_chapters %>% \n  group_by(book) %>% \n  summarise(chapters = n())\n#> # A tibble: 6 × 2\n#>   book                chapters\n#>   <fct>                  <int>\n#> 1 Sense & Sensibility       51\n#> 2 Pride & Prejudice         62\n#> 3 Mansfield Park            49\n#> 4 Emma                      56\n#> 5 Northanger Abbey          32\n#> 6 Persuasion                25\nbingnegative <- get_sentiments(\"bing\") %>% \n  filter(sentiment == \"negative\")\n\nwordcounts <- tidy_books %>%\n  group_by(book, chapter) %>%\n  summarize(words = n())\n\ntidy_books %>%\n  semi_join(bingnegative) %>%\n  group_by(book, chapter) %>%\n  summarize(negativewords = n()) %>%\n  left_join(wordcounts, by = c(\"book\", \"chapter\")) %>%\n  mutate(ratio = negativewords/words) %>%\n  filter(chapter != 0) %>%\n  slice_max(ratio, n = 1) %>% \n  ungroup()\n#> # A tibble: 6 × 5\n#>   book                chapter negativewords words  ratio\n#>   <fct>                 <int>         <int> <int>  <dbl>\n#> 1 Sense & Sensibility      43           161  3405 0.0473\n#> 2 Pride & Prejudice        34           111  2104 0.0528\n#> 3 Mansfield Park           46           173  3685 0.0469\n#> 4 Emma                     15           151  3340 0.0452\n#> 5 Northanger Abbey         21           149  2982 0.0500\n#> 6 Persuasion                4            62  1807 0.0343"},{"path":"sentiment.html","id":"summary-1","chapter":"2 Sentiment analysis with tidy data","heading":"2.7 Summary","text":"Sentiment analysis provides way understand attitudes opinions expressed texts. chapter, explored approach sentiment analysis using tidy data principles; text data tidy data structure, sentiment analysis can implemented inner join. can use sentiment analysis understand narrative arc changes throughout course words emotional opinion content important particular text. continue develop toolbox applying sentiment analysis different kinds text case studies later book.","code":""},{"path":"tfidf.html","id":"tfidf","chapter":"3 Analyzing word and document frequency: tf-idf","heading":"3 Analyzing word and document frequency: tf-idf","text":"central question text mining natural language processing quantify document . Can looking words make document? One measure important word may term frequency (tf), frequently word occurs document, examined Chapter 1. words document, however, occur many times may important; English, probably words like “”, “”, “”, forth. might take approach adding words like list stop words removing analysis, possible words might important documents others. list stop words sophisticated approach adjusting term frequency commonly used words.Another approach look term’s inverse document frequency (idf), decreases weight commonly used words increases weight words used much collection documents. can combined term frequency calculate term’s tf-idf (two quantities multiplied together), frequency term adjusted rarely used.\nstatistic tf-idf intended measure \nimportant word document collection (corpus) \ndocuments, example, one novel collection novels one\nwebsite collection websites.\nrule--thumb heuristic quantity; proved useful text mining, search engines, etc., theoretical foundations considered less firm information theory experts. inverse document frequency given term defined \\[idf(\\text{term}) = \\ln{\\left(\\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}}\\right)}\\]can use tidy data principles, described Chapter 1, approach tf-idf analysis use consistent, effective tools quantify important various terms document part collection.","code":""},{"path":"tfidf.html","id":"term-frequency-in-jane-austens-novels","chapter":"3 Analyzing word and document frequency: tf-idf","heading":"3.1 Term frequency in Jane Austen’s novels","text":"Let’s start looking published novels Jane Austen examine first term frequency, tf-idf. can start just using dplyr verbs group_by() join(). commonly used words Jane Austen’s novels? (Let’s also calculate total words novel , later use.)one row book_words data frame word-book combination; n number times word used book total total words book. usual suspects highest n, “”, “”, “”, forth. Figure 3.1, let’s look distribution n/total novel, number times word appears novel divided total number terms (words) novel. exactly term frequency .\nFigure 3.1: Term frequency distribution Jane Austen’s novels\nlong tails right novels (extremely rare words!) shown plots. plots exhibit similar distributions novels, many words occur rarely fewer words occur frequently.","code":"\nlibrary(dplyr)\nlibrary(janeaustenr)\nlibrary(tidytext)\n\nbook_words <- austen_books() %>%\n  unnest_tokens(word, text) %>%\n  count(book, word, sort = TRUE)\n\ntotal_words <- book_words %>% \n  group_by(book) %>% \n  summarize(total = sum(n))\n\nbook_words <- left_join(book_words, total_words)\n\nbook_words\n#> # A tibble: 40,379 × 4\n#>    book              word      n  total\n#>    <fct>             <chr> <int>  <int>\n#>  1 Mansfield Park    the    6206 160460\n#>  2 Mansfield Park    to     5475 160460\n#>  3 Mansfield Park    and    5438 160460\n#>  4 Emma              to     5239 160996\n#>  5 Emma              the    5201 160996\n#>  6 Emma              and    4896 160996\n#>  7 Mansfield Park    of     4778 160460\n#>  8 Pride & Prejudice the    4331 122204\n#>  9 Emma              of     4291 160996\n#> 10 Pride & Prejudice to     4162 122204\n#> # … with 40,369 more rows\nlibrary(ggplot2)\n\nggplot(book_words, aes(n/total, fill = book)) +\n  geom_histogram(show.legend = FALSE) +\n  xlim(NA, 0.0009) +\n  facet_wrap(~book, ncol = 2, scales = \"free_y\")"},{"path":"tfidf.html","id":"zipfs-law","chapter":"3 Analyzing word and document frequency: tf-idf","heading":"3.2 Zipf’s law","text":"Distributions like shown Figure 3.1 typical language. fact, types long-tailed distributions common given corpus natural language (like book, lot text website, spoken words) relationship frequency word used rank subject study; classic version relationship called Zipf’s law, George Zipf, 20th century American linguist.\nZipf’s law states frequency word appears inversely\nproportional rank.\nSince data frame used plot term frequency, can examine Zipf’s law Jane Austen’s novels just lines dplyr functions.rank column tells us rank word within frequency table; table already ordered n use row_number() find rank. , can calculate term frequency way . Zipf’s law often visualized plotting rank x-axis term frequency y-axis, logarithmic scales. Plotting way, inversely proportional relationship constant, negative slope.\nFigure 3.2: Zipf’s law Jane Austen’s novels\nNotice Figure 3.2 log-log coordinates. see six Jane Austen’s novels similar , relationship rank frequency negative slope. quite constant, though; perhaps view broken power law , say, three sections. Let’s see exponent power law middle section rank range.Classic versions Zipf’s law \\[\\text{frequency} \\propto \\frac{1}{\\text{rank}}\\]\nfact gotten slope close -1 . Let’s plot fitted power law data Figure 3.3 see looks.\nFigure 3.3: Fitting exponent Zipf’s law Jane Austen’s novels\nfound result close classic version Zipf’s law corpus Jane Austen’s novels. deviations see high rank uncommon many kinds language; corpus language often contains fewer rare words predicted single power law. deviations low rank unusual. Jane Austen uses lower percentage common words many collections language. kind analysis extended compare authors, compare collections text; can implemented simply using tidy data principles.","code":"\nfreq_by_rank <- book_words %>% \n  group_by(book) %>% \n  mutate(rank = row_number(), \n         `term frequency` = n/total) %>%\n  ungroup()\n\nfreq_by_rank\n#> # A tibble: 40,379 × 6\n#>    book              word      n  total  rank `term frequency`\n#>    <fct>             <chr> <int>  <int> <int>            <dbl>\n#>  1 Mansfield Park    the    6206 160460     1           0.0387\n#>  2 Mansfield Park    to     5475 160460     2           0.0341\n#>  3 Mansfield Park    and    5438 160460     3           0.0339\n#>  4 Emma              to     5239 160996     1           0.0325\n#>  5 Emma              the    5201 160996     2           0.0323\n#>  6 Emma              and    4896 160996     3           0.0304\n#>  7 Mansfield Park    of     4778 160460     4           0.0298\n#>  8 Pride & Prejudice the    4331 122204     1           0.0354\n#>  9 Emma              of     4291 160996     4           0.0267\n#> 10 Pride & Prejudice to     4162 122204     2           0.0341\n#> # … with 40,369 more rows\nfreq_by_rank %>% \n  ggplot(aes(rank, `term frequency`, color = book)) + \n  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + \n  scale_x_log10() +\n  scale_y_log10()\nrank_subset <- freq_by_rank %>% \n  filter(rank < 500,\n         rank > 10)\n\nlm(log10(`term frequency`) ~ log10(rank), data = rank_subset)\n#> \n#> Call:\n#> lm(formula = log10(`term frequency`) ~ log10(rank), data = rank_subset)\n#> \n#> Coefficients:\n#> (Intercept)  log10(rank)  \n#>     -0.6226      -1.1125\nfreq_by_rank %>% \n  ggplot(aes(rank, `term frequency`, color = book)) + \n  geom_abline(intercept = -0.62, slope = -1.1, \n              color = \"gray50\", linetype = 2) +\n  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + \n  scale_x_log10() +\n  scale_y_log10()"},{"path":"tfidf.html","id":"the-bind_tf_idf-function","chapter":"3 Analyzing word and document frequency: tf-idf","heading":"3.3 The bind_tf_idf() function","text":"idea tf-idf find important words content document decreasing weight commonly used words increasing weight words used much collection corpus documents, case, group Jane Austen’s novels whole. Calculating tf-idf attempts find words important (.e., common) text, common. Let’s now.bind_tf_idf() function tidytext package takes tidy text dataset input one row per token (term), per document. One column (word ) contains terms/tokens, one column contains documents (book case), last necessary column contains counts, many times document contains term (n example). calculated total book explorations previous sections, necessary bind_tf_idf() function; table needs contain words document.Notice idf thus tf-idf zero extremely common words. words appear six Jane Austen’s novels, idf term (natural log 1) zero. inverse document frequency (thus tf-idf) low (near zero) words occur many documents collection; approach decreases weight common words. inverse document frequency higher number words occur fewer documents collection.Let’s look terms high tf-idf Jane Austen’s works.see proper nouns, names fact important novels. None occur novels, important, characteristic words text within corpus Jane Austen’s novels.\nvalues idf different terms \n6 documents corpus seeing numerical\nvalue \\(\\ln(6/1)\\), \\(\\ln(6/2)\\), etc.\nLet’s look visualization high tf-idf words Figure 3.4.\nFigure 3.4: Highest tf-idf words Jane Austen novel\nStill proper nouns Figure 3.4! words , measured tf-idf, important novel readers likely agree. measuring tf-idf done show us Jane Austen used similar language across six novels, distinguishes one novel rest within collection works proper nouns, names people places. point tf-idf; identifies words important one document within collection documents.","code":"\nbook_tf_idf <- book_words %>%\n  bind_tf_idf(word, book, n)\n\nbook_tf_idf\n#> # A tibble: 40,379 × 7\n#>    book              word      n  total     tf   idf tf_idf\n#>    <fct>             <chr> <int>  <int>  <dbl> <dbl>  <dbl>\n#>  1 Mansfield Park    the    6206 160460 0.0387     0      0\n#>  2 Mansfield Park    to     5475 160460 0.0341     0      0\n#>  3 Mansfield Park    and    5438 160460 0.0339     0      0\n#>  4 Emma              to     5239 160996 0.0325     0      0\n#>  5 Emma              the    5201 160996 0.0323     0      0\n#>  6 Emma              and    4896 160996 0.0304     0      0\n#>  7 Mansfield Park    of     4778 160460 0.0298     0      0\n#>  8 Pride & Prejudice the    4331 122204 0.0354     0      0\n#>  9 Emma              of     4291 160996 0.0267     0      0\n#> 10 Pride & Prejudice to     4162 122204 0.0341     0      0\n#> # … with 40,369 more rows\nbook_tf_idf %>%\n  select(-total) %>%\n  arrange(desc(tf_idf))\n#> # A tibble: 40,379 × 6\n#>    book                word          n      tf   idf  tf_idf\n#>    <fct>               <chr>     <int>   <dbl> <dbl>   <dbl>\n#>  1 Sense & Sensibility elinor      623 0.00519  1.79 0.00931\n#>  2 Sense & Sensibility marianne    492 0.00410  1.79 0.00735\n#>  3 Mansfield Park      crawford    493 0.00307  1.79 0.00551\n#>  4 Pride & Prejudice   darcy       373 0.00305  1.79 0.00547\n#>  5 Persuasion          elliot      254 0.00304  1.79 0.00544\n#>  6 Emma                emma        786 0.00488  1.10 0.00536\n#>  7 Northanger Abbey    tilney      196 0.00252  1.79 0.00452\n#>  8 Emma                weston      389 0.00242  1.79 0.00433\n#>  9 Pride & Prejudice   bennet      294 0.00241  1.79 0.00431\n#> 10 Persuasion          wentworth   191 0.00228  1.79 0.00409\n#> # … with 40,369 more rows\nlibrary(forcats)\n\nbook_tf_idf %>%\n  group_by(book) %>%\n  slice_max(tf_idf, n = 15) %>%\n  ungroup() %>%\n  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = book)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~book, ncol = 2, scales = \"free\") +\n  labs(x = \"tf-idf\", y = NULL)"},{"path":"tfidf.html","id":"a-corpus-of-physics-texts","chapter":"3 Analyzing word and document frequency: tf-idf","heading":"3.4 A corpus of physics texts","text":"Let’s work another corpus documents, see terms important different set works. fact, let’s leave world fiction narrative entirely. Let’s download classic physics texts Project Gutenberg see terms important works, measured tf-idf. Let’s download Discourse Floating Bodies Galileo Galilei, Treatise Light Christiaan Huygens, Experiments Alternate Currents High Potential High Frequency Nikola Tesla, Relativity: Special General Theory Albert Einstein.pretty diverse bunch. may physics classics, written across 300-year timespan, first written languages translated English. Perfectly homogeneous , doesn’t stop interesting exercise!Now texts, let’s use unnest_tokens() count() find many times word used text.see just raw counts; need remember documents different lengths. Let’s go ahead calculate tf-idf, visualize high tf-idf words Figure 3.5.\nFigure 3.5: Highest tf-idf words physics texts\ninteresting indeed. One thing see “k” Einstein text?!cleaning text may order. Also notice separate “co” “ordinate” items high tf-idf words Einstein text; unnest_tokens() function separates around punctuation like hyphens default. Notice tf-idf scores “co” “ordinate” close !“AB”, “RC”, forth names rays, circles, angles, forth Huygens.Let’s remove less meaningful words make better, meaningful plot. Notice make custom list stop words use anti_join() remove ; flexible approach can used many situations. need go back steps since removing words tidy data frame.\nFigure 3.6: Highest tf-idf words classic physics texts\nOne thing can conclude Figure 3.6 don’t hear enough ramparts things ethereal physics today.","code":"\nlibrary(gutenbergr)\nphysics <- gutenberg_download(c(37729, 14725, 13476, 30155), \n                              meta_fields = \"author\")\nphysics_words <- physics %>%\n  unnest_tokens(word, text) %>%\n  count(author, word, sort = TRUE)\n\nphysics_words\n#> # A tibble: 12,671 × 3\n#>    author              word      n\n#>    <chr>               <chr> <int>\n#>  1 Galilei, Galileo    the    3760\n#>  2 Tesla, Nikola       the    3604\n#>  3 Huygens, Christiaan the    3553\n#>  4 Einstein, Albert    the    2993\n#>  5 Galilei, Galileo    of     2049\n#>  6 Einstein, Albert    of     2028\n#>  7 Tesla, Nikola       of     1737\n#>  8 Huygens, Christiaan of     1708\n#>  9 Huygens, Christiaan to     1207\n#> 10 Tesla, Nikola       a      1176\n#> # … with 12,661 more rows\nplot_physics <- physics_words %>%\n  bind_tf_idf(word, author, n) %>%\n  mutate(author = factor(author, levels = c(\"Galilei, Galileo\",\n                                            \"Huygens, Christiaan\", \n                                            \"Tesla, Nikola\",\n                                            \"Einstein, Albert\")))\n\nplot_physics %>% \n  group_by(author) %>% \n  slice_max(tf_idf, n = 15) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, tf_idf)) %>%\n  ggplot(aes(tf_idf, word, fill = author)) +\n  geom_col(show.legend = FALSE) +\n  labs(x = \"tf-idf\", y = NULL) +\n  facet_wrap(~author, ncol = 2, scales = \"free\")\nlibrary(stringr)\n\nphysics %>% \n  filter(str_detect(text, \"_k_\")) %>% \n  select(text)\n#> # A tibble: 7 × 1\n#>   text                                                                  \n#>   <chr>                                                                 \n#> 1 surface AB at the points AK_k_B. Then instead of the hemispherical    \n#> 2 would needs be that from all the other points K_k_B there should      \n#> 3 necessarily be equal to CD, because C_k_ is equal to CK, and C_g_ to  \n#> 4 the crystal at K_k_, all the points of the wave CO_oc_ will have      \n#> 5 O_o_ has reached K_k_. Which is easy to comprehend, since, of these   \n#> 6 CO_oc_ in the crystal, when O_o_ has arrived at K_k_, because it forms\n#> 7 ρ is the average density of the matter and _k_ is a constant connected\nphysics %>% \n  filter(str_detect(text, \"RC\")) %>% \n  select(text)\n#> # A tibble: 44 × 1\n#>    text                                                                  \n#>    <chr>                                                                 \n#>  1 line RC, parallel and equal to AB, to be a portion of a wave of light,\n#>  2 represents the partial wave coming from the point A, after the wave RC\n#>  3 be the propagation of the wave RC which fell on AB, and would be the  \n#>  4 transparent body; seeing that the wave RC, having come to the aperture\n#>  5 incident rays. Let there be such a ray RC falling upon the surface    \n#>  6 CK. Make CO perpendicular to RC, and across the angle KCO adjust OK,  \n#>  7 the required refraction of the ray RC. The demonstration of this is,  \n#>  8 explaining ordinary refraction. For the refraction of the ray RC is   \n#>  9 29. Now as we have found CI the refraction of the ray RC, similarly   \n#> 10 the ray _r_C is inclined equally with RC, the line C_d_ will          \n#> # … with 34 more rows\nmystopwords <- tibble(word = c(\"eq\", \"co\", \"rc\", \"ac\", \"ak\", \"bn\", \n                                   \"fig\", \"file\", \"cg\", \"cb\", \"cm\",\n                               \"ab\", \"_k\", \"_k_\", \"_x\"))\n\nphysics_words <- anti_join(physics_words, mystopwords, \n                           by = \"word\")\n\nplot_physics <- physics_words %>%\n  bind_tf_idf(word, author, n) %>%\n  mutate(word = str_remove_all(word, \"_\")) %>%\n  group_by(author) %>% \n  slice_max(tf_idf, n = 15) %>%\n  ungroup() %>%\n  mutate(word = fct_reorder(word, tf_idf)) %>%\n  mutate(author = factor(author, levels = c(\"Galilei, Galileo\",\n                                            \"Huygens, Christiaan\",\n                                            \"Tesla, Nikola\",\n                                            \"Einstein, Albert\")))\n\nggplot(plot_physics, aes(tf_idf, word, fill = author)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~author, ncol = 2, scales = \"free\") +\n  labs(x = \"tf-idf\", y = NULL)"},{"path":"tfidf.html","id":"summary-2","chapter":"3 Analyzing word and document frequency: tf-idf","heading":"3.5 Summary","text":"Using term frequency inverse document frequency allows us find words characteristic one document within collection documents, whether document novel physics text webpage. Exploring term frequency can give us insight language used collection natural language, dplyr verbs like count() rank() give us tools reason term frequency. tidytext package uses implementation tf-idf consistent tidy data principles enables us see different words important documents within collection corpus documents.","code":""},{"path":"ngrams.html","id":"ngrams","chapter":"4 Relationships between words: n-grams and correlations","heading":"4 Relationships between words: n-grams and correlations","text":"far ’ve considered words individual units, considered relationships sentiments documents. However, many interesting text analyses based relationships words, whether examining words tend follow others immediately, tend co-occur within documents.chapter, ’ll explore methods tidytext offers calculating visualizing relationships words text dataset. includes token = \"ngrams\" argument, tokenizes pairs adjacent words rather individual ones. ’ll also introduce two new packages: ggraph, extends ggplot2 construct network plots, widyr, calculates pairwise correlations distances within tidy data frame. Together expand toolbox exploring text within tidy data framework.","code":""},{"path":"ngrams.html","id":"tokenizing-by-n-gram","chapter":"4 Relationships between words: n-grams and correlations","heading":"4.1 Tokenizing by n-gram","text":"’ve using unnest_tokens function tokenize word, sometimes sentence, useful kinds sentiment frequency analyses ’ve far. can also use function tokenize consecutive sequences words, called n-grams. seeing often word X followed word Y, can build model relationships .adding token = \"ngrams\" option unnest_tokens(), setting n number words wish capture n-gram. set n 2, examining pairs two consecutive words, often called “bigrams”:data structure still variation tidy text format. structured one-token-per-row (extra metadata, book, still preserved), token now represents bigram.\nNotice bigrams overlap: “sense ” one token, \n“sensibility” another.\n","code":"\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(janeaustenr)\n\nausten_bigrams <- austen_books() %>%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n\nausten_bigrams\n#> # A tibble: 675,025 × 2\n#>    book                bigram         \n#>    <fct>               <chr>          \n#>  1 Sense & Sensibility sense and      \n#>  2 Sense & Sensibility and sensibility\n#>  3 Sense & Sensibility <NA>           \n#>  4 Sense & Sensibility by jane        \n#>  5 Sense & Sensibility jane austen    \n#>  6 Sense & Sensibility <NA>           \n#>  7 Sense & Sensibility <NA>           \n#>  8 Sense & Sensibility <NA>           \n#>  9 Sense & Sensibility <NA>           \n#> 10 Sense & Sensibility <NA>           \n#> # … with 675,015 more rows"},{"path":"ngrams.html","id":"counting-and-filtering-n-grams","chapter":"4 Relationships between words: n-grams and correlations","heading":"4.1.1 Counting and filtering n-grams","text":"usual tidy tools apply equally well n-gram analysis. can examine common bigrams using dplyr’s count():one might expect, lot common bigrams pairs common (uninteresting) words, : call “stop-words” (see Chapter 1). useful time use tidyr’s separate(), splits column multiple based delimiter. lets us separate two columns, “word1” “word2”, point can remove cases either stop-word.can see names (whether first last salutation) common pairs Jane Austen books.analyses, may want work recombined words. tidyr’s unite() function inverse separate(), lets us recombine columns one. Thus, “separate/filter/count/unite” let us find common bigrams containing stop-words.analyses may interested common trigrams, consecutive sequences 3 words. can find setting n = 3:","code":"\nausten_bigrams %>%\n  count(bigram, sort = TRUE)\n#> # A tibble: 193,210 × 2\n#>    bigram      n\n#>    <chr>   <int>\n#>  1 <NA>    12242\n#>  2 of the   2853\n#>  3 to be    2670\n#>  4 in the   2221\n#>  5 it was   1691\n#>  6 i am     1485\n#>  7 she had  1405\n#>  8 of her   1363\n#>  9 to the   1315\n#> 10 she was  1309\n#> # … with 193,200 more rows\nlibrary(tidyr)\n\nbigrams_separated <- austen_bigrams %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\n\nbigrams_filtered <- bigrams_separated %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word)\n\n# new bigram counts:\nbigram_counts <- bigrams_filtered %>% \n  count(word1, word2, sort = TRUE)\n\nbigram_counts\n#> # A tibble: 28,975 × 3\n#>    word1   word2         n\n#>    <chr>   <chr>     <int>\n#>  1 <NA>    <NA>      12242\n#>  2 sir     thomas      266\n#>  3 miss    crawford    196\n#>  4 captain wentworth   143\n#>  5 miss    woodhouse   143\n#>  6 frank   churchill   114\n#>  7 lady    russell     110\n#>  8 sir     walter      108\n#>  9 lady    bertram     101\n#> 10 miss    fairfax      98\n#> # … with 28,965 more rows\nbigrams_united <- bigrams_filtered %>%\n  unite(bigram, word1, word2, sep = \" \")\n\nbigrams_united\n#> # A tibble: 51,155 × 2\n#>    book                bigram     \n#>    <fct>               <chr>      \n#>  1 Sense & Sensibility NA NA      \n#>  2 Sense & Sensibility jane austen\n#>  3 Sense & Sensibility NA NA      \n#>  4 Sense & Sensibility NA NA      \n#>  5 Sense & Sensibility NA NA      \n#>  6 Sense & Sensibility NA NA      \n#>  7 Sense & Sensibility NA NA      \n#>  8 Sense & Sensibility NA NA      \n#>  9 Sense & Sensibility chapter 1  \n#> 10 Sense & Sensibility NA NA      \n#> # … with 51,145 more rows\nausten_books() %>%\n  unnest_tokens(trigram, text, token = \"ngrams\", n = 3) %>%\n  separate(trigram, c(\"word1\", \"word2\", \"word3\"), sep = \" \") %>%\n  filter(!word1 %in% stop_words$word,\n         !word2 %in% stop_words$word,\n         !word3 %in% stop_words$word) %>%\n  count(word1, word2, word3, sort = TRUE)\n#> # A tibble: 6,141 × 4\n#>    word1     word2     word3         n\n#>    <chr>     <chr>     <chr>     <int>\n#>  1 <NA>      <NA>      <NA>      13260\n#>  2 dear      miss      woodhouse    20\n#>  3 miss      de        bourgh       17\n#>  4 lady      catherine de           11\n#>  5 poor      miss      taylor       11\n#>  6 sir       walter    elliot       10\n#>  7 catherine de        bourgh        9\n#>  8 dear      sir       thomas        8\n#>  9 replied   miss      crawford      7\n#> 10 sir       william   lucas         7\n#> # … with 6,131 more rows"},{"path":"ngrams.html","id":"analyzing-bigrams","chapter":"4 Relationships between words: n-grams and correlations","heading":"4.1.2 Analyzing bigrams","text":"one-bigram-per-row format helpful exploratory analyses text. simple example, might interested common “streets” mentioned book:bigram can also treated term document way treated individual words. example, can look tf-idf (Chapter 3) bigrams across Austen novels. tf-idf values can visualized within book, just words (Figure 4.1).\nFigure 4.1: Bigrams highest tf-idf Jane Austen novel\nMuch discovered Chapter 3, units distinguish Austen book almost exclusively names. also notice pairings common verb name, “replied elizabeth” Pride & Prejudice, “cried emma” Emma.advantages disadvantages examining tf-idf bigrams rather individual words. Pairs consecutive words might capture structure isn’t present one just counting single words, may provide context makes tokens understandable (example, “pulteney street”, Northanger Abbey, informative “pulteney”). However, per-bigram counts also sparser: typical two-word pair rarer either component words. Thus, bigrams can especially useful large text dataset.","code":"\nbigrams_filtered %>%\n  filter(word2 == \"street\") %>%\n  count(book, word1, sort = TRUE)\n#> # A tibble: 33 × 3\n#>    book                word1           n\n#>    <fct>               <chr>       <int>\n#>  1 Sense & Sensibility harley         16\n#>  2 Sense & Sensibility berkeley       15\n#>  3 Northanger Abbey    milsom         10\n#>  4 Northanger Abbey    pulteney       10\n#>  5 Mansfield Park      wimpole         9\n#>  6 Pride & Prejudice   gracechurch     8\n#>  7 Persuasion          milsom          5\n#>  8 Sense & Sensibility bond            4\n#>  9 Sense & Sensibility conduit         4\n#> 10 Persuasion          rivers          4\n#> # … with 23 more rows\nbigram_tf_idf <- bigrams_united %>%\n  count(book, bigram) %>%\n  bind_tf_idf(bigram, book, n) %>%\n  arrange(desc(tf_idf))\n\nbigram_tf_idf\n#> # A tibble: 31,397 × 6\n#>    book                bigram                n     tf   idf tf_idf\n#>    <fct>               <chr>             <int>  <dbl> <dbl>  <dbl>\n#>  1 Mansfield Park      sir thomas          266 0.0244  1.79 0.0438\n#>  2 Persuasion          captain wentworth   143 0.0232  1.79 0.0416\n#>  3 Mansfield Park      miss crawford       196 0.0180  1.79 0.0322\n#>  4 Persuasion          lady russell        110 0.0179  1.79 0.0320\n#>  5 Persuasion          sir walter          108 0.0175  1.79 0.0314\n#>  6 Emma                miss woodhouse      143 0.0129  1.79 0.0231\n#>  7 Northanger Abbey    miss tilney          74 0.0128  1.79 0.0229\n#>  8 Sense & Sensibility colonel brandon      96 0.0115  1.79 0.0205\n#>  9 Sense & Sensibility sir john             94 0.0112  1.79 0.0201\n#> 10 Emma                frank churchill     114 0.0103  1.79 0.0184\n#> # … with 31,387 more rows"},{"path":"ngrams.html","id":"using-bigrams-to-provide-context-in-sentiment-analysis","chapter":"4 Relationships between words: n-grams and correlations","heading":"4.1.3 Using bigrams to provide context in sentiment analysis","text":"sentiment analysis approach Chapter 2 simply counted appearance positive negative words, according reference lexicon. One problems approach word’s context can matter nearly much presence. example, words “happy” “like” counted positive, even sentence like “’m happy don’t like !”Now data organized bigrams, ’s easy tell often words preceded word like “”:performing sentiment analysis bigram data, can examine often sentiment-associated words preceded “” negating words. use ignore even reverse contribution sentiment score.Let’s use AFINN lexicon sentiment analysis, may recall gives numeric sentiment value word, positive negative numbers indicating direction sentiment.can examine frequent words preceded “” associated sentiment.example, common sentiment-associated word follow “” “like”, normally (positive) score 2.’s worth asking words contributed “wrong” direction. compute , can multiply value number times appear (word value +3 occurring 10 times much impact word sentiment value +1 occurring 30 times). visualize result bar plot (Figure 4.2).\nFigure 4.2: Words preceded ‘’ greatest contribution sentiment values, either positive negative direction\nbigrams “like” “help” overwhelmingly largest causes misidentification, making text seem much positive . can see phrases like “afraid” “fail” sometimes suggest text negative .“” isn’t term provides context following word. pick four common words () negate subsequent term, use joining counting approach examine .visualize common words follow particular negation (Figure 4.3). “like” “help” still two common examples, can also see pairings “great” “never loved.” combine approaches Chapter 2 reverse AFINN values word follows negation. just examples finding consecutive words can give context text mining methods.\nFigure 4.3: common positive negative words follow negations ‘never’, ‘’, ‘’, ‘without’\n","code":"\nbigrams_separated %>%\n  filter(word1 == \"not\") %>%\n  count(word1, word2, sort = TRUE)\n#> # A tibble: 1,178 × 3\n#>    word1 word2     n\n#>    <chr> <chr> <int>\n#>  1 not   be      580\n#>  2 not   to      335\n#>  3 not   have    307\n#>  4 not   know    237\n#>  5 not   a       184\n#>  6 not   think   162\n#>  7 not   been    151\n#>  8 not   the     135\n#>  9 not   at      126\n#> 10 not   in      110\n#> # … with 1,168 more rows\nAFINN <- get_sentiments(\"afinn\")\n\nAFINN#> # A tibble: 2,477 × 2\n#>    word       value\n#>    <chr>      <dbl>\n#>  1 abandon       -2\n#>  2 abandoned     -2\n#>  3 abandons      -2\n#>  4 abducted      -2\n#>  5 abduction     -2\n#>  6 abductions    -2\n#>  7 abhor         -3\n#>  8 abhorred      -3\n#>  9 abhorrent     -3\n#> 10 abhors        -3\n#> # … with 2,467 more rows\nnot_words <- bigrams_separated %>%\n  filter(word1 == \"not\") %>%\n  inner_join(AFINN, by = c(word2 = \"word\")) %>%\n  count(word2, value, sort = TRUE)\n\nnot_words\n#> # A tibble: 229 × 3\n#>    word2   value     n\n#>    <chr>   <dbl> <int>\n#>  1 like        2    95\n#>  2 help        2    77\n#>  3 want        1    41\n#>  4 wish        1    39\n#>  5 allow       1    30\n#>  6 care        2    21\n#>  7 sorry      -1    20\n#>  8 leave      -1    17\n#>  9 pretend    -1    17\n#> 10 worth       2    17\n#> # … with 219 more rows\n\nlibrary(ggplot2)\n\nnot_words %>%\n  mutate(contribution = n * value) %>%\n  arrange(desc(abs(contribution))) %>%\n  head(20) %>%\n  mutate(word2 = reorder(word2, contribution)) %>%\n  ggplot(aes(n * value, word2, fill = n * value > 0)) +\n  geom_col(show.legend = FALSE) +\n  labs(x = \"Sentiment value * number of occurrences\",\n       y = \"Words preceded by \\\"not\\\"\")\nnegation_words <- c(\"not\", \"no\", \"never\", \"without\")\n\nnegated_words <- bigrams_separated %>%\n  filter(word1 %in% negation_words) %>%\n  inner_join(AFINN, by = c(word2 = \"word\")) %>%\n  count(word1, word2, value, sort = TRUE)"},{"path":"ngrams.html","id":"visualizing-a-network-of-bigrams-with-ggraph","chapter":"4 Relationships between words: n-grams and correlations","heading":"4.1.4 Visualizing a network of bigrams with ggraph","text":"may interested visualizing relationships among words simultaneously, rather just top time. one common visualization, can arrange words network, “graph.” ’ll referring “graph” sense visualization, combination connected nodes. graph can constructed tidy object since three variables:: node edge coming fromto: node edge going towardsweight: numeric value associated edgeThe igraph package many powerful functions manipulating analyzing networks. One way create igraph object tidy data graph_from_data_frame() function, takes data frame edges columns “”, “”, edge attributes (case n):igraph plotting functions built , ’re package designed , many packages developed visualization methods graph objects. recommend ggraph package (Pedersen 2017), implements visualizations terms grammar graphics, already familiar ggplot2.can convert igraph object ggraph ggraph function, add layers , much like layers added ggplot2. example, basic graph need add three layers: nodes, edges, text.\nFigure 4.4: Common bigrams Jane Austen’s novels, showing occurred 20 times neither word stop word\nFigure 4.4, can visualize details text structure. example, see salutations “miss”, “lady”, “sir”, “colonel” form common centers nodes, often followed names. also see pairs triplets along outside form common short phrases (“half hour”, “thousand pounds”, “short time/pause”).conclude polishing operations make better looking graph (Figure 4.5):add edge_alpha aesthetic link layer make links transparent based common rare bigram isWe add directionality arrow, constructed using grid::arrow(), including end_cap option tells arrow end touching nodeWe tinker options node layer make nodes attractive (larger, blue points)add theme ’s useful plotting networks, theme_void()\nFigure 4.5: Common bigrams Jane Austen’s novels, polishing\nmay take experimentation ggraph get networks presentable format like , network structure useful flexible way visualize relational tidy data.\nNote visualization Markov chain,\ncommon model text processing. Markov chain, choice \nword depends previous word. case, random generator\nfollowing model might spit “dear”, “sir”, \n“william/walter/thomas/thomas’s”, following word \ncommon words follow . make visualization interpretable, \nchose show common word word connections, one\nimagine enormous graph representing connections occur\ntext.\n","code":"\nlibrary(igraph)\n\n# original counts\nbigram_counts\n#> # A tibble: 28,975 × 3\n#>    word1   word2         n\n#>    <chr>   <chr>     <int>\n#>  1 <NA>    <NA>      12242\n#>  2 sir     thomas      266\n#>  3 miss    crawford    196\n#>  4 captain wentworth   143\n#>  5 miss    woodhouse   143\n#>  6 frank   churchill   114\n#>  7 lady    russell     110\n#>  8 sir     walter      108\n#>  9 lady    bertram     101\n#> 10 miss    fairfax      98\n#> # … with 28,965 more rows\n\n# filter for only relatively common combinations\nbigram_graph <- bigram_counts %>%\n  filter(n > 20) %>%\n  graph_from_data_frame()\n\nbigram_graph\n#> IGRAPH 17b51c0 DN-- 86 71 -- \n#> + attr: name (v/c), n (e/n)\n#> + edges from 17b51c0 (vertex names):\n#>  [1] NA      ->NA         sir     ->thomas     miss    ->crawford  \n#>  [4] captain ->wentworth  miss    ->woodhouse  frank   ->churchill \n#>  [7] lady    ->russell    sir     ->walter     lady    ->bertram   \n#> [10] miss    ->fairfax    colonel ->brandon    sir     ->john      \n#> [13] miss    ->bates      jane    ->fairfax    lady    ->catherine \n#> [16] lady    ->middleton  miss    ->tilney     miss    ->bingley   \n#> [19] thousand->pounds     miss    ->dashwood   dear    ->miss      \n#> [22] miss    ->bennet     miss    ->morland    captain ->benwick   \n#> + ... omitted several edges\nlibrary(ggraph)\nset.seed(2017)\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1)\nset.seed(2020)\n\na <- grid::arrow(type = \"closed\", length = unit(.15, \"inches\"))\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,\n                 arrow = a, end_cap = circle(.07, 'inches')) +\n  geom_node_point(color = \"lightblue\", size = 5) +\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +\n  theme_void()"},{"path":"ngrams.html","id":"visualizing-bigrams-in-other-texts","chapter":"4 Relationships between words: n-grams and correlations","heading":"4.1.5 Visualizing bigrams in other texts","text":"went good amount work cleaning visualizing bigrams text dataset, let’s collect function easily perform text datasets.\nmake easy use count_bigrams() \nvisualize_bigrams() , ’ve also reloaded \npackages necessary .\npoint, visualize bigrams works, King James Version Bible:\nFigure 4.6: Directed graph common bigrams King James Bible, showing occurred 40 times\nFigure 4.6 thus lays common “blueprint” language within Bible, particularly focused around “thy” “thou” (probably considered stopwords!) can use gutenbergr package count_bigrams/visualize_bigrams functions visualize bigrams classic books ’re interested .","code":"\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tidytext)\nlibrary(ggplot2)\nlibrary(igraph)\nlibrary(ggraph)\n\ncount_bigrams <- function(dataset) {\n  dataset %>%\n    unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %>%\n    separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\n    filter(!word1 %in% stop_words$word,\n           !word2 %in% stop_words$word) %>%\n    count(word1, word2, sort = TRUE)\n}\n\nvisualize_bigrams <- function(bigrams) {\n  set.seed(2016)\n  a <- grid::arrow(type = \"closed\", length = unit(.15, \"inches\"))\n  \n  bigrams %>%\n    graph_from_data_frame() %>%\n    ggraph(layout = \"fr\") +\n    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +\n    geom_node_point(color = \"lightblue\", size = 5) +\n    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +\n    theme_void()\n}\n# the King James version is book 10 on Project Gutenberg:\nlibrary(gutenbergr)\nkjv <- gutenberg_download(10)\nlibrary(stringr)\n\nkjv_bigrams <- kjv %>%\n  count_bigrams()\n\n# filter out rare combinations, as well as digits\nkjv_bigrams %>%\n  filter(n > 40,\n         !str_detect(word1, \"\\\\d\"),\n         !str_detect(word2, \"\\\\d\")) %>%\n  visualize_bigrams()"},{"path":"ngrams.html","id":"counting-and-correlating-pairs-of-words-with-the-widyr-package","chapter":"4 Relationships between words: n-grams and correlations","heading":"4.2 Counting and correlating pairs of words with the widyr package","text":"Tokenizing n-gram useful way explore pairs adjacent words. However, may also interested words tend co-occur within particular documents particular chapters, even don’t occur next .Tidy data useful structure comparing variables grouping rows, can challenging compare rows: example, count number times two words appear within document, see correlated . operations finding pairwise counts correlations need turn data wide matrix first.\nFigure 4.7: philosophy behind widyr package, can perform operations counting correlating pairs values tidy dataset. widyr package first ‘casts’ tidy dataset wide matrix, performs operation correlation , re-tidies result.\n’ll examine ways tidy text can turned wide matrix Chapter 5, case isn’t necessary. widyr package makes operations computing counts correlations easy, simplifying pattern “widen data, perform operation, re-tidy data” (Figure 4.7). ’ll focus set functions make pairwise comparisons groups observations (example, documents, sections text).","code":""},{"path":"ngrams.html","id":"counting-and-correlating-among-sections","chapter":"4 Relationships between words: n-grams and correlations","heading":"4.2.1 Counting and correlating among sections","text":"Consider book “Pride Prejudice” divided 10-line sections, (larger sections) sentiment analysis Chapter 2. may interested words tend appear within section.One useful function widyr pairwise_count() function. prefix pairwise_ means result one row pair words word variable. lets us count common pairs words co-appearing within section:Notice input one row pair document (10-line section) word, output one row pair words. also tidy format, different structure can use answer new questions.example, can see common pair words section “Elizabeth” “Darcy” (two main characters). can easily find words often occur Darcy:","code":"\nausten_section_words <- austen_books() %>%\n  filter(book == \"Pride & Prejudice\") %>%\n  mutate(section = row_number() %/% 10) %>%\n  filter(section > 0) %>%\n  unnest_tokens(word, text) %>%\n  filter(!word %in% stop_words$word)\n\nausten_section_words\n#> # A tibble: 37,240 × 3\n#>    book              section word        \n#>    <fct>               <dbl> <chr>       \n#>  1 Pride & Prejudice       1 truth       \n#>  2 Pride & Prejudice       1 universally \n#>  3 Pride & Prejudice       1 acknowledged\n#>  4 Pride & Prejudice       1 single      \n#>  5 Pride & Prejudice       1 possession  \n#>  6 Pride & Prejudice       1 fortune     \n#>  7 Pride & Prejudice       1 wife        \n#>  8 Pride & Prejudice       1 feelings    \n#>  9 Pride & Prejudice       1 views       \n#> 10 Pride & Prejudice       1 entering    \n#> # … with 37,230 more rows\nlibrary(widyr)\n\n# count words co-occuring within sections\nword_pairs <- austen_section_words %>%\n  pairwise_count(word, section, sort = TRUE)\n\nword_pairs\n#> # A tibble: 796,008 × 3\n#>    item1     item2         n\n#>    <chr>     <chr>     <dbl>\n#>  1 darcy     elizabeth   144\n#>  2 elizabeth darcy       144\n#>  3 miss      elizabeth   110\n#>  4 elizabeth miss        110\n#>  5 elizabeth jane        106\n#>  6 jane      elizabeth   106\n#>  7 miss      darcy        92\n#>  8 darcy     miss         92\n#>  9 elizabeth bingley      91\n#> 10 bingley   elizabeth    91\n#> # … with 795,998 more rows\nword_pairs %>%\n  filter(item1 == \"darcy\")\n#> # A tibble: 2,930 × 3\n#>    item1 item2         n\n#>    <chr> <chr>     <dbl>\n#>  1 darcy elizabeth   144\n#>  2 darcy miss         92\n#>  3 darcy bingley      86\n#>  4 darcy jane         46\n#>  5 darcy bennet       45\n#>  6 darcy sister       45\n#>  7 darcy time         41\n#>  8 darcy lady         38\n#>  9 darcy friend       37\n#> 10 darcy wickham      37\n#> # … with 2,920 more rows"},{"path":"ngrams.html","id":"pairwise-correlation","chapter":"4 Relationships between words: n-grams and correlations","heading":"4.2.2 Pairwise correlation","text":"Pairs like “Elizabeth” “Darcy” common co-occurring words, ’s particularly meaningful since ’re also common individual words. may instead want examine correlation among words, indicates often appear together relative often appear separately.particular, ’ll focus phi coefficient, common measure binary correlation. focus phi coefficient much likely either word X Y appear, neither , one appears without .Consider following table:example, \\(n_{11}\\) represents number documents word X word Y appear, \\(n_{00}\\) number neither appears, \\(n_{10}\\) \\(n_{01}\\) cases one appears without . terms table, phi coefficient :\\[\\phi=\\frac{n_{11}n_{00}-n_{10}n_{01}}{\\sqrt{n_{1\\cdot}n_{0\\cdot}n_{\\cdot0}n_{\\cdot1}}}\\]\nphi coefficient equivalent Pearson correlation, \nmay heard elsewhere applied binary data.\npairwise_cor() function widyr lets us find phi coefficient words based often appear section. syntax similar pairwise_count().output format helpful exploration. example, find words correlated word like “pounds” using filter operation.lets us pick particular interesting words find words associated (Figure 4.8).\nFigure 4.8: Words Pride Prejudice correlated ‘elizabeth’, ‘pounds’, ‘married’, ‘pride’\nJust used ggraph visualize bigrams, can use visualize correlations clusters words found widyr package (Figure 4.9).\nFigure 4.9: Pairs words Pride Prejudice show least .15 correlation appearing within 10-line section\nNote unlike bigram analysis, relationships symmetrical, rather directional (arrows). can also see pairings names titles dominated bigram pairings common, “colonel/fitzwilliam”, can also see pairings words appear close , “walk” “park”, “dance” “ball”.","code":"\n# we need to filter for at least relatively common words first\nword_cors <- austen_section_words %>%\n  group_by(word) %>%\n  filter(n() >= 20) %>%\n  pairwise_cor(word, section, sort = TRUE)\n\nword_cors\n#> # A tibble: 154,842 × 3\n#>    item1     item2     correlation\n#>    <chr>     <chr>           <dbl>\n#>  1 bourgh    de              0.951\n#>  2 de        bourgh          0.951\n#>  3 pounds    thousand        0.701\n#>  4 thousand  pounds          0.701\n#>  5 william   sir             0.664\n#>  6 sir       william         0.664\n#>  7 catherine lady            0.663\n#>  8 lady      catherine       0.663\n#>  9 forster   colonel         0.622\n#> 10 colonel   forster         0.622\n#> # … with 154,832 more rows\nword_cors %>%\n  filter(item1 == \"pounds\")\n#> # A tibble: 393 × 3\n#>    item1  item2     correlation\n#>    <chr>  <chr>           <dbl>\n#>  1 pounds thousand       0.701 \n#>  2 pounds ten            0.231 \n#>  3 pounds fortune        0.164 \n#>  4 pounds settled        0.149 \n#>  5 pounds wickham's      0.142 \n#>  6 pounds children       0.129 \n#>  7 pounds mother's       0.119 \n#>  8 pounds believed       0.0932\n#>  9 pounds estate         0.0890\n#> 10 pounds ready          0.0860\n#> # … with 383 more rows\nword_cors %>%\n  filter(item1 %in% c(\"elizabeth\", \"pounds\", \"married\", \"pride\")) %>%\n  group_by(item1) %>%\n  slice_max(correlation, n = 6) %>%\n  ungroup() %>%\n  mutate(item2 = reorder(item2, correlation)) %>%\n  ggplot(aes(item2, correlation)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free\") +\n  coord_flip()\nset.seed(2016)\n\nword_cors %>%\n  filter(correlation > .15) %>%\n  graph_from_data_frame() %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +\n  geom_node_point(color = \"lightblue\", size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE) +\n  theme_void()"},{"path":"ngrams.html","id":"summary-3","chapter":"4 Relationships between words: n-grams and correlations","heading":"4.3 Summary","text":"chapter showed tidy text approach useful analyzing individual words, also exploring relationships connections words. relationships can involve n-grams, enable us see words tend appear others, co-occurences correlations, words appear proximity . chapter also demonstrated ggraph package visualizing types relationships networks. network visualizations flexible tool exploring relationships, play important role case studies later chapters.","code":""},{"path":"dtm.html","id":"dtm","chapter":"5 Converting to and from non-tidy formats","heading":"5 Converting to and from non-tidy formats","text":"previous chapters, ’ve analyzing text arranged tidy text format: table one-token-per-document-per-row, constructed unnest_tokens() function. lets us use popular suite tidy tools dplyr, tidyr, ggplot2 explore visualize text data. ’ve demonstrated many informative text analyses can performed using tools.However, existing R tools natural language processing, besides tidytext package, aren’t compatible format. CRAN Task View Natural Language Processing lists large selection packages take structures input provide non-tidy outputs. packages useful text mining applications, many existing text datasets structured according formats.Computer scientist Hal Abelson observed “matter complex polished individual operations , often quality glue directly determines power system” (Abelson 2008). spirit, chapter discuss “glue” connects tidy text format important packages data structures, allowing rely existing text mining packages suite tidy tools perform analysis.\nFigure 5.1: flowchart typical text analysis combines tidytext tools data formats, particularly tm quanteda packages. chapter shows convert back forth document-term matrices tidy data frames, well converting Corpus object text data frame.\nFigure 5.1 illustrates analysis might switch tidy non-tidy data structures tools. chapter focus process tidying document-term matrices, well casting tidy data frame sparse matrix. ’ll also explore tidy Corpus objects, combine raw text document metadata, text data frames, leading case study ingesting analyzing financial articles.","code":""},{"path":"dtm.html","id":"tidy-dtm","chapter":"5 Converting to and from non-tidy formats","heading":"5.1 Tidying a document-term matrix","text":"One common structures text mining packages work document-term matrix (DTM). matrix :row represents one document (book article),column represents one term, andeach value (typically) contains number appearances term document.Since pairings document term occur (value zero), DTMs usually implemented sparse matrices. objects can treated though matrices (example, accessing particular rows columns), stored efficient format. ’ll discuss several implementations matrices chapter.DTM objects used directly tidy tools, just tidy data frames used input text mining packages. Thus, tidytext package provides two verbs convert two formats.tidy() turns document-term matrix tidy data frame. verb comes broom package (Robinson 2017), provides similar tidying functions many statistical models objects.cast() turns tidy one-term-per-row data frame matrix. tidytext provides three variations verb, converting different type matrix: cast_sparse() (converting sparse matrix Matrix package), cast_dtm() (converting DocumentTermMatrix object tm), cast_dfm() (converting dfm object quanteda).shown Figure 5.1, DTM typically comparable tidy data frame count group_by/summarize contains counts another statistic combination term document.","code":""},{"path":"dtm.html","id":"tidying-documenttermmatrix-objects","chapter":"5 Converting to and from non-tidy formats","heading":"5.1.1 Tidying DocumentTermMatrix objects","text":"Perhaps widely used implementation DTMs R DocumentTermMatrix class tm package. Many available text mining datasets provided format. example, consider collection Associated Press newspaper articles included topicmodels package.see dataset contains documents (AP article) terms (distinct words). Notice DTM 99% sparse (99% document-word pairs zero). access terms document Terms() function.wanted analyze data tidy tools, first need turn data frame one-token-per-document-per-row. broom package introduced tidy() verb, takes non-tidy object turns tidy data frame. tidytext package implements method DocumentTermMatrix objects.Notice now tidy three-column tbl_df, variables document, term, count. tidying operation similar melt() function reshape2 package (Wickham 2007) non-sparse matrices.\nNotice non-zero values included tidied\noutput: document 1 includes terms “adding” “adult”, \n“aaron” “abandon”. means tidied version rows \ncount zero.\n’ve seen previous chapters, form convenient analysis dplyr, tidytext ggplot2 packages. example, can perform sentiment analysis newspaper articles approach described Chapter 2.let us visualize words AP articles often contributed positive negative sentiment, seen Figure 5.2. can see common positive words include “like”, “work”, “support”, “good”, negative words include “killed”, “death”, “vice”. (inclusion “vice” negative term probably mistake algorithm’s part, since likely usually refers “vice president”).\nFigure 5.2: Words AP articles greatest contribution positive negative sentiments, using Bing sentiment lexicon\n","code":"\nlibrary(tm)\n\ndata(\"AssociatedPress\", package = \"topicmodels\")\nAssociatedPress\n#> <<DocumentTermMatrix (documents: 2246, terms: 10473)>>\n#> Non-/sparse entries: 302031/23220327\n#> Sparsity           : 99%\n#> Maximal term length: 18\n#> Weighting          : term frequency (tf)\nterms <- Terms(AssociatedPress)\nhead(terms)\n#> [1] \"aaron\"      \"abandon\"    \"abandoned\"  \"abandoning\" \"abbott\"    \n#> [6] \"abboud\"\nlibrary(dplyr)\nlibrary(tidytext)\n\nap_td <- tidy(AssociatedPress)\nap_td\n#> # A tibble: 302,031 × 3\n#>    document term       count\n#>       <int> <chr>      <dbl>\n#>  1        1 adding         1\n#>  2        1 adult          2\n#>  3        1 ago            1\n#>  4        1 alcohol        1\n#>  5        1 allegedly      1\n#>  6        1 allen          1\n#>  7        1 apparently     2\n#>  8        1 appeared       1\n#>  9        1 arrested       1\n#> 10        1 assault        1\n#> # … with 302,021 more rows\nap_sentiments <- ap_td %>%\n  inner_join(get_sentiments(\"bing\"), by = c(term = \"word\"))\n\nap_sentiments\n#> # A tibble: 30,094 × 4\n#>    document term    count sentiment\n#>       <int> <chr>   <dbl> <chr>    \n#>  1        1 assault     1 negative \n#>  2        1 complex     1 negative \n#>  3        1 death       1 negative \n#>  4        1 died        1 negative \n#>  5        1 good        2 positive \n#>  6        1 illness     1 negative \n#>  7        1 killed      2 negative \n#>  8        1 like        2 positive \n#>  9        1 liked       1 positive \n#> 10        1 miracle     1 positive \n#> # … with 30,084 more rows\nlibrary(ggplot2)\n\nap_sentiments %>%\n  count(sentiment, term, wt = count) %>%\n  ungroup() %>%\n  filter(n >= 200) %>%\n  mutate(n = ifelse(sentiment == \"negative\", -n, n)) %>%\n  mutate(term = reorder(term, n)) %>%\n  ggplot(aes(n, term, fill = sentiment)) +\n  geom_col() +\n  labs(x = \"Contribution to sentiment\", y = NULL)"},{"path":"dtm.html","id":"tidying-dfm-objects","chapter":"5 Converting to and from non-tidy formats","heading":"5.1.2 Tidying dfm objects","text":"text mining packages provide alternative implementations document-term matrices, dfm (document-feature matrix) class quanteda package (Benoit Nulty 2016). example, quanteda package comes corpus presidential inauguration speeches, can converted dfm using appropriate functions.tidy method works document-feature matrices well, turning one-token-per-document-per-row table:may interested finding words specific inaugural speeches. quantified calculating tf-idf term-speech pair using bind_tf_idf() function, described Chapter 3.use data pick four notable inaugural addresses (Presidents Lincoln, Roosevelt, Kennedy, Obama), visualize words specific speech, shown Figure 5.3.\nFigure 5.3: terms highest tf-idf four selected inaugural addresses. Note quanteda’s tokenizer includes ‘?’ punctuation mark term, though texts ’ve tokenized unnest_tokens() .\nanother example visualization possible tidy data, extract year document’s name, compute total number words within year.\nNote ’ve used tidyr’s complete() function \ninclude zeroes (cases word didn’t appear document) \ntable.\nlets us pick several words visualize changed frequency time, shown 5.4. can see time, American presidents became less likely refer country “Union” likely refer “America”. also became less likely talk “constitution” foreign” countries, likely mention “freedom” “God”.\nFigure 5.4: Changes word frequency time within Presidential inaugural addresses, six selected terms\nexamples show can use tidytext, related suite tidy tools, analyze sources even origin tidy format.","code":"\ndata(\"data_corpus_inaugural\", package = \"quanteda\")\ninaug_dfm <- data_corpus_inaugural %>%\n  quanteda::tokens() %>%\n  quanteda::dfm(verbose = FALSE)\ninaug_dfm\n#> Document-feature matrix of: 59 documents, 9,439 features (91.84% sparse) and 4 docvars.\ninaug_td <- tidy(inaug_dfm)\ninaug_td\n#> # A tibble: 45,453 × 3\n#>    document        term            count\n#>    <chr>           <chr>           <dbl>\n#>  1 1789-Washington fellow-citizens     1\n#>  2 1797-Adams      fellow-citizens     3\n#>  3 1801-Jefferson  fellow-citizens     2\n#>  4 1809-Madison    fellow-citizens     1\n#>  5 1813-Madison    fellow-citizens     1\n#>  6 1817-Monroe     fellow-citizens     5\n#>  7 1821-Monroe     fellow-citizens     1\n#>  8 1841-Harrison   fellow-citizens    11\n#>  9 1845-Polk       fellow-citizens     1\n#> 10 1849-Taylor     fellow-citizens     1\n#> # … with 45,443 more rows\ninaug_tf_idf <- inaug_td %>%\n  bind_tf_idf(term, document, count) %>%\n  arrange(desc(tf_idf))\n\ninaug_tf_idf\n#> # A tibble: 45,453 × 6\n#>    document        term        count      tf   idf tf_idf\n#>    <chr>           <chr>       <dbl>   <dbl> <dbl>  <dbl>\n#>  1 1793-Washington arrive          1 0.00680  4.08 0.0277\n#>  2 1793-Washington upbraidings     1 0.00680  4.08 0.0277\n#>  3 1793-Washington violated        1 0.00680  3.38 0.0230\n#>  4 1793-Washington willingly       1 0.00680  3.38 0.0230\n#>  5 1793-Washington incurring       1 0.00680  3.38 0.0230\n#>  6 1793-Washington previous        1 0.00680  2.98 0.0203\n#>  7 1793-Washington knowingly       1 0.00680  2.98 0.0203\n#>  8 1793-Washington injunctions     1 0.00680  2.98 0.0203\n#>  9 1793-Washington witnesses       1 0.00680  2.98 0.0203\n#> 10 1793-Washington besides         1 0.00680  2.69 0.0183\n#> # … with 45,443 more rows\nlibrary(tidyr)\n\nyear_term_counts <- inaug_td %>%\n  extract(document, \"year\", \"(\\\\d+)\", convert = TRUE) %>%\n  complete(year, term, fill = list(count = 0)) %>%\n  group_by(year) %>%\n  mutate(year_total = sum(count))\nyear_term_counts %>%\n  filter(term %in% c(\"god\", \"america\", \"foreign\", \"union\", \"constitution\", \"freedom\")) %>%\n  ggplot(aes(year, count / year_total)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(~ term, scales = \"free_y\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(y = \"% frequency of word in inaugural address\")"},{"path":"dtm.html","id":"cast-dtm","chapter":"5 Converting to and from non-tidy formats","heading":"5.2 Casting tidy text data into a matrix","text":"Just existing text mining packages provide document-term matrices sample data output, algorithms expect matrices input. Therefore, tidytext provides cast_ verbs converting tidy form matrices.example, take tidied AP dataset cast back document-term matrix using cast_dtm() function.Similarly, cast table dfm object quanteda’s dfm cast_dfm().tools simply require sparse matrix:kind conversion easily done tidy text structures ’ve used far book. example, create DTM Jane Austen’s books just lines code.casting process allows reading, filtering, processing done using dplyr tidy tools, data can converted document-term matrix machine learning applications. Chapter 6, ’ll examine examples tidy-text dataset converted DocumentTermMatrix processing.","code":"\nap_td %>%\n  cast_dtm(document, term, count)\n#> <<DocumentTermMatrix (documents: 2246, terms: 10473)>>\n#> Non-/sparse entries: 302031/23220327\n#> Sparsity           : 99%\n#> Maximal term length: 18\n#> Weighting          : term frequency (tf)\nap_td %>%\n  cast_dfm(document, term, count)\n#> Document-feature matrix of: 2,246 documents, 10,473 features (98.72% sparse) and 0 docvars.\nlibrary(Matrix)\n\n# cast into a Matrix object\nm <- ap_td %>%\n  cast_sparse(document, term, count)\n\nclass(m)\n#> [1] \"dgCMatrix\"\n#> attr(,\"package\")\n#> [1] \"Matrix\"\ndim(m)\n#> [1]  2246 10473\nlibrary(janeaustenr)\n\nausten_dtm <- austen_books() %>%\n  unnest_tokens(word, text) %>%\n  count(book, word) %>%\n  cast_dtm(book, word, n)\n\nausten_dtm\n#> <<DocumentTermMatrix (documents: 6, terms: 14520)>>\n#> Non-/sparse entries: 40379/46741\n#> Sparsity           : 54%\n#> Maximal term length: 19\n#> Weighting          : term frequency (tf)"},{"path":"dtm.html","id":"tidying-corpus-objects-with-metadata","chapter":"5 Converting to and from non-tidy formats","heading":"5.3 Tidying corpus objects with metadata","text":"data structures designed store document collections tokenization, often called “corpus”. One common example Corpus objects tm package. store text alongside metadata, may include ID, date/time, title, language document.example, tm package comes acq corpus, containing 50 articles news service Reuters.corpus object structured like list, item containing text metadata (see tm documentation working Corpus documents). flexible storage method documents, doesn’t lend processing tidy tools.can thus use tidy() method construct table one row per document, including metadata (id datetimestamp) columns alongside text.can used unnest_tokens() , example, find common words across 50 Reuters articles, ones specific article.","code":"\ndata(\"acq\")\nacq\n#> <<VCorpus>>\n#> Metadata:  corpus specific: 0, document level (indexed): 0\n#> Content:  documents: 50\n\n# first document\nacq[[1]]\n#> <<PlainTextDocument>>\n#> Metadata:  15\n#> Content:  chars: 1287\nacq_td <- tidy(acq)\nacq_td\n#> # A tibble: 50 × 16\n#>    author   datetimestamp       description heading id    language origin topics\n#>    <chr>    <dttm>              <chr>       <chr>   <chr> <chr>    <chr>  <chr> \n#>  1 <NA>     1987-02-26 15:18:06 \"\"          COMPUT… 10    en       Reute… YES   \n#>  2 <NA>     1987-02-26 15:19:15 \"\"          OHIO M… 12    en       Reute… YES   \n#>  3 <NA>     1987-02-26 15:49:56 \"\"          MCLEAN… 44    en       Reute… YES   \n#>  4 By Cal … 1987-02-26 15:51:17 \"\"          CHEMLA… 45    en       Reute… YES   \n#>  5 <NA>     1987-02-26 16:08:33 \"\"          <COFAB… 68    en       Reute… YES   \n#>  6 <NA>     1987-02-26 16:32:37 \"\"          INVEST… 96    en       Reute… YES   \n#>  7 By Patt… 1987-02-26 16:43:13 \"\"          AMERIC… 110   en       Reute… YES   \n#>  8 <NA>     1987-02-26 16:59:25 \"\"          HONG K… 125   en       Reute… YES   \n#>  9 <NA>     1987-02-26 17:01:28 \"\"          LIEBER… 128   en       Reute… YES   \n#> 10 <NA>     1987-02-26 17:08:27 \"\"          GULF A… 134   en       Reute… YES   \n#> # … with 40 more rows, and 8 more variables: lewissplit <chr>, cgisplit <chr>,\n#> #   oldid <chr>, places <named list>, people <lgl>, orgs <lgl>,\n#> #   exchanges <lgl>, text <chr>\nacq_tokens <- acq_td %>%\n  select(-places) %>%\n  unnest_tokens(word, text) %>%\n  anti_join(stop_words, by = \"word\")\n\n# most common words\nacq_tokens %>%\n  count(word, sort = TRUE)\n#> # A tibble: 1,566 × 2\n#>    word         n\n#>    <chr>    <int>\n#>  1 dlrs       100\n#>  2 pct         70\n#>  3 mln         65\n#>  4 company     63\n#>  5 shares      52\n#>  6 reuter      50\n#>  7 stock       46\n#>  8 offer       34\n#>  9 share       34\n#> 10 american    28\n#> # … with 1,556 more rows\n\n# tf-idf\nacq_tokens %>%\n  count(id, word) %>%\n  bind_tf_idf(word, id, n) %>%\n  arrange(desc(tf_idf))\n#> # A tibble: 2,853 × 6\n#>    id    word         n     tf   idf tf_idf\n#>    <chr> <chr>    <int>  <dbl> <dbl>  <dbl>\n#>  1 186   groupe       2 0.133   3.91  0.522\n#>  2 128   liebert      3 0.130   3.91  0.510\n#>  3 474   esselte      5 0.109   3.91  0.425\n#>  4 371   burdett      6 0.103   3.91  0.405\n#>  5 442   hazleton     4 0.103   3.91  0.401\n#>  6 199   circuit      5 0.102   3.91  0.399\n#>  7 162   suffield     2 0.1     3.91  0.391\n#>  8 498   west         3 0.1     3.91  0.391\n#>  9 441   rmj          8 0.121   3.22  0.390\n#> 10 467   nursery      3 0.0968  3.91  0.379\n#> # … with 2,843 more rows"},{"path":"dtm.html","id":"financial","chapter":"5 Converting to and from non-tidy formats","heading":"5.3.1 Example: mining financial articles","text":"Corpus objects common output format data ingesting packages, means tidy() function gives us access wide variety text data. One example tm.plugin.webmining, connects online feeds retrieve news articles based keyword. instance, performing WebCorpus(GoogleFinanceSource(\"NASDAQ:MSFT\")) allows us retrieve 20 recent articles related Microsoft (MSFT) stock.’ll retrieve recent articles relevant nine major technology stocks: Microsoft, Apple, Google, Amazon, Facebook, Twitter, IBM, Yahoo, Netflix.\nresults downloaded January 2017, chapter \nwritten, ’ll certainly find different results ran \n. Note code takes several minutes run.\nuses map() function purrr package, applies function item symbol create list, store corpus list column.items corpus list column WebCorpus object, special case corpus like acq. can thus turn data frame using tidy() function, unnest tidyr’s unnest(), tokenize text column individual articles using unnest_tokens().see article’s metadata alongside words used. use tf-idf determine words specific stock symbol.top terms visualized Figure 5.5. ’d expect, company’s name symbol typically included, several product offerings executives, well companies making deals (Disney Netflix).\nFigure 5.5: 8 words highest tf-idf recent articles specific company\ninterested using recent news analyze market make investment decisions, ’d likely want use sentiment analysis determine whether news coverage positive negative. run analysis, look words contribute positive negative sentiments, shown Chapter 2.4. example, examine within AFINN lexicon (Figure 5.6).\nFigure 5.6: words largest contribution sentiment values recent financial articles, according AFINN dictionary. ‘contribution’ product word sentiment score.\ncontext financial articles, big red flags . words “share” “shares” counted positive verbs AFINN lexicon (“Alice share cake Bob”), ’re actually neutral nouns (“stock price $12 per share”) just easily positive sentence negative one. word “fool” even deceptive: refers Motley Fool, financial services company. short, can see AFINN sentiment lexicon entirely unsuited context financial data (NRC Bing lexicons).Instead, introduce another sentiment lexicon: Loughran McDonald dictionary financial sentiment terms (Loughran McDonald 2011). dictionary developed based analyses financial reports, intentionally avoids words like “share” “fool”, well subtler terms like “liability” “risk” may negative meaning financial context.Loughran data divides words six sentiments: “positive”, “negative”, “litigious”, “uncertain”, “constraining”, “superfluous”. start examining common words belonging sentiment within text dataset.\nFigure 5.7: common words financial news articles associated six sentiments Loughran McDonald lexicon\nassignments (Figure 5.7) words sentiments look reasonable: common positive words include “strong” “better”, “shares” “growth”, negative words include “volatility” “fool”. sentiments look reasonable well: common “uncertainty” terms include “” “may”.Now know can trust dictionary approximate articles’ sentiments, can use typical methods counting number uses sentiment-associated word corpus.might interesting examine company news “litigious” “uncertain” terms. simplest measure, much analysis Chapter 2, see whether news positive negative. general quantitative measure sentiment, ’ll use “(positive - negative) / (positive + negative)” (Figure 5.8).\nFigure 5.8: “Positivity” news coverage around stock January 2017, calculated (positive - negative) / (positive + negative), based uses positive negative words 20 recent news articles company\nBased analysis, ’d say January 2017 coverage Yahoo Twitter strongly negative, coverage Google Amazon positive. glance current financial headlines suggest ’s right track. interested analysis, use one R’s many quantitative finance packages compare articles recent stock prices metrics.","code":"\nlibrary(tm.plugin.webmining)\nlibrary(purrr)\n\ncompany <- c(\"Microsoft\", \"Apple\", \"Google\", \"Amazon\", \"Facebook\",\n             \"Twitter\", \"IBM\", \"Yahoo\", \"Netflix\")\nsymbol  <- c(\"MSFT\", \"AAPL\", \"GOOG\", \"AMZN\", \"FB\", \n             \"TWTR\", \"IBM\", \"YHOO\", \"NFLX\")\n\ndownload_articles <- function(symbol) {\n  WebCorpus(GoogleFinanceSource(paste0(\"NASDAQ:\", symbol)))\n}\n\nstock_articles <- tibble(company = company,\n                         symbol = symbol) %>%\n  mutate(corpus = map(symbol, download_articles))\nstock_articles\n#> # A tibble: 9 × 3\n#>   company   symbol corpus    \n#>   <chr>     <chr>  <list>    \n#> 1 Microsoft MSFT   <WebCorps>\n#> 2 Apple     AAPL   <WebCorps>\n#> 3 Google    GOOG   <WebCorps>\n#> 4 Amazon    AMZN   <WebCorps>\n#> 5 Facebook  FB     <WebCorps>\n#> 6 Twitter   TWTR   <WebCorps>\n#> 7 IBM       IBM    <WebCorps>\n#> 8 Yahoo     YHOO   <WebCorps>\n#> 9 Netflix   NFLX   <WebCorps>\nstock_tokens <- stock_articles %>%\n  mutate(corpus = map(corpus, tidy)) %>%\n  unnest(cols = (corpus)) %>%\n  unnest_tokens(word, text) %>%\n  select(company, datetimestamp, word, id, heading)\n\nstock_tokens\n#> # A tibble: 105,057 × 5\n#>    company   datetimestamp       word        id                          heading\n#>    <chr>     <dttm>              <chr>       <chr>                       <chr>  \n#>  1 Microsoft 2017-01-17 12:07:24 microsoft   tag:finance.google.com,clu… Micros…\n#>  2 Microsoft 2017-01-17 12:07:24 corporation tag:finance.google.com,clu… Micros…\n#>  3 Microsoft 2017-01-17 12:07:24 data        tag:finance.google.com,clu… Micros…\n#>  4 Microsoft 2017-01-17 12:07:24 privacy     tag:finance.google.com,clu… Micros…\n#>  5 Microsoft 2017-01-17 12:07:24 could       tag:finance.google.com,clu… Micros…\n#>  6 Microsoft 2017-01-17 12:07:24 send        tag:finance.google.com,clu… Micros…\n#>  7 Microsoft 2017-01-17 12:07:24 msft        tag:finance.google.com,clu… Micros…\n#>  8 Microsoft 2017-01-17 12:07:24 stock       tag:finance.google.com,clu… Micros…\n#>  9 Microsoft 2017-01-17 12:07:24 soaring     tag:finance.google.com,clu… Micros…\n#> 10 Microsoft 2017-01-17 12:07:24 by          tag:finance.google.com,clu… Micros…\n#> # … with 105,047 more rows\nlibrary(stringr)\n\nstock_tf_idf <- stock_tokens %>%\n  count(company, word) %>%\n  filter(!str_detect(word, \"\\\\d+\")) %>%\n  bind_tf_idf(word, company, n) %>%\n  arrange(-tf_idf)\nstock_tokens %>%\n  anti_join(stop_words, by = \"word\") %>%\n  count(word, id, sort = TRUE) %>%\n  inner_join(get_sentiments(\"afinn\"), by = \"word\") %>%\n  group_by(word) %>%\n  summarize(contribution = sum(n * value)) %>%\n  slice_max(abs(contribution), n = 12) %>%\n  mutate(word = reorder(word, contribution)) %>%\n  ggplot(aes(contribution, word)) +\n  geom_col() +\n  labs(x = \"Frequency of word * AFINN value\", y = NULL)\nstock_tokens %>%\n  count(word) %>%\n  inner_join(get_sentiments(\"loughran\"), by = \"word\") %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 5, with_ties = FALSE) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  facet_wrap(~ sentiment, scales = \"free\") +\n  labs(x = \"Frequency of this word in the recent financial articles\", y = NULL)\nstock_sentiment_count <- stock_tokens %>%\n  inner_join(get_sentiments(\"loughran\"), by = \"word\") %>%\n  count(sentiment, company) %>%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)\n\nstock_sentiment_count#> # A tibble: 9 × 7\n#>   company   constraining litigious negative positive superfluous uncertainty\n#>   <chr>            <int>     <int>    <int>    <int>       <int>       <int>\n#> 1 Amazon               7         8       84      144           3          70\n#> 2 Apple                9        11      161      156           2         132\n#> 3 Facebook             4        32      128      150           4          81\n#> 4 Google               7         8       60      103           0          58\n#> 5 IBM                  8        22      147      148           0         104\n#> 6 Microsoft            6        19       92      129           3         116\n#> 7 Netflix              4         7      111      162           0         106\n#> 8 Twitter              4        12      157       79           1          75\n#> 9 Yahoo                3        28      130       74           0          71\nstock_sentiment_count %>%\n  mutate(score = (positive - negative) / (positive + negative)) %>%\n  mutate(company = reorder(company, score)) %>%\n  ggplot(aes(score, company, fill = score > 0)) +\n  geom_col(show.legend = FALSE) +\n  labs(x = \"Positivity score among 20 recent news articles\", y = NULL)"},{"path":"dtm.html","id":"summary-4","chapter":"5 Converting to and from non-tidy formats","heading":"5.4 Summary","text":"Text analysis requires working variety tools, many inputs outputs aren’t tidy form. chapter showed convert tidy text data frame sparse document-term matrices, well tidy Corpus object containing document metadata. next chapter demonstrate another notable example package, topicmodels, requires document-term matrix input, showing conversion tools essential part text analysis.","code":""},{"path":"topicmodeling.html","id":"topicmodeling","chapter":"6 Topic modeling","heading":"6 Topic modeling","text":"text mining, often collections documents, blog posts news articles, ’d like divide natural groups can understand separately. Topic modeling method unsupervised classification documents, similar clustering numeric data, finds natural groups items even ’re sure ’re looking .Latent Dirichlet allocation (LDA) particularly popular method fitting topic model. treats document mixture topics, topic mixture words. allows documents “overlap” terms content, rather separated discrete groups, way mirrors typical use natural language.\nFigure 6.1: flowchart text analysis incorporates topic modeling. topicmodels package takes Document-Term Matrix input produces model can tided tidytext, can manipulated visualized dplyr ggplot2.\nFigure 6.1 shows, can use tidy text principles approach topic modeling set tidy tools ’ve used throughout book. chapter, ’ll learn work LDA objects topicmodels package, particularly tidying models can manipulated ggplot2 dplyr. ’ll also explore example clustering chapters several books, can see topic model “learns” tell difference four books based text content.","code":""},{"path":"topicmodeling.html","id":"latent-dirichlet-allocation","chapter":"6 Topic modeling","heading":"6.1 Latent Dirichlet allocation","text":"Latent Dirichlet allocation one common algorithms topic modeling. Without diving math behind model, can understand guided two principles.Every document mixture topics. imagine document may contain words several topics particular proportions. example, two-topic model say “Document 1 90% topic 10% topic B, Document 2 30% topic 70% topic B.”Every topic mixture words. example, imagine two-topic model American news, one topic “politics” one “entertainment.” common words politics topic might “President”, “Congress”, “government”, entertainment topic may made words “movies”, “television”, “actor”. Importantly, words can shared topics; word like “budget” might appear equally.LDA mathematical method estimating time: finding mixture words associated topic, also determining mixture topics describes document. number existing implementations algorithm, ’ll explore one depth.Chapter 5 briefly introduced AssociatedPress dataset provided topicmodels package, example DocumentTermMatrix. collection 2246 news articles American news agency, mostly published around 1988.can use LDA() function topicmodels package, setting k = 2, create two-topic LDA model.\nAlmost topic model practice use larger k,\nsoon see analysis approach extends larger\nnumber topics.\nfunction returns object containing full details model fit, words associated topics topics associated documents.Fitting model “easy part”: rest analysis involve exploring interpreting model using tidying functions tidytext package.","code":"\nlibrary(topicmodels)\n\ndata(\"AssociatedPress\")\nAssociatedPress\n#> <<DocumentTermMatrix (documents: 2246, terms: 10473)>>\n#> Non-/sparse entries: 302031/23220327\n#> Sparsity           : 99%\n#> Maximal term length: 18\n#> Weighting          : term frequency (tf)\n# set a seed so that the output of the model is predictable\nap_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))\nap_lda\n#> A LDA_VEM topic model with 2 topics."},{"path":"topicmodeling.html","id":"word-topic-probabilities","chapter":"6 Topic modeling","heading":"6.1.1 Word-topic probabilities","text":"Chapter 5 introduced tidy() method, originally broom package (Robinson 2017), tidying model objects. tidytext package provides method extracting per-topic-per-word probabilities, called \\(\\beta\\) (“beta”), model.Notice turned model one-topic-per-term-per-row format. combination, model computes probability term generated topic. example, term “aaron” \\(1.686917\\times 10^{-12}\\) probability generated topic 1, \\(3.8959408\\times 10^{-5}\\) probability generated topic 2.use dplyr’s slice_max() find 10 terms common within topic. tidy data frame, lends well ggplot2 visualization (Figure 6.2).\nFigure 6.2: terms common within topic\nvisualization lets us understand two topics extracted articles. common words topic 1 include “percent”, “million”, “billion”, “company”, suggests may represent business financial news. common topic 2 include “president”, “government”, “soviet”, suggesting topic represents political news. One important observation words topic words, “new” “people”, common within topics. advantage topic modeling opposed “hard clustering” methods: topics used natural language overlap terms words.alternative, consider terms greatest difference \\(\\beta\\) topic 1 topic 2. can estimated based log ratio two: \\(\\log_2(\\frac{\\beta_2}{\\beta_1})\\) (log ratio useful makes difference symmetrical: \\(\\beta_2\\) twice large leads log ratio 1, \\(\\beta_1\\) twice large results -1). constrain set especially relevant words, can filter relatively common words, \\(\\beta\\) greater 1/1000 least one topic.words greatest differences two topics visualized Figure 6.3.\nFigure 6.3: Words greatest difference \\(\\beta\\) topic 2 topic 1\ncan see words common topic 2 include political parties “democratic” “republican”, well politician’s names “dukakis” “gorbachev”. Topic 1 characterized currencies like “yen” “dollar”, well financial terms “index”, “prices” “rates”. helps confirm two topics algorithm identified political financial news.","code":"\nlibrary(tidytext)\n\nap_topics <- tidy(ap_lda, matrix = \"beta\")\nap_topics\n#> # A tibble: 20,946 × 3\n#>    topic term           beta\n#>    <int> <chr>         <dbl>\n#>  1     1 aaron      1.69e-12\n#>  2     2 aaron      3.90e- 5\n#>  3     1 abandon    2.65e- 5\n#>  4     2 abandon    3.99e- 5\n#>  5     1 abandoned  1.39e- 4\n#>  6     2 abandoned  5.88e- 5\n#>  7     1 abandoning 2.45e-33\n#>  8     2 abandoning 2.34e- 5\n#>  9     1 abbott     2.13e- 6\n#> 10     2 abbott     2.97e- 5\n#> # … with 20,936 more rows\nlibrary(ggplot2)\nlibrary(dplyr)\n\nap_top_terms <- ap_topics %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\nap_top_terms %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\nlibrary(tidyr)\n\nbeta_wide <- ap_topics %>%\n  mutate(topic = paste0(\"topic\", topic)) %>%\n  pivot_wider(names_from = topic, values_from = beta) %>% \n  filter(topic1 > .001 | topic2 > .001) %>%\n  mutate(log_ratio = log2(topic2 / topic1))\n\nbeta_wide\n#> # A tibble: 198 × 4\n#>    term              topic1      topic2 log_ratio\n#>    <chr>              <dbl>       <dbl>     <dbl>\n#>  1 administration 0.000431  0.00138         1.68 \n#>  2 ago            0.00107   0.000842       -0.339\n#>  3 agreement      0.000671  0.00104         0.630\n#>  4 aid            0.0000476 0.00105         4.46 \n#>  5 air            0.00214   0.000297       -2.85 \n#>  6 american       0.00203   0.00168        -0.270\n#>  7 analysts       0.00109   0.000000578   -10.9  \n#>  8 area           0.00137   0.000231       -2.57 \n#>  9 army           0.000262  0.00105         2.00 \n#> 10 asked          0.000189  0.00156         3.05 \n#> # … with 188 more rows"},{"path":"topicmodeling.html","id":"document-topic-probabilities","chapter":"6 Topic modeling","heading":"6.1.2 Document-topic probabilities","text":"Besides estimating topic mixture words, LDA also models document mixture topics. can examine per-document-per-topic probabilities, called \\(\\gamma\\) (“gamma”), matrix = \"gamma\" argument tidy().values estimated proportion words document generated topic. example, model estimates 25% words document 1 generated topic 1.can see many documents drawn mix two topics, document 6 drawn almost entirely topic 2, \\(\\gamma\\) topic 1 close zero. check answer, tidy() document-term matrix (see Chapter 5.1) check common words document .Based common words, appears article relationship American government Panamanian dictator Manuel Noriega, means algorithm right place topic 2 (political/national news).","code":"\nap_documents <- tidy(ap_lda, matrix = \"gamma\")\nap_documents\n#> # A tibble: 4,492 × 3\n#>    document topic    gamma\n#>       <int> <int>    <dbl>\n#>  1        1     1 0.248   \n#>  2        2     1 0.362   \n#>  3        3     1 0.527   \n#>  4        4     1 0.357   \n#>  5        5     1 0.181   \n#>  6        6     1 0.000588\n#>  7        7     1 0.773   \n#>  8        8     1 0.00445 \n#>  9        9     1 0.967   \n#> 10       10     1 0.147   \n#> # … with 4,482 more rows\ntidy(AssociatedPress) %>%\n  filter(document == 6) %>%\n  arrange(desc(count))\n#> # A tibble: 287 × 3\n#>    document term           count\n#>       <int> <chr>          <dbl>\n#>  1        6 noriega           16\n#>  2        6 panama            12\n#>  3        6 jackson            6\n#>  4        6 powell             6\n#>  5        6 administration     5\n#>  6        6 economic           5\n#>  7        6 general            5\n#>  8        6 i                  5\n#>  9        6 panamanian         5\n#> 10        6 american           4\n#> # … with 277 more rows"},{"path":"topicmodeling.html","id":"library-heist","chapter":"6 Topic modeling","heading":"6.2 Example: the great library heist","text":"examining statistical method, can useful try simple case know “right answer”. example, collect set documents definitely relate four separate topics, perform topic modeling see whether algorithm can correctly distinguish four groups. lets us double-check method useful, gain sense can go wrong. ’ll try data classic literature.Suppose vandal broken study torn apart four books:Great Expectations Charles DickensThe War Worlds H.G. WellsTwenty Thousand Leagues Sea Jules VernePride Prejudice Jane AustenThis vandal torn books individual chapters, left one large pile. can restore disorganized chapters original books? challenging problem since individual chapters unlabeled: don’t know words might distinguish groups. ’ll thus use topic modeling discover chapters cluster distinct topics, (presumably) representing one books.’ll retrieve text four books using gutenbergr package introduced Chapter 3.pre-processing, divide chapters, use tidytext’s unnest_tokens() separate words, remove stop_words. ’re treating every chapter separate “document”, name like Great Expectations_1 Pride Prejudice_11. (applications, document might one newspaper article, one blog post).","code":"\ntitles <- c(\"Twenty Thousand Leagues under the Sea\", \n            \"The War of the Worlds\",\n            \"Pride and Prejudice\", \n            \"Great Expectations\")\nlibrary(gutenbergr)\n\nbooks <- gutenberg_works(title %in% titles) %>%\n  gutenberg_download(meta_fields = \"title\")\nlibrary(stringr)\n\n# divide into documents, each representing one chapter\nby_chapter <- books %>%\n  group_by(title) %>%\n  mutate(chapter = cumsum(str_detect(\n    text, regex(\"^chapter \", ignore_case = TRUE)\n  ))) %>%\n  ungroup() %>%\n  filter(chapter > 0) %>%\n  unite(document, title, chapter)\n\n# split into words\nby_chapter_word <- by_chapter %>%\n  unnest_tokens(word, text)\n\n# find document-word counts\nword_counts <- by_chapter_word %>%\n  anti_join(stop_words) %>%\n  count(document, word, sort = TRUE)\n\nword_counts\n#> # A tibble: 104,721 × 3\n#>    document                 word        n\n#>    <chr>                    <chr>   <int>\n#>  1 Great Expectations_57    joe        88\n#>  2 Great Expectations_7     joe        70\n#>  3 Great Expectations_17    biddy      63\n#>  4 Great Expectations_27    joe        58\n#>  5 Great Expectations_38    estella    58\n#>  6 Great Expectations_2     joe        56\n#>  7 Great Expectations_23    pocket     53\n#>  8 Great Expectations_15    joe        50\n#>  9 Great Expectations_18    joe        50\n#> 10 The War of the Worlds_16 brother    50\n#> # … with 104,711 more rows"},{"path":"topicmodeling.html","id":"lda-on-chapters","chapter":"6 Topic modeling","heading":"6.2.1 LDA on chapters","text":"Right now data frame word_counts tidy form, one-term-per-document-per-row, topicmodels package requires DocumentTermMatrix. described Chapter 5.2, can cast one-token-per-row table DocumentTermMatrix tidytext’s cast_dtm().can use LDA() function create four-topic model. case know ’re looking four topics four books; problems may need try different values k.Much Associated Press data, can examine per-topic-per-word probabilities.Notice turned model one-topic-per-term-per-row format. combination, model computes probability term generated topic. example, term “joe” almost zero probability generated topics 1, 2, 3, makes 1% topic 4.use dplyr’s slice_max() find top 5 terms within topic.tidy output lends well ggplot2 visualization (Figure 6.4).\nFigure 6.4: terms common within topic\ntopics pretty clearly associated four books! ’s question topic “captain”, “nautilus”, “sea”, “nemo” belongs Twenty Thousand Leagues Sea, “jane”, “darcy”, “elizabeth” belongs Pride Prejudice. see “pip” “joe” Great Expectations “martians”, “black”, “night” War Worlds. also notice , line LDA “fuzzy clustering” method, can words common multiple topics, “miss” topics 1 4, “time” topics 3 4.","code":"\nchapters_dtm <- word_counts %>%\n  cast_dtm(document, word, n)\n\nchapters_dtm\n#> <<DocumentTermMatrix (documents: 193, terms: 18215)>>\n#> Non-/sparse entries: 104721/3410774\n#> Sparsity           : 97%\n#> Maximal term length: 19\n#> Weighting          : term frequency (tf)\nchapters_lda <- LDA(chapters_dtm, k = 4, control = list(seed = 1234))\nchapters_lda\n#> A LDA_VEM topic model with 4 topics.\nchapter_topics <- tidy(chapters_lda, matrix = \"beta\")\nchapter_topics\n#> # A tibble: 72,860 × 3\n#>    topic term        beta\n#>    <int> <chr>      <dbl>\n#>  1     1 joe     5.83e-17\n#>  2     2 joe     3.19e-57\n#>  3     3 joe     4.16e-24\n#>  4     4 joe     1.45e- 2\n#>  5     1 biddy   7.85e-27\n#>  6     2 biddy   4.67e-69\n#>  7     3 biddy   2.26e-46\n#>  8     4 biddy   4.77e- 3\n#>  9     1 estella 3.83e- 6\n#> 10     2 estella 5.32e-65\n#> # … with 72,850 more rows\ntop_terms <- chapter_topics %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 5) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\ntop_terms\n#> # A tibble: 20 × 3\n#>    topic term         beta\n#>    <int> <chr>       <dbl>\n#>  1     1 elizabeth 0.0141 \n#>  2     1 darcy     0.00881\n#>  3     1 miss      0.00871\n#>  4     1 bennet    0.00695\n#>  5     1 jane      0.00650\n#>  6     2 captain   0.0155 \n#>  7     2 nautilus  0.0131 \n#>  8     2 sea       0.00885\n#>  9     2 nemo      0.00871\n#> 10     2 ned       0.00803\n#> 11     3 people    0.00680\n#> 12     3 martians  0.00651\n#> 13     3 time      0.00535\n#> 14     3 black     0.00528\n#> 15     3 night     0.00448\n#> 16     4 joe       0.0145 \n#> 17     4 time      0.00685\n#> 18     4 pip       0.00682\n#> 19     4 looked    0.00637\n#> 20     4 miss      0.00623\nlibrary(ggplot2)\n\ntop_terms %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()"},{"path":"topicmodeling.html","id":"per-document","chapter":"6 Topic modeling","heading":"6.2.2 Per-document classification","text":"document analysis represented single chapter. Thus, may want know topics associated document. Can put chapters back together correct books? can find examining per-document-per-topic probabilities, \\(\\gamma\\) (“gamma”).values estimated proportion words document generated topic. example, model estimates word Great Expectations_57 document 0% probability coming topic 1 (Pride Prejudice).Now topic probabilities, can see well unsupervised learning distinguishing four books. ’d expect chapters within book found mostly (entirely), generated corresponding topic.First re-separate document name title chapter, can visualize per-document-per-topic probability (Figure 6.5).\nFigure 6.5: gamma probabilities chapter within book\nnotice almost chapters Pride Prejudice, War Worlds, Twenty Thousand Leagues Sea uniquely identified single topic .look like chapters Great Expectations (topic 4) somewhat associated topics. cases topic associated chapter belonged another book? First ’d find topic associated chapter using slice_max(), effectively “classification” chapter.can compare “consensus” topic book (common topic among chapters), see often misidentified.see two chapters Great Expectations misclassified, LDA described one coming “Pride Prejudice” topic (topic 1) one War Worlds (topic 3). ’s bad unsupervised clustering!","code":"\nchapters_gamma <- tidy(chapters_lda, matrix = \"gamma\")\nchapters_gamma\n#> # A tibble: 772 × 3\n#>    document                 topic     gamma\n#>    <chr>                    <int>     <dbl>\n#>  1 Great Expectations_57        1 0.0000135\n#>  2 Great Expectations_7         1 0.0000147\n#>  3 Great Expectations_17        1 0.0000212\n#>  4 Great Expectations_27        1 0.0000192\n#>  5 Great Expectations_38        1 0.354    \n#>  6 Great Expectations_2         1 0.0000172\n#>  7 Great Expectations_23        1 0.551    \n#>  8 Great Expectations_15        1 0.0168   \n#>  9 Great Expectations_18        1 0.0000127\n#> 10 The War of the Worlds_16     1 0.0000108\n#> # … with 762 more rows\nchapters_gamma <- chapters_gamma %>%\n  separate(document, c(\"title\", \"chapter\"), sep = \"_\", convert = TRUE)\n\nchapters_gamma\n#> # A tibble: 772 × 4\n#>    title                 chapter topic     gamma\n#>    <chr>                   <int> <int>     <dbl>\n#>  1 Great Expectations         57     1 0.0000135\n#>  2 Great Expectations          7     1 0.0000147\n#>  3 Great Expectations         17     1 0.0000212\n#>  4 Great Expectations         27     1 0.0000192\n#>  5 Great Expectations         38     1 0.354    \n#>  6 Great Expectations          2     1 0.0000172\n#>  7 Great Expectations         23     1 0.551    \n#>  8 Great Expectations         15     1 0.0168   \n#>  9 Great Expectations         18     1 0.0000127\n#> 10 The War of the Worlds      16     1 0.0000108\n#> # … with 762 more rows\n# reorder titles in order of topic 1, topic 2, etc before plotting\nchapters_gamma %>%\n  mutate(title = reorder(title, gamma * topic)) %>%\n  ggplot(aes(factor(topic), gamma)) +\n  geom_boxplot() +\n  facet_wrap(~ title) +\n  labs(x = \"topic\", y = expression(gamma))\nchapter_classifications <- chapters_gamma %>%\n  group_by(title, chapter) %>%\n  slice_max(gamma) %>%\n  ungroup()\n\nchapter_classifications\n#> # A tibble: 193 × 4\n#>    title              chapter topic gamma\n#>    <chr>                <int> <int> <dbl>\n#>  1 Great Expectations       1     4 0.821\n#>  2 Great Expectations       2     4 1.00 \n#>  3 Great Expectations       3     4 0.687\n#>  4 Great Expectations       4     4 1.00 \n#>  5 Great Expectations       5     4 0.782\n#>  6 Great Expectations       6     4 1.00 \n#>  7 Great Expectations       7     4 1.00 \n#>  8 Great Expectations       8     4 0.686\n#>  9 Great Expectations       9     4 0.992\n#> 10 Great Expectations      10     4 1.00 \n#> # … with 183 more rows\nbook_topics <- chapter_classifications %>%\n  count(title, topic) %>%\n  group_by(title) %>%\n  slice_max(n, n = 1) %>% \n  ungroup() %>%\n  transmute(consensus = title, topic)\n\nchapter_classifications %>%\n  inner_join(book_topics, by = \"topic\") %>%\n  filter(title != consensus)\n#> # A tibble: 2 × 5\n#>   title              chapter topic gamma consensus            \n#>   <chr>                <int> <int> <dbl> <chr>                \n#> 1 Great Expectations      23     1 0.551 Pride and Prejudice  \n#> 2 Great Expectations      54     3 0.480 The War of the Worlds"},{"path":"topicmodeling.html","id":"by-word-assignments-augment","chapter":"6 Topic modeling","heading":"6.2.3 By word assignments: augment","text":"One step LDA algorithm assigning word document topic. words document assigned topic, generally, weight (gamma) go document-topic classification.may want take original document-word pairs find words document assigned topic. job augment() function, also originated broom package way tidying model output. tidy() retrieves statistical components model, augment() uses model add information observation original data.returns tidy data frame book-term counts, adds extra column: .topic, topic term assigned within document. (Extra columns added augment always start ., prevent overwriting existing columns). can combine assignments table consensus book titles find words incorrectly classified.combination true book (title) book assigned (consensus) useful exploration. can, example, visualize confusion matrix, showing often words one book assigned another, using dplyr’s count() ggplot2’s geom_tile (Figure 6.6).\nFigure 6.6: Confusion matrix showing LDA assigned words book. row table represents true book word came , column represents book assigned .\nnotice almost words Pride Prejudice, Twenty Thousand Leagues Sea, War Worlds correctly assigned, Great Expectations fair number misassigned words (, saw , led two chapters getting misclassified).commonly mistaken words?can see number words often assigned Pride Prejudice War Worlds cluster even appeared Great Expectations. words, “love” “lady”, ’s ’re common Pride Prejudice (confirm examining counts).hand, wrongly classified words never appeared novel misassigned . example, can confirm “flopson” appears Great Expectations, even though ’s assigned “Pride Prejudice” cluster.LDA algorithm stochastic, can accidentally land topic spans multiple books.","code":"\nassignments <- augment(chapters_lda, data = chapters_dtm)\nassignments\n#> # A tibble: 104,721 × 4\n#>    document              term  count .topic\n#>    <chr>                 <chr> <dbl>  <dbl>\n#>  1 Great Expectations_57 joe      88      4\n#>  2 Great Expectations_7  joe      70      4\n#>  3 Great Expectations_17 joe       5      4\n#>  4 Great Expectations_27 joe      58      4\n#>  5 Great Expectations_2  joe      56      4\n#>  6 Great Expectations_23 joe       1      4\n#>  7 Great Expectations_15 joe      50      4\n#>  8 Great Expectations_18 joe      50      4\n#>  9 Great Expectations_9  joe      44      4\n#> 10 Great Expectations_13 joe      40      4\n#> # … with 104,711 more rows\nassignments <- assignments %>%\n  separate(document, c(\"title\", \"chapter\"), \n           sep = \"_\", convert = TRUE) %>%\n  inner_join(book_topics, by = c(\".topic\" = \"topic\"))\n\nassignments\n#> # A tibble: 104,721 × 6\n#>    title              chapter term  count .topic consensus         \n#>    <chr>                <int> <chr> <dbl>  <dbl> <chr>             \n#>  1 Great Expectations      57 joe      88      4 Great Expectations\n#>  2 Great Expectations       7 joe      70      4 Great Expectations\n#>  3 Great Expectations      17 joe       5      4 Great Expectations\n#>  4 Great Expectations      27 joe      58      4 Great Expectations\n#>  5 Great Expectations       2 joe      56      4 Great Expectations\n#>  6 Great Expectations      23 joe       1      4 Great Expectations\n#>  7 Great Expectations      15 joe      50      4 Great Expectations\n#>  8 Great Expectations      18 joe      50      4 Great Expectations\n#>  9 Great Expectations       9 joe      44      4 Great Expectations\n#> 10 Great Expectations      13 joe      40      4 Great Expectations\n#> # … with 104,711 more rows\nlibrary(scales)\n\nassignments %>%\n  count(title, consensus, wt = count) %>%\n  mutate(across(c(title, consensus), ~str_wrap(., 20))) %>%\n  group_by(title) %>%\n  mutate(percent = n / sum(n)) %>%\n  ggplot(aes(consensus, title, fill = percent)) +\n  geom_tile() +\n  scale_fill_gradient2(high = \"darkred\", label = percent_format()) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        panel.grid = element_blank()) +\n  labs(x = \"Book words were assigned to\",\n       y = \"Book words came from\",\n       fill = \"% of assignments\")\nwrong_words <- assignments %>%\n  filter(title != consensus)\n\nwrong_words\n#> # A tibble: 4,535 × 6\n#>    title                                 chapter term     count .topic consensus\n#>    <chr>                                   <int> <chr>    <dbl>  <dbl> <chr>    \n#>  1 Great Expectations                         38 brother      2      1 Pride an…\n#>  2 Great Expectations                         22 brother      4      1 Pride an…\n#>  3 Great Expectations                         23 miss         2      1 Pride an…\n#>  4 Great Expectations                         22 miss        23      1 Pride an…\n#>  5 Twenty Thousand Leagues under the Sea       8 miss         1      1 Pride an…\n#>  6 Great Expectations                         31 miss         1      1 Pride an…\n#>  7 Great Expectations                          5 sergeant    37      1 Pride an…\n#>  8 Great Expectations                         46 captain      1      2 Twenty T…\n#>  9 Great Expectations                         32 captain      1      2 Twenty T…\n#> 10 The War of the Worlds                      17 captain      5      2 Twenty T…\n#> # … with 4,525 more rows\n\nwrong_words %>%\n  count(title, consensus, term, wt = count) %>%\n  ungroup() %>%\n  arrange(desc(n))\n#> # A tibble: 3,500 × 4\n#>    title              consensus             term         n\n#>    <chr>              <chr>                 <chr>    <dbl>\n#>  1 Great Expectations Pride and Prejudice   love        44\n#>  2 Great Expectations Pride and Prejudice   sergeant    37\n#>  3 Great Expectations Pride and Prejudice   lady        32\n#>  4 Great Expectations Pride and Prejudice   miss        26\n#>  5 Great Expectations The War of the Worlds boat        25\n#>  6 Great Expectations Pride and Prejudice   father      19\n#>  7 Great Expectations The War of the Worlds water       19\n#>  8 Great Expectations Pride and Prejudice   baby        18\n#>  9 Great Expectations Pride and Prejudice   flopson     18\n#> 10 Great Expectations Pride and Prejudice   family      16\n#> # … with 3,490 more rows\nword_counts %>%\n  filter(word == \"flopson\")\n#> # A tibble: 3 × 3\n#>   document              word        n\n#>   <chr>                 <chr>   <int>\n#> 1 Great Expectations_22 flopson    10\n#> 2 Great Expectations_23 flopson     7\n#> 3 Great Expectations_33 flopson     1"},{"path":"topicmodeling.html","id":"alternative-lda-implementations","chapter":"6 Topic modeling","heading":"6.3 Alternative LDA implementations","text":"LDA() function topicmodels package one implementation latent Dirichlet allocation algorithm. example, mallet package (Mimno 2013) implements wrapper around MALLET Java package text classification tools, tidytext package provides tidiers model output well.mallet package takes somewhat different approach input format. instance, takes non-tokenized documents performs tokenization , requires separate file stopwords. means collapse text one string document performing LDA.model created, however, can use tidy() augment() functions described rest chapter almost identical way. includes extracting probabilities words within topic topics within document.use ggplot2 explore visualize model way LDA output.","code":"\nlibrary(mallet)\n\n# create a vector with one string per chapter\ncollapsed <- by_chapter_word %>%\n  anti_join(stop_words, by = \"word\") %>%\n  mutate(word = str_replace(word, \"'\", \"\")) %>%\n  group_by(document) %>%\n  summarize(text = paste(word, collapse = \" \"))\n\n# create an empty file of \"stopwords\"\nfile.create(empty_file <- tempfile())\ndocs <- mallet.import(collapsed$document, collapsed$text, empty_file)\n\nmallet_model <- MalletLDA(num.topics = 4)\nmallet_model$loadDocuments(docs)\nmallet_model$train(100)\n# word-topic pairs\ntidy(mallet_model)\n\n# document-topic pairs\ntidy(mallet_model, matrix = \"gamma\")\n\n# column needs to be named \"term\" for \"augment\"\nterm_counts <- rename(word_counts, term = word)\naugment(mallet_model, term_counts)"},{"path":"topicmodeling.html","id":"summary-5","chapter":"6 Topic modeling","heading":"6.4 Summary","text":"chapter introduces topic modeling finding clusters words characterize set documents, shows tidy() verb lets us explore understand models using dplyr ggplot2. one advantages tidy approach model exploration: challenges different output formats handled tidying functions, can explore model results using standard set tools. particular, saw topic modeling able separate distinguish chapters four separate books, explored limitations model finding words chapters assigned incorrectly.","code":""},{"path":"twitter.html","id":"twitter","chapter":"7 Case study: comparing Twitter archives","heading":"7 Case study: comparing Twitter archives","text":"One type text gets plenty attention text shared online via Twitter. fact, several sentiment lexicons used book (commonly used general) designed use validated tweets. authors book Twitter fairly regular users , case study, let’s compare entire Twitter archives Julia David.","code":""},{"path":"twitter.html","id":"getting-the-data-and-distribution-of-tweets","chapter":"7 Case study: comparing Twitter archives","heading":"7.1 Getting the data and distribution of tweets","text":"individual can download Twitter archive following directions available Twitter’s website. downloaded now open . Let’s use lubridate package convert string timestamps date-time objects initially take look tweeting patterns overall (Figure 7.1).\nFigure 7.1: tweets accounts\nDavid Julia tweet rate currently joined Twitter year apart , 5 years David active Twitter Julia . total, Julia 4 times many tweets David.","code":"\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\n\ntweets_julia <- read_csv(\"data/tweets_julia.csv\")\ntweets_dave <- read_csv(\"data/tweets_dave.csv\")\ntweets <- bind_rows(tweets_julia %>% \n                      mutate(person = \"Julia\"),\n                    tweets_dave %>% \n                      mutate(person = \"David\")) %>%\n  mutate(timestamp = ymd_hms(timestamp))\n\nggplot(tweets, aes(x = timestamp, fill = person)) +\n  geom_histogram(position = \"identity\", bins = 20, show.legend = FALSE) +\n  facet_wrap(~person, ncol = 1)"},{"path":"twitter.html","id":"word-frequencies-1","chapter":"7 Case study: comparing Twitter archives","heading":"7.2 Word frequencies","text":"Let’s use unnest_tokens() make tidy data frame words tweets, remove common English stop words. certain conventions people use text Twitter, use specialized tokenizer bit work text , example, narrative text Project Gutenberg.First, remove tweets dataset retweets tweets wrote . Next, mutate() line cleans characters don’t want like ampersands .\ncall unnest_tokens(), unnest using \nspecialized “tweets” tokenizer built \ntokenizers package (Mullen 2016). \ntool useful dealing Twitter text text \nonline forums; retains hashtags mentions usernames \n@ symbol.\nkept text hashtags usernames dataset, can’t use simple anti_join() remove stop words. Instead, can take approach shown filter() line uses str_detect() stringr package.Now can calculate word frequencies person. First, group person count many times person used word. use left_join() add column total number words used person. (higher Julia David since tweets David.) Finally, calculate frequency person word.nice tidy data frame actually like plot frequencies x- y-axes plot, need use pivot_wider() tidyr make differently shaped data frame.Now ready us plot. Let’s use geom_jitter() don’t see discreteness low end frequency much, check_overlap = TRUE text labels don’t print top (print).\nFigure 7.2: Comparing frequency words used Julia David\nWords near line Figure 7.2 used equal frequencies David Julia, words far away line used much one person compared . Words, hashtags, usernames appear plot ones used least tweets.may even need pointed , David Julia used Twitter accounts rather differently course past several years. David used Twitter account almost exclusively professional purposes since became active, Julia used entirely personal purposes late 2015 still uses personally David. see differences immediately plot exploring word frequencies, continue obvious rest chapter.","code":"\nlibrary(tidytext)\nlibrary(stringr)\n\nremove_reg <- \"&amp;|&lt;|&gt;\"\ntidy_tweets <- tweets %>% \n  filter(!str_detect(text, \"^RT\")) %>%\n  mutate(text = str_remove_all(text, remove_reg)) %>%\n  unnest_tokens(word, text, token = \"tweets\") %>%\n  filter(!word %in% stop_words$word,\n         !word %in% str_remove_all(stop_words$word, \"'\"),\n         str_detect(word, \"[a-z]\"))\nfrequency <- tidy_tweets %>% \n  count(person, word, sort = TRUE) %>% \n  left_join(tidy_tweets %>% \n              count(person, name = \"total\")) %>%\n  mutate(freq = n/total)\n\nfrequency\n#> # A tibble: 24,067 × 5\n#>    person word               n total    freq\n#>    <chr>  <chr>          <int> <int>   <dbl>\n#>  1 Julia  @selkie1970      570 74152 0.00769\n#>  2 Julia  time             557 74152 0.00751\n#>  3 Julia  @skedman         531 74152 0.00716\n#>  4 Julia  day              437 74152 0.00589\n#>  5 Julia  baby             392 74152 0.00529\n#>  6 David  @hadleywickham   308 20699 0.0149 \n#>  7 Julia  love             302 74152 0.00407\n#>  8 Julia  @haleynburke     298 74152 0.00402\n#>  9 Julia  house            283 74152 0.00382\n#> 10 Julia  morning          278 74152 0.00375\n#> # … with 24,057 more rows\nlibrary(tidyr)\n\nfrequency <- frequency %>% \n  select(person, word, freq) %>% \n  pivot_wider(names_from = person, values_from = freq) %>%\n  arrange(Julia, David)\n\nfrequency\n#> # A tibble: 21,071 × 3\n#>    word               Julia     David\n#>    <chr>              <dbl>     <dbl>\n#>  1 @accidentalart 0.0000135 0.0000483\n#>  2 @alicedata     0.0000135 0.0000483\n#>  3 @alistaire     0.0000135 0.0000483\n#>  4 @corynissen    0.0000135 0.0000483\n#>  5 @jennybryans   0.0000135 0.0000483\n#>  6 @jsvine        0.0000135 0.0000483\n#>  7 @lewislab      0.0000135 0.0000483\n#>  8 @lizasperling  0.0000135 0.0000483\n#>  9 @ognyanova     0.0000135 0.0000483\n#> 10 @rbloggers     0.0000135 0.0000483\n#> # … with 21,061 more rows\nlibrary(scales)\n\nggplot(frequency, aes(Julia, David)) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\")"},{"path":"twitter.html","id":"comparing-word-usage","chapter":"7 Case study: comparing Twitter archives","heading":"7.3 Comparing word usage","text":"just made plot comparing raw word frequencies whole Twitter histories; now let’s find words less likely come person’s account using log odds ratio. First, let’s restrict analysis moving forward tweets David Julia sent 2016. David consistently active Twitter 2016 Julia transitioned data science career.Next, let’s use str_detect() remove Twitter usernames word column, otherwise, results dominated people Julia David know . removing , count many times person uses word keep words used 10 times. pivot_wider() operation, can calculate log odds ratio word, using\\[\\text{log odds ratio} = \\ln{\\left(\\frac{\\left[\\frac{n+1}{\\text{total}+1}\\right]_\\text{David}}{\\left[\\frac{n+1}{\\text{total}+1}\\right]_\\text{Julia}}\\right)}\\]\\(n\\) number times word question used person total indicates total words person.words equally likely come David Julia’s account 2016?equally likely tweet words, science, ideas, email.words likely Julia’s account David’s account? Let’s just take top 15 distinctive words account plot Figure 7.3.\nFigure 7.3: Comparing odds ratios words accounts\nDavid tweeted specific conferences gone Stack Overflow, Julia tweeted Utah, Census data, family.","code":"\ntidy_tweets <- tidy_tweets %>%\n  filter(timestamp >= as.Date(\"2016-01-01\"),\n         timestamp < as.Date(\"2017-01-01\"))\nword_ratios <- tidy_tweets %>%\n  filter(!str_detect(word, \"^@\")) %>%\n  count(word, person) %>%\n  group_by(word) %>%\n  filter(sum(n) >= 10) %>%\n  ungroup() %>%\n  pivot_wider(names_from = person, values_from = n, values_fill = 0) %>%\n  mutate_if(is.numeric, list(~(. + 1) / (sum(.) + 1))) %>%\n  mutate(logratio = log(David / Julia)) %>%\n  arrange(desc(logratio))\nword_ratios %>% \n  arrange(abs(logratio))\n#> # A tibble: 351 × 4\n#>    word      David   Julia logratio\n#>    <chr>     <dbl>   <dbl>    <dbl>\n#>  1 words   0.00377 0.00378 -0.00334\n#>  2 science 0.00653 0.00648  0.00771\n#>  3 idea    0.00577 0.00594 -0.0279 \n#>  4 email   0.00251 0.00243  0.0330 \n#>  5 file    0.00251 0.00243  0.0330 \n#>  6 purrr   0.00251 0.00243  0.0330 \n#>  7 test    0.00226 0.00216  0.0454 \n#>  8 account 0.00201 0.00189  0.0612 \n#>  9 api     0.00201 0.00189  0.0612 \n#> 10 sad     0.00201 0.00189  0.0612 \n#> # … with 341 more rows\nword_ratios %>%\n  group_by(logratio < 0) %>%\n  slice_max(abs(logratio), n = 15) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, logratio)) %>%\n  ggplot(aes(word, logratio, fill = logratio < 0)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  ylab(\"log odds ratio (David/Julia)\") +\n  scale_fill_discrete(name = \"\", labels = c(\"David\", \"Julia\"))"},{"path":"twitter.html","id":"changes-in-word-use","chapter":"7 Case study: comparing Twitter archives","heading":"7.4 Changes in word use","text":"section looked overall word use, now let’s ask different question. words’ frequencies changed fastest Twitter feeds? state another way, words tweeted higher lower rate time passed? , define new time variable data frame defines unit time tweet posted . can use floor_date() lubridate , unit choosing; using 1 month seems work well year tweets us.time bins defined, count many times us used word time bin. , add columns data frame total number words used time bin person total number times word used person. can filter() keep words used least minimum number times (30, case).row data frame corresponds one person using one word given time bin. count column tells us many times person used word time bin, time_total column tells us many words person used time bin, word_total column tells us many times person used word whole year. data set can use modeling.can use nest() tidyr make data frame list column contains little miniature data frames word. Let’s now take look resulting structure.data frame one row person-word combination; data column list column contains data frames, one combination person word. Let’s use map() purrr (Henry Wickham 2018) apply modeling procedure little data frames inside big data frame. count data let’s use glm() family = \"binomial\" modeling.\ncan think modeling procedure answering question like,\n“given word mentioned given time bin? Yes ? \ncount word mentions depend time?”\nNow notice new column modeling results; another list column contains glm objects. next step use map() tidy() broom package pull slopes models find important ones. comparing many slopes statistically significant, let’s apply adjustment p-values multiple comparisons.Now let’s find important slopes. words changed frequency moderately significant level tweets?visualize results, can plot words’ use David Julia year tweets.\nFigure 7.4: Trending words David’s tweets\nsee Figure 7.4 David tweeted lot UseR conference quickly stopped. tweeted Stack Overflow toward end year less ggplot2 year progressed.\n: sick data science wars. #rstats vs Python, frequentist vs Bayesian…: base vs ggplot2…: SIDE \nNow let’s plot words changed frequency Julia’s tweets Figure 7.5.\nFigure 7.5: Trending words Julia’s tweets\nsignificant slopes Julia negative. means tweeted higher rate using specific words, instead using variety different words; tweets earlier year contained words shown plot higher proportions. Words uses publicizing new blog post like #rstats hashtag “post” gone frequency.","code":"\nwords_by_time <- tidy_tweets %>%\n  filter(!str_detect(word, \"^@\")) %>%\n  mutate(time_floor = floor_date(timestamp, unit = \"1 month\")) %>%\n  count(time_floor, person, word) %>%\n  group_by(person, time_floor) %>%\n  mutate(time_total = sum(n)) %>%\n  group_by(person, word) %>%\n  mutate(word_total = sum(n)) %>%\n  ungroup() %>%\n  rename(count = n) %>%\n  filter(word_total > 30)\n\nwords_by_time\n#> # A tibble: 326 × 6\n#>    time_floor          person word    count time_total word_total\n#>    <dttm>              <chr>  <chr>   <int>      <int>      <int>\n#>  1 2016-01-01 00:00:00 David  #rstats     2        315        205\n#>  2 2016-01-01 00:00:00 David  broom       2        315         34\n#>  3 2016-01-01 00:00:00 David  data        2        315        148\n#>  4 2016-01-01 00:00:00 David  ggplot2     1        315         37\n#>  5 2016-01-01 00:00:00 David  time        2        315         56\n#>  6 2016-01-01 00:00:00 David  tweets      1        315         46\n#>  7 2016-01-01 00:00:00 Julia  #rstats    10        437        116\n#>  8 2016-01-01 00:00:00 Julia  blog        2        437         33\n#>  9 2016-01-01 00:00:00 Julia  data        5        437        105\n#> 10 2016-01-01 00:00:00 Julia  day         1        437         43\n#> # … with 316 more rows\nnested_data <- words_by_time %>%\n  nest(data = c(-word, -person)) \n\nnested_data\n#> # A tibble: 32 × 3\n#>    person word    data             \n#>    <chr>  <chr>   <list>           \n#>  1 David  #rstats <tibble [12 × 4]>\n#>  2 David  broom   <tibble [10 × 4]>\n#>  3 David  data    <tibble [12 × 4]>\n#>  4 David  ggplot2 <tibble [10 × 4]>\n#>  5 David  time    <tibble [12 × 4]>\n#>  6 David  tweets  <tibble [8 × 4]> \n#>  7 Julia  #rstats <tibble [12 × 4]>\n#>  8 Julia  blog    <tibble [10 × 4]>\n#>  9 Julia  data    <tibble [12 × 4]>\n#> 10 Julia  day     <tibble [12 × 4]>\n#> # … with 22 more rows\nlibrary(purrr)\n\nnested_models <- nested_data %>%\n  mutate(models = map(data, ~ glm(cbind(count, time_total) ~ time_floor, ., \n                                  family = \"binomial\")))\n\nnested_models\n#> # A tibble: 32 × 4\n#>    person word    data              models\n#>    <chr>  <chr>   <list>            <list>\n#>  1 David  #rstats <tibble [12 × 4]> <glm> \n#>  2 David  broom   <tibble [10 × 4]> <glm> \n#>  3 David  data    <tibble [12 × 4]> <glm> \n#>  4 David  ggplot2 <tibble [10 × 4]> <glm> \n#>  5 David  time    <tibble [12 × 4]> <glm> \n#>  6 David  tweets  <tibble [8 × 4]>  <glm> \n#>  7 Julia  #rstats <tibble [12 × 4]> <glm> \n#>  8 Julia  blog    <tibble [10 × 4]> <glm> \n#>  9 Julia  data    <tibble [12 × 4]> <glm> \n#> 10 Julia  day     <tibble [12 × 4]> <glm> \n#> # … with 22 more rows\nlibrary(broom)\n\nslopes <- nested_models %>%\n  mutate(models = map(models, tidy)) %>%\n  unnest(cols = c(models)) %>%\n  filter(term == \"time_floor\") %>%\n  mutate(adjusted.p.value = p.adjust(p.value))\ntop_slopes <- slopes %>% \n  filter(adjusted.p.value < 0.05)\n\ntop_slopes\n#> # A tibble: 6 × 9\n#>   person word      data              term   estimate std.error statistic p.value\n#>   <chr>  <chr>     <list>            <chr>     <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 David  ggplot2   <tibble [10 × 4]> time_… -8.07e-8   2.00e-8     -4.04 5.23e-5\n#> 2 Julia  #rstats   <tibble [12 × 4]> time_… -4.50e-8   1.11e-8     -4.04 5.41e-5\n#> 3 Julia  post      <tibble [12 × 4]> time_… -5.14e-8   1.49e-8     -3.46 5.46e-4\n#> 4 David  overflow  <tibble [10 × 4]> time_…  6.96e-8   2.23e-8      3.12 1.81e-3\n#> 5 David  stack     <tibble [10 × 4]> time_…  7.38e-8   2.19e-8      3.37 7.51e-4\n#> 6 David  #user2016 <tibble [3 × 4]>  time_… -8.17e-7   1.55e-7     -5.27 1.39e-7\n#> # … with 1 more variable: adjusted.p.value <dbl>\nwords_by_time %>%\n  inner_join(top_slopes, by = c(\"word\", \"person\")) %>%\n  filter(person == \"David\") %>%\n  ggplot(aes(time_floor, count/time_total, color = word)) +\n  geom_line(size = 1.3) +\n  labs(x = NULL, y = \"Word frequency\")\nwords_by_time %>%\n  inner_join(top_slopes, by = c(\"word\", \"person\")) %>%\n  filter(person == \"Julia\") %>%\n  ggplot(aes(time_floor, count/time_total, color = word)) +\n  geom_line(size = 1.3) +\n  labs(x = NULL, y = \"Word frequency\")"},{"path":"twitter.html","id":"favorites-and-retweets","chapter":"7 Case study: comparing Twitter archives","heading":"7.5 Favorites and retweets","text":"Another important characteristic tweets many times favorited retweeted. Let’s explore words likely retweeted favorited Julia’s David’s tweets. user downloads Twitter archive, favorites retweets included, constructed another dataset authors’ tweets includes information. accessed tweets via Twitter API downloaded 3200 tweets person. cases, last 18 months worth Twitter activity. corresponds period increasing activity increasing numbers followers us.Now second, smaller set recent tweets, let’s use unnest_tokens() transform tweets tidy data set. Let’s remove retweets replies data set look regular tweets David Julia posted directly.Notice word column contains tokenized emoji.start , let’s look number times tweets retweeted. Let’s find total number retweets person.Now let’s find median number retweets word person. probably want count tweet/word combination , use group_by() summarise() twice, one right . first summarise() statement counts many times word retweeted, tweet person. second summarise() statement, can find median retweets person word, also count number times word used ever person keep uses. Next, can join data frame retweet totals. Let’s filter() keep words mentioned least 5 times.top sorted data frame, see tweets Julia David packages work , like gganimate tidytext. Let’s plot words highest median retweets accounts (Figure 7.6).\nFigure 7.6: Words highest median retweets\nsee lots word R packages, including tidytext, package reading right now!can follow similar procedure see words led favorites. different words lead retweets?built data frames need. Now let’s make visualization Figure 7.7.\nFigure 7.7: Words highest median favorites\nsee minor differences Figures 7.6 7.7, especially near bottom top 10 list, largely words retweets. general, words lead retweets lead favorites. prominent word Julia plots hashtag NASA Datanauts program participated ; read Chapter 8 learn NASA data can learn text analysis NASA datasets. Wondering “=” David’s list?\n: just add two p-values together.Dev: hell :newPval = pval1 + pval2;: -Dev: statistics easy\n","code":"\ntweets_julia <- read_csv(\"data/juliasilge_tweets.csv\")\ntweets_dave <- read_csv(\"data/drob_tweets.csv\")\ntweets <- bind_rows(tweets_julia %>% \n                      mutate(person = \"Julia\"),\n                    tweets_dave %>% \n                      mutate(person = \"David\")) %>%\n  mutate(created_at = ymd_hms(created_at))\ntidy_tweets <- tweets %>% \n  filter(!str_detect(text, \"^(RT|@)\")) %>%\n  mutate(text = str_remove_all(text, remove_reg)) %>%\n  unnest_tokens(word, text, token = \"tweets\", strip_url = TRUE) %>%\n  filter(!word %in% stop_words$word,\n         !word %in% str_remove_all(stop_words$word, \"'\"))\n\ntidy_tweets\n#> # A tibble: 11,014 × 7\n#>         id created_at          source            retweets favorites person word \n#>      <dbl> <dttm>              <chr>                <dbl>     <dbl> <chr>  <chr>\n#>  1 8.04e17 2016-12-01 16:44:03 Twitter Web Clie…        0         0 Julia  \"sco…\n#>  2 8.04e17 2016-12-01 16:44:03 Twitter Web Clie…        0         0 Julia  \"50\" \n#>  3 8.04e17 2016-12-01 16:42:03 Twitter Web Clie…        0         9 Julia  \"sno…\n#>  4 8.04e17 2016-12-01 16:42:03 Twitter Web Clie…        0         9 Julia  \"\\U0…\n#>  5 8.04e17 2016-12-01 16:42:03 Twitter Web Clie…        0         9 Julia  \"dri…\n#>  6 8.04e17 2016-12-01 16:42:03 Twitter Web Clie…        0         9 Julia  \"tea\"\n#>  7 8.04e17 2016-12-01 16:42:03 Twitter Web Clie…        0         9 Julia  \"\\U0…\n#>  8 8.04e17 2016-12-01 16:42:03 Twitter Web Clie…        0         9 Julia  \"#rs…\n#>  9 8.04e17 2016-12-01 16:42:03 Twitter Web Clie…        0         9 Julia  \"\\U0…\n#> 10 8.04e17 2016-12-01 02:56:10 Twitter Web Clie…        0        11 Julia  \"jul…\n#> # … with 11,004 more rows\ntotals <- tidy_tweets %>% \n  group_by(person, id) %>% \n  summarise(rts = first(retweets)) %>% \n  group_by(person) %>% \n  summarise(total_rts = sum(rts))\n\ntotals\n#> # A tibble: 2 × 2\n#>   person total_rts\n#>   <chr>      <dbl>\n#> 1 David      13014\n#> 2 Julia       1750\nword_by_rts <- tidy_tweets %>% \n  group_by(id, word, person) %>% \n  summarise(rts = first(retweets)) %>% \n  group_by(person, word) %>% \n  summarise(retweets = median(rts), uses = n()) %>%\n  left_join(totals) %>%\n  filter(retweets != 0) %>%\n  ungroup()\n\nword_by_rts %>% \n  filter(uses >= 5) %>%\n  arrange(desc(retweets))\n#> # A tibble: 170 × 5\n#>    person word          retweets  uses total_rts\n#>    <chr>  <chr>            <dbl> <int>     <dbl>\n#>  1 David  animation           85     5     13014\n#>  2 David  gganimate           75     6     13014\n#>  3 David  error               56     7     13014\n#>  4 David  start               56     6     13014\n#>  5 David  download            52     5     13014\n#>  6 Julia  tidytext            50     7      1750\n#>  7 David  introducing         45     6     13014\n#>  8 David  understanding       37     6     13014\n#>  9 David  ab                  36     5     13014\n#> 10 David  bayesian            34     7     13014\n#> # … with 160 more rows\nword_by_rts %>%\n  filter(uses >= 5) %>%\n  group_by(person) %>%\n  slice_max(retweets, n = 10) %>% \n  arrange(retweets) %>%\n  ungroup() %>%\n  mutate(word = factor(word, unique(word))) %>%\n  ungroup() %>%\n  ggplot(aes(word, retweets, fill = person)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ person, scales = \"free\", ncol = 2) +\n  coord_flip() +\n  labs(x = NULL, \n       y = \"Median # of retweets for tweets containing each word\")\ntotals <- tidy_tweets %>% \n  group_by(person, id) %>% \n  summarise(favs = first(favorites)) %>% \n  group_by(person) %>% \n  summarise(total_favs = sum(favs))\n\nword_by_favs <- tidy_tweets %>% \n  group_by(id, word, person) %>% \n  summarise(favs = first(favorites)) %>% \n  group_by(person, word) %>% \n  summarise(favorites = median(favs), uses = n()) %>%\n  left_join(totals) %>%\n  filter(favorites != 0) %>%\n  ungroup()\nword_by_favs %>%\n  filter(uses >= 5) %>%\n  group_by(person) %>%\n  slice_max(favorites, n = 10) %>% \n  arrange(favorites) %>%\n  ungroup() %>%\n  mutate(word = factor(word, unique(word))) %>%\n  ungroup() %>%\n  ggplot(aes(word, favorites, fill = person)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ person, scales = \"free\", ncol = 2) +\n  coord_flip() +\n  labs(x = NULL, \n       y = \"Median # of favorites for tweets containing each word\")"},{"path":"twitter.html","id":"summary-6","chapter":"7 Case study: comparing Twitter archives","heading":"7.6 Summary","text":"chapter first case study, beginning--end analysis demonstrates bring together concepts code exploring cohesive way understand text data set. Comparing word frequencies allows us see words tweeted less frequently, log odds ratio shows us words likely tweeted accounts. can use nest() map() glm() function find words tweeted higher lower rates time passed. Finally, can find words tweets led higher numbers retweets favorites. examples approaches measure use words similar different ways characteristics tweets changing compare . flexible approaches text mining can applied types text well.","code":""},{"path":"nasa.html","id":"nasa","chapter":"8 Case study: mining NASA metadata","heading":"8 Case study: mining NASA metadata","text":"32,000 datasets hosted /maintained NASA; datasets cover topics Earth science aerospace engineering management NASA . can use metadata datasets understand connections .\nmetadata? Metadata term refers data gives\ninformation data; case, metadata informs users\nnumerous NASA datasets include \ncontent datasets .\nmetadata includes information like title dataset, description field, organization(s) within NASA responsible dataset, keywords dataset assigned human , forth. NASA places high priority making data open accessible, even requiring NASA-funded research openly accessible online. metadata datasets publicly available online JSON format.chapter, treat NASA metadata text dataset show implement several tidy text approaches real-life text. use word co-occurrences correlations, tf-idf, topic modeling explore connections datasets. Can find datasets related ? Can find clusters similar datasets? Since several text fields NASA metadata, importantly title, description, keyword fields, can explore connections fields better understand complex world data NASA. type approach can extended domain deals text, let’s take look metadata get started.","code":""},{"path":"nasa.html","id":"how-data-is-organized-at-nasa","chapter":"8 Case study: mining NASA metadata","heading":"8.1 How data is organized at NASA","text":"First, let’s download JSON file take look names stored metadata.see extract information publishes dataset license released .seems likely title, description, keywords dataset may fruitful drawing connections datasets. Let’s check .title description fields stored character vectors, keywords stored list character vectors.","code":"\nlibrary(jsonlite)\nmetadata <- fromJSON(\"https://data.nasa.gov/data.json\")\nnames(metadata$dataset)#>  [1] \"_id\"                \"@type\"              \"accessLevel\"       \n#>  [4] \"accrualPeriodicity\" \"bureauCode\"         \"contactPoint\"      \n#>  [7] \"description\"        \"distribution\"       \"identifier\"        \n#> [10] \"issued\"             \"keyword\"            \"landingPage\"       \n#> [13] \"language\"           \"modified\"           \"programCode\"       \n#> [16] \"publisher\"          \"spatial\"            \"temporal\"          \n#> [19] \"theme\"              \"title\"              \"license\"           \n#> [22] \"isPartOf\"           \"references\"         \"rights\"            \n#> [25] \"describedBy\"\nclass(metadata$dataset$title)\n#> [1] \"character\"\nclass(metadata$dataset$description)\n#> [1] \"character\"\nclass(metadata$dataset$keyword)\n#> [1] \"list\""},{"path":"nasa.html","id":"wrangling-and-tidying-the-data","chapter":"8 Case study: mining NASA metadata","heading":"8.1.1 Wrangling and tidying the data","text":"Let’s set separate tidy data frames title, description, keyword, keeping dataset ids can connect later analysis necessary.just example titles datasets exploring. Notice NASA-assigned ids , also duplicate titles separate datasets.see first part several selected description fields metadata.Now can build tidy data frame keywords. one, need use unnest() tidyr, list-column.tidy data frame one row keyword; means multiple rows dataset dataset can one keyword.Now time use tidytext’s unnest_tokens() title description fields can text analysis. Let’s also remove stop words titles descriptions. remove stop words keywords, short, human-assigned keywords like “RADIATION” “CLIMATE INDICATORS”.now tidy text format working throughout book, one token (word, case) per row; let’s take look move analysis.","code":"\nlibrary(dplyr)\n\nnasa_title <- tibble(id = metadata$dataset$`_id`$`$oid`, \n                     title = metadata$dataset$title)\nnasa_title\n#> # A tibble: 32,089 × 2\n#>    id                       title                                              \n#>    <chr>                    <chr>                                              \n#>  1 55942a57c63a7fe59b495a77 15 Minute Stream Flow Data: USGS (FIFE)            \n#>  2 55942a57c63a7fe59b495a78 15 Minute Stream Flow Data: USGS (FIFE)            \n#>  3 55942a58c63a7fe59b495a79 15 Minute Stream Flow Data: USGS (FIFE)            \n#>  4 55942a58c63a7fe59b495a7a 2000 Pilot Environmental Sustainability Index (ESI)\n#>  5 55942a58c63a7fe59b495a7b 2000 Pilot Environmental Sustainability Index (ESI)\n#>  6 55942a58c63a7fe59b495a7c 2000 Pilot Environmental Sustainability Index (ESI)\n#>  7 55942a58c63a7fe59b495a7d 2001 Environmental Sustainability Index (ESI)      \n#>  8 55942a58c63a7fe59b495a7e 2001 Environmental Sustainability Index (ESI)      \n#>  9 55942a58c63a7fe59b495a7f 2001 Environmental Sustainability Index (ESI)      \n#> 10 55942a58c63a7fe59b495a80 2001 Environmental Sustainability Index (ESI)      \n#> # … with 32,079 more rows\nnasa_desc <- tibble(id = metadata$dataset$`_id`$`$oid`, \n                    desc = metadata$dataset$description)\n\nnasa_desc %>% \n  select(desc) %>% \n  sample_n(5)\n#> # A tibble: 5 × 1\n#>   desc                                                                          \n#>   <chr>                                                                         \n#> 1 \"Lightning induced damage is one of the major concerns in aircraft health mon…\n#> 2 \"The Natural Resource Management Index (NRMI), 2009 Release is a composite in…\n#> 3 \"The Coastal Zone Color Scanner Experiment (CZCS) was the first instrument de…\n#> 4 \"MODIS (or Moderate Resolution Imaging Spectroradiometer) is a key instrument…\n#> 5 \"This file contains the MISR Level 3 Component Global Aerosol Product coverin…\nlibrary(tidyr)\n\nnasa_keyword <- tibble(id = metadata$dataset$`_id`$`$oid`, \n                       keyword = metadata$dataset$keyword) %>%\n  unnest(keyword)\n\nnasa_keyword\n#> # A tibble: 126,814 × 2\n#>    id                       keyword      \n#>    <chr>                    <chr>        \n#>  1 55942a57c63a7fe59b495a77 EARTH SCIENCE\n#>  2 55942a57c63a7fe59b495a77 HYDROSPHERE  \n#>  3 55942a57c63a7fe59b495a77 SURFACE WATER\n#>  4 55942a57c63a7fe59b495a78 EARTH SCIENCE\n#>  5 55942a57c63a7fe59b495a78 HYDROSPHERE  \n#>  6 55942a57c63a7fe59b495a78 SURFACE WATER\n#>  7 55942a58c63a7fe59b495a79 EARTH SCIENCE\n#>  8 55942a58c63a7fe59b495a79 HYDROSPHERE  \n#>  9 55942a58c63a7fe59b495a79 SURFACE WATER\n#> 10 55942a58c63a7fe59b495a7a EARTH SCIENCE\n#> # … with 126,804 more rows\nlibrary(tidytext)\n\nnasa_title <- nasa_title %>% \n  unnest_tokens(word, title) %>% \n  anti_join(stop_words)\n\nnasa_desc <- nasa_desc %>% \n  unnest_tokens(word, desc) %>% \n  anti_join(stop_words)\nnasa_title\n#> # A tibble: 210,914 × 2\n#>    id                       word  \n#>    <chr>                    <chr> \n#>  1 55942a57c63a7fe59b495a77 15    \n#>  2 55942a57c63a7fe59b495a77 minute\n#>  3 55942a57c63a7fe59b495a77 stream\n#>  4 55942a57c63a7fe59b495a77 flow  \n#>  5 55942a57c63a7fe59b495a77 data  \n#>  6 55942a57c63a7fe59b495a77 usgs  \n#>  7 55942a57c63a7fe59b495a77 fife  \n#>  8 55942a57c63a7fe59b495a78 15    \n#>  9 55942a57c63a7fe59b495a78 minute\n#> 10 55942a57c63a7fe59b495a78 stream\n#> # … with 210,904 more rows\nnasa_desc\n#> # A tibble: 2,677,811 × 2\n#>    id                       word   \n#>    <chr>                    <chr>  \n#>  1 55942a57c63a7fe59b495a77 usgs   \n#>  2 55942a57c63a7fe59b495a77 15     \n#>  3 55942a57c63a7fe59b495a77 minute \n#>  4 55942a57c63a7fe59b495a77 stream \n#>  5 55942a57c63a7fe59b495a77 flow   \n#>  6 55942a57c63a7fe59b495a77 data   \n#>  7 55942a57c63a7fe59b495a77 kings  \n#>  8 55942a57c63a7fe59b495a77 creek  \n#>  9 55942a57c63a7fe59b495a77 konza  \n#> 10 55942a57c63a7fe59b495a77 prairie\n#> # … with 2,677,801 more rows"},{"path":"nasa.html","id":"some-initial-simple-exploration","chapter":"8 Case study: mining NASA metadata","heading":"8.1.2 Some initial simple exploration","text":"common words NASA dataset titles? can use count() dplyr check .descriptions?Words like “data” “global” used often NASA titles descriptions. may want remove digits “words” like “v1” data frames many types analyses; meaningful audiences.\ncan making list custom stop words using\nanti_join() remove data frame, just like\nremoved default stop words tidytext package. \napproach can used many instances great tool bear \nmind.\ncommon keywords?likely want change keywords either lower upper case get rid duplicates like “OCEANS” “Oceans”. Let’s .","code":"\nnasa_title %>%\n  count(word, sort = TRUE)\n#> # A tibble: 11,614 × 2\n#>    word        n\n#>    <chr>   <int>\n#>  1 project  7735\n#>  2 data     3354\n#>  3 1        2841\n#>  4 level    2400\n#>  5 global   1809\n#>  6 v1       1478\n#>  7 daily    1397\n#>  8 3        1364\n#>  9 aura     1363\n#> 10 l2       1311\n#> # … with 11,604 more rows\nnasa_desc %>% \n  count(word, sort = TRUE)\n#> # A tibble: 35,940 × 2\n#>    word           n\n#>    <chr>      <int>\n#>  1 data       68871\n#>  2 modis      24420\n#>  3 global     23028\n#>  4 2          16599\n#>  5 1          15770\n#>  6 system     15480\n#>  7 product    14780\n#>  8 aqua       14738\n#>  9 earth      14373\n#> 10 resolution 13879\n#> # … with 35,930 more rows\nmy_stopwords <- tibble(word = c(as.character(1:10), \n                                \"v1\", \"v03\", \"l2\", \"l3\", \"l4\", \"v5.2.0\", \n                                \"v003\", \"v004\", \"v005\", \"v006\", \"v7\"))\nnasa_title <- nasa_title %>% \n  anti_join(my_stopwords)\nnasa_desc <- nasa_desc %>% \n  anti_join(my_stopwords)\nnasa_keyword %>% \n  group_by(keyword) %>% \n  count(sort = TRUE)\n#> # A tibble: 1,774 × 2\n#> # Groups:   keyword [1,774]\n#>    keyword                     n\n#>    <chr>                   <int>\n#>  1 EARTH SCIENCE           14362\n#>  2 Project                  7452\n#>  3 ATMOSPHERE               7321\n#>  4 Ocean Color              7268\n#>  5 Ocean Optics             7268\n#>  6 Oceans                   7268\n#>  7 completed                6452\n#>  8 ATMOSPHERIC WATER VAPOR  3142\n#>  9 OCEANS                   2765\n#> 10 LAND SURFACE             2720\n#> # … with 1,764 more rows\nnasa_keyword <- nasa_keyword %>% \n  mutate(keyword = toupper(keyword))"},{"path":"nasa.html","id":"word-co-ocurrences-and-correlations","chapter":"8 Case study: mining NASA metadata","heading":"8.2 Word co-ocurrences and correlations","text":"next step, let’s examine words commonly occur together titles, descriptions, keywords NASA datasets, described Chapter 4. can examine word networks fields; may help us see, example, datasets related .","code":""},{"path":"nasa.html","id":"networks-of-description-and-title-words","chapter":"8 Case study: mining NASA metadata","heading":"8.2.1 Networks of Description and Title Words","text":"can use pairwise_count() widyr package count many times pair words occurs together title description field.pairs words occur together often title fields. words obviously acronyms used within NASA, see often words like “project” “system” used.pairs words occur together often description fields. “Data” common word description fields; shortage data datasets NASA!Let’s plot networks co-occurring words can see relationships better Figure 8.1. use ggraph package visualizing networks.\nFigure 8.1: Word network NASA dataset titles\nsee clear clustering network title words; words NASA dataset titles largely organized several families words tend go together.words description fields?\nFigure 8.2: Word network NASA dataset descriptions\nFigure 8.2 shows strong connections top dozen words (words like “data”, “global”, “resolution”, “instrument”) see clear clustering structure network. may want use tf-idf (described detail Chapter 3) metric find characteristic words description field, instead looking counts words.","code":"\nlibrary(widyr)\n\ntitle_word_pairs <- nasa_title %>% \n  pairwise_count(word, id, sort = TRUE, upper = FALSE)\n\ntitle_word_pairs\n#> # A tibble: 156,689 × 3\n#>    item1  item2       n\n#>    <chr>  <chr>   <dbl>\n#>  1 system project   796\n#>  2 lba    eco       683\n#>  3 airs   aqua      641\n#>  4 level  aqua      623\n#>  5 level  airs      612\n#>  6 aura   omi       607\n#>  7 global grid      597\n#>  8 global daily     574\n#>  9 data   boreas    551\n#> 10 ground gpm       550\n#> # … with 156,679 more rows\ndesc_word_pairs <- nasa_desc %>% \n  pairwise_count(word, id, sort = TRUE, upper = FALSE)\n\ndesc_word_pairs\n#> # A tibble: 10,889,084 × 3\n#>    item1      item2          n\n#>    <chr>      <chr>      <dbl>\n#>  1 data       global      9864\n#>  2 data       resolution  9302\n#>  3 instrument resolution  8189\n#>  4 data       surface     8180\n#>  5 global     resolution  8139\n#>  6 data       instrument  7994\n#>  7 data       system      7870\n#>  8 resolution bands       7584\n#>  9 data       earth       7576\n#> 10 orbit      resolution  7462\n#> # … with 10,889,074 more rows\nlibrary(ggplot2)\nlibrary(igraph)\nlibrary(ggraph)\n\nset.seed(1234)\ntitle_word_pairs %>%\n  filter(n >= 250) %>%\n  graph_from_data_frame() %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = \"cyan4\") +\n  geom_node_point(size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE, \n                 point.padding = unit(0.2, \"lines\")) +\n  theme_void()\nset.seed(1234)\ndesc_word_pairs %>%\n  filter(n >= 5000) %>%\n  graph_from_data_frame() %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = \"darkred\") +\n  geom_node_point(size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE,\n                 point.padding = unit(0.2, \"lines\")) +\n  theme_void()"},{"path":"nasa.html","id":"networks-of-keywords","chapter":"8 Case study: mining NASA metadata","heading":"8.2.2 Networks of Keywords","text":"Next, let’s make network keywords Figure 8.3 see keywords commonly occur together datasets.\nFigure 8.3: Co-occurrence network NASA dataset keywords\ndefinitely see clustering , strong connections keywords like “OCEANS”, “OCEAN OPTICS”, “OCEAN COLOR”, “PROJECT” “COMPLETED”.\ncommonly co-occurring words, also just \ncommon keywords general.\nexamine relationships among keywords different way, can find correlation among keywords described Chapter 4. looks keywords likely occur together keywords dataset.Notice keywords top sorted data frame correlation coefficients equal 1; always occur together. means redundant keywords. may make sense continue use keywords sets pairs; instead, just one keyword used.Let’s visualize network keyword correlations, just keyword co-occurences.\nFigure 8.4: Correlation network NASA dataset keywords\nnetwork Figure 8.4 appears much different co-occurence network. difference co-occurrence network asks question keyword pairs occur often, correlation network asks question keywords occur often together keywords. Notice high number small clusters keywords; network structure can extracted (analysis) graph_from_data_frame() function .","code":"\nkeyword_pairs <- nasa_keyword %>% \n  pairwise_count(keyword, id, sort = TRUE, upper = FALSE)\n\nkeyword_pairs\n#> # A tibble: 13,390 × 3\n#>    item1         item2                       n\n#>    <chr>         <chr>                   <dbl>\n#>  1 OCEANS        OCEAN OPTICS             7324\n#>  2 EARTH SCIENCE ATMOSPHERE               7318\n#>  3 OCEANS        OCEAN COLOR              7270\n#>  4 OCEAN OPTICS  OCEAN COLOR              7270\n#>  5 PROJECT       COMPLETED                6450\n#>  6 EARTH SCIENCE ATMOSPHERIC WATER VAPOR  3142\n#>  7 ATMOSPHERE    ATMOSPHERIC WATER VAPOR  3142\n#>  8 EARTH SCIENCE OCEANS                   2762\n#>  9 EARTH SCIENCE LAND SURFACE             2718\n#> 10 EARTH SCIENCE BIOSPHERE                2448\n#> # … with 13,380 more rows\n\nset.seed(1234)\nkeyword_pairs %>%\n  filter(n >= 700) %>%\n  graph_from_data_frame() %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = \"royalblue\") +\n  geom_node_point(size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE,\n                 point.padding = unit(0.2, \"lines\")) +\n  theme_void()\nkeyword_cors <- nasa_keyword %>% \n  group_by(keyword) %>%\n  filter(n() >= 50) %>%\n  pairwise_cor(keyword, id, sort = TRUE, upper = FALSE)\n\nkeyword_cors\n#> # A tibble: 7,875 × 3\n#>    item1               item2       correlation\n#>    <chr>               <chr>             <dbl>\n#>  1 KNOWLEDGE           SHARING           1    \n#>  2 DASHLINK            AMES              1    \n#>  3 SCHEDULE            EXPEDITION        1    \n#>  4 TURBULENCE          MODELS            0.997\n#>  5 APPEL               KNOWLEDGE         0.997\n#>  6 APPEL               SHARING           0.997\n#>  7 OCEAN OPTICS        OCEAN COLOR       0.995\n#>  8 ATMOSPHERIC SCIENCE CLOUD             0.994\n#>  9 LAUNCH              SCHEDULE          0.984\n#> 10 LAUNCH              EXPEDITION        0.984\n#> # … with 7,865 more rows\nset.seed(1234)\nkeyword_cors %>%\n  filter(correlation > .6) %>%\n  graph_from_data_frame() %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = \"royalblue\") +\n  geom_node_point(size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE,\n                 point.padding = unit(0.2, \"lines\")) +\n  theme_void()"},{"path":"nasa.html","id":"calculating-tf-idf-for-the-description-fields","chapter":"8 Case study: mining NASA metadata","heading":"8.3 Calculating tf-idf for the description fields","text":"network graph Figure 8.2 showed us description fields dominated common words like “data”, “global”, “resolution”; excellent opportunity use tf-idf statistic find characteristic words individual description fields. discussed Chapter 3, can use tf-idf, term frequency times inverse document frequency, identify words especially important document within collection documents. Let’s apply approach description fields NASA datasets.","code":""},{"path":"nasa.html","id":"what-is-tf-idf-for-the-description-field-words","chapter":"8 Case study: mining NASA metadata","heading":"8.3.1 What is tf-idf for the description field words?","text":"consider description field document, whole set description fields collection corpus documents. already used unnest_tokens() earlier chapter make tidy data frame words description fields, now can use bind_tf_idf() calculate tf-idf word.highest tf-idf words NASA description fields?important words description fields measured tf-idf, meaning common common.\nNotice run issue ; \\(n\\) term frequency equal 1 \nterms, meaning description fields \nsingle word . description field contains one word, \ntf-idf algorithm think important word.\nDepending analytic goals, might good idea throw description fields words.","code":"\ndesc_tf_idf <- nasa_desc %>% \n  count(id, word, sort = TRUE) %>%\n  bind_tf_idf(word, id, n)\ndesc_tf_idf %>% \n  arrange(-tf_idf)\n#> # A tibble: 1,913,224 × 6\n#>    id                       word                            n    tf   idf tf_idf\n#>    <chr>                    <chr>                       <int> <dbl> <dbl>  <dbl>\n#>  1 55942a7cc63a7fe59b49774a rdr                             1     1 10.4   10.4 \n#>  2 55942ac9c63a7fe59b49b688 palsar_radiometric_terrain…     1     1 10.4   10.4 \n#>  3 55942ac9c63a7fe59b49b689 palsar_radiometric_terrain…     1     1 10.4   10.4 \n#>  4 55942a7bc63a7fe59b4976ca lgrs                            1     1  8.77   8.77\n#>  5 55942a7bc63a7fe59b4976d2 lgrs                            1     1  8.77   8.77\n#>  6 55942a7bc63a7fe59b4976e3 lgrs                            1     1  8.77   8.77\n#>  7 55942a7dc63a7fe59b497820 mri                             1     1  8.58   8.58\n#>  8 55942ad8c63a7fe59b49cf6c template_proddescription        1     1  8.30   8.30\n#>  9 55942ad8c63a7fe59b49cf6d template_proddescription        1     1  8.30   8.30\n#> 10 55942ad8c63a7fe59b49cf6e template_proddescription        1     1  8.30   8.30\n#> # … with 1,913,214 more rows"},{"path":"nasa.html","id":"connecting-description-fields-to-keywords","chapter":"8 Case study: mining NASA metadata","heading":"8.3.2 Connecting description fields to keywords","text":"now know words descriptions high tf-idf, also labels descriptions keywords. Let’s full join keyword data frame data frame description words tf-idf, find highest tf-idf words given keyword.Let’s plot important words, measured tf-idf, example keywords used NASA datasets. First, let’s use dplyr operations filter keywords want examine take just top 15 words keyword. , let’s plot words Figure 8.5.\nFigure 8.5: Distribution tf-idf words datasets labeled selected keywords\nUsing tf-idf allowed us identify important description words keywords. Datasets labeled keyword “SEISMOLOGY” words like “earthquake”, “risk”, “hazard” description, labeled “HUMAN HEALTH” descriptions characterized words like “wellbeing”, “vulnerability”, “children.” combinations letters English words certainly acronyms (like OMB Office Management Budget), examples years numbers important topics. tf-idf statistic identified kinds words intended , important words individual documents within collection documents.","code":"\ndesc_tf_idf <- full_join(desc_tf_idf, nasa_keyword, by = \"id\")\ndesc_tf_idf %>% \n  filter(!near(tf, 1)) %>%\n  filter(keyword %in% c(\"SOLAR ACTIVITY\", \"CLOUDS\", \n                        \"SEISMOLOGY\", \"ASTROPHYSICS\",\n                        \"HUMAN HEALTH\", \"BUDGET\")) %>%\n  arrange(desc(tf_idf)) %>%\n  group_by(keyword) %>%\n  distinct(word, keyword, .keep_all = TRUE) %>%\n  slice_max(tf_idf, n = 15, with_ties = FALSE) %>% \n  ungroup() %>%\n  mutate(word = factor(word, levels = rev(unique(word)))) %>%\n  ggplot(aes(tf_idf, word, fill = keyword)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~keyword, ncol = 3, scales = \"free\") +\n  labs(title = \"Highest tf-idf words in NASA metadata description fields\",\n       caption = \"NASA metadata from https://data.nasa.gov/data.json\",\n       x = \"tf-idf\", y = NULL)"},{"path":"nasa.html","id":"topic-modeling","chapter":"8 Case study: mining NASA metadata","heading":"8.4 Topic modeling","text":"Using tf-idf statistic already given us insight content NASA description fields, let’s try additional approach question NASA descriptions fields . can use topic modeling described Chapter 6 model document (description field) mixture topics topic mixture words. earlier chapters, use latent Dirichlet allocation (LDA) topic modeling; possible approaches topic modeling.","code":""},{"path":"nasa.html","id":"casting-to-a-document-term-matrix","chapter":"8 Case study: mining NASA metadata","heading":"8.4.1 Casting to a document-term matrix","text":"topic modeling implemented , need make DocumentTermMatrix, special kind matrix tm package (course, just specific implementation general concept “document-term matrix”). Rows correspond documents (description texts case) columns correspond terms (.e., words); sparse matrix values word counts.Let’s clean text bit using stop words remove nonsense “words” leftover HTML character encoding. can use bind_rows() add custom stop words list default stop words tidytext package, use anti_join() remove data frame.information need, number times word used document, make DocumentTermMatrix. can cast() tidy text format non-tidy format described detail Chapter 5.see dataset contains documents (NASA description field) terms (words). Notice example document-term matrix (close ) 100% sparse, meaning almost entries matrix zero. non-zero entry corresponds certain word appearing certain document.","code":"\nmy_stop_words <- bind_rows(stop_words, \n                           tibble(word = c(\"nbsp\", \"amp\", \"gt\", \"lt\",\n                                           \"timesnewromanpsmt\", \"font\",\n                                           \"td\", \"li\", \"br\", \"tr\", \"quot\",\n                                           \"st\", \"img\", \"src\", \"strong\",\n                                           \"http\", \"file\", \"files\",\n                                           as.character(1:12)), \n                                  lexicon = rep(\"custom\", 30)))\n\nword_counts <- nasa_desc %>%\n  anti_join(my_stop_words) %>%\n  count(id, word, sort = TRUE) %>%\n  ungroup()\n\nword_counts\n#> # A tibble: 1,895,310 × 3\n#>    id                       word         n\n#>    <chr>                    <chr>    <int>\n#>  1 55942a8ec63a7fe59b4986ef suit        82\n#>  2 55942a8ec63a7fe59b4986ef space       69\n#>  3 56cf5b00a759fdadc44e564a data        41\n#>  4 56cf5b00a759fdadc44e564a leak        40\n#>  5 56cf5b00a759fdadc44e564a tree        39\n#>  6 55942a8ec63a7fe59b4986ef pressure    34\n#>  7 55942a8ec63a7fe59b4986ef system      34\n#>  8 55942a89c63a7fe59b4982d9 em          32\n#>  9 55942a8ec63a7fe59b4986ef al          32\n#> 10 55942a8ec63a7fe59b4986ef human       31\n#> # … with 1,895,300 more rows\ndesc_dtm <- word_counts %>%\n  cast_dtm(id, word, n)\n\ndesc_dtm\n#> <<DocumentTermMatrix (documents: 32003, terms: 35901)>>\n#> Non-/sparse entries: 1895310/1147044393\n#> Sparsity           : 100%\n#> Maximal term length: 166\n#> Weighting          : term frequency (tf)"},{"path":"nasa.html","id":"ready-for-topic-modeling","chapter":"8 Case study: mining NASA metadata","heading":"8.4.2 Ready for topic modeling","text":"Now let’s use topicmodels package create LDA model. many topics tell algorithm make? question much like \\(k\\)-means clustering; don’t really know ahead time. tried following modeling procedure using 8, 16, 24, 32, 64 topics; found 24 topics, documents still getting sorted topics cleanly going much beyond caused distributions \\(\\gamma\\), probability document belongs topic, look worrisome. show details later.stochastic algorithm different results depending algorithm starts, need specify seed reproducibility shown .","code":"\nlibrary(topicmodels)\n\n# be aware that running this model is time intensive\ndesc_lda <- LDA(desc_dtm, k = 24, control = list(seed = 1234))\ndesc_lda#> A LDA_VEM topic model with 24 topics."},{"path":"nasa.html","id":"interpreting-the-topic-model","chapter":"8 Case study: mining NASA metadata","heading":"8.4.3 Interpreting the topic model","text":"Now built model, let’s tidy() results model, .e., construct tidy data frame summarizes results model. tidytext package includes tidying method LDA models topicmodels package.column \\(\\beta\\) tells us probability term generated topic document. probability term (word) belonging topic. Notice values \\(\\beta\\) , low, low.topic ? Let’s examine top 10 terms topic.easy interpret topics data frame like let’s look information visually Figure 8.6.\nFigure 8.6: Top terms topic modeling NASA metadata description field texts\ncan see dominant word “data” description texts. addition, meaningful differences collections terms, terms soil, forests, biomass topic 12 terms design, systems, technology topic 21. topic modeling process identified groupings terms can understand human readers description fields.just explored words associated topics. Next, let’s examine topics associated description fields (.e., documents). look different probability , \\(\\gamma\\), probability document belongs topic, using tidy verb.Notice probabilities visible top data frame low higher. model assigned probability description belonging topics constructed sets words. probabilities distributed? Let’s visualize (Figure 8.7).\nFigure 8.7: Probability distribution topic modeling NASA metadata description field texts\nFirst notice y-axis plotted log scale; otherwise difficult make detail plot. Next, notice \\(\\gamma\\) runs 0 1; remember probability given document belongs given topic. many values near zero, means many documents belong topic. Also, many values near \\(\\gamma = 1\\); documents belong topics. distribution shows documents well discriminated belonging topic . can also look probabilities distributed within topic, shown Figure 8.8.\nFigure 8.8: Probability distribution topic topic modeling NASA metadata description field texts\nLet’s look specifically topic 18 Figure 8.8, topic documents cleanly sorted . many documents \\(\\gamma\\) close 1; documents belong topic 18 according model. also many documents \\(\\gamma\\) close 0; documents belong topic 18. document appears panel plot, \\(\\gamma\\) topic tells us document’s probability belonging topic.plot displays type information used choose many topics topic modeling procedure. tried options higher 24 (32 64), distributions \\(\\gamma\\) started look flat toward \\(\\gamma = 1\\); documents getting sorted topics well.","code":"\ntidy_lda <- tidy(desc_lda)\n\ntidy_lda\n#> # A tibble: 861,624 × 3\n#>    topic term       beta\n#>    <int> <chr>     <dbl>\n#>  1     1 suit  1.00e-121\n#>  2     2 suit  2.63e-145\n#>  3     3 suit  1.92e- 79\n#>  4     4 suit  6.72e- 45\n#>  5     5 suit  1.74e- 85\n#>  6     6 suit  7.69e- 84\n#>  7     7 suit  3.28e-  4\n#>  8     8 suit  3.74e- 20\n#>  9     9 suit  4.85e- 15\n#> 10    10 suit  4.77e- 10\n#> # … with 861,614 more rows\ntop_terms <- tidy_lda %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10, with_ties = FALSE) %>%\n  ungroup() %>%\n  arrange(topic, -beta)\n\ntop_terms\n#> # A tibble: 240 × 3\n#>    topic term          beta\n#>    <int> <chr>        <dbl>\n#>  1     1 data        0.0449\n#>  2     1 soil        0.0368\n#>  3     1 moisture    0.0295\n#>  4     1 amsr        0.0244\n#>  5     1 sst         0.0168\n#>  6     1 validation  0.0132\n#>  7     1 temperature 0.0132\n#>  8     1 surface     0.0129\n#>  9     1 accuracy    0.0123\n#> 10     1 set         0.0116\n#> # … with 230 more rows\ntop_terms %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  group_by(topic, term) %>%    \n  arrange(desc(beta)) %>%  \n  ungroup() %>%\n  ggplot(aes(beta, term, fill = as.factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  scale_y_reordered() +\n  labs(title = \"Top 10 terms in each LDA topic\",\n       x = expression(beta), y = NULL) +\n  facet_wrap(~ topic, ncol = 4, scales = \"free\")\nlda_gamma <- tidy(desc_lda, matrix = \"gamma\")\n\nlda_gamma\n#> # A tibble: 768,072 × 3\n#>    document                 topic      gamma\n#>    <chr>                    <int>      <dbl>\n#>  1 55942a8ec63a7fe59b4986ef     1 0.00000645\n#>  2 56cf5b00a759fdadc44e564a     1 0.0000116 \n#>  3 55942a89c63a7fe59b4982d9     1 0.0492    \n#>  4 56cf5b00a759fdadc44e55cd     1 0.0000225 \n#>  5 55942a89c63a7fe59b4982c6     1 0.0000661 \n#>  6 55942a86c63a7fe59b498077     1 0.0000567 \n#>  7 56cf5b00a759fdadc44e56f8     1 0.0000475 \n#>  8 55942a8bc63a7fe59b4984b5     1 0.0000431 \n#>  9 55942a6ec63a7fe59b496bf7     1 0.0000441 \n#> 10 55942a8ec63a7fe59b4986f6     1 0.0000288 \n#> # … with 768,062 more rows\nggplot(lda_gamma, aes(gamma)) +\n  geom_histogram(alpha = 0.8) +\n  scale_y_log10() +\n  labs(title = \"Distribution of probabilities for all topics\",\n       y = \"Number of documents\", x = expression(gamma))\nggplot(lda_gamma, aes(gamma, fill = as.factor(topic))) +\n  geom_histogram(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~ topic, ncol = 4) +\n  scale_y_log10() +\n  labs(title = \"Distribution of probability for each topic\",\n       y = \"Number of documents\", x = expression(gamma))"},{"path":"nasa.html","id":"connecting-topic-modeling-with-keywords","chapter":"8 Case study: mining NASA metadata","heading":"8.4.4 Connecting topic modeling with keywords","text":"Let’s connect topic models keywords see relationships can find. can full_join() human-tagged keywords discover keywords associated topic.Now can use filter() keep document-topic entries probabilities (\\(\\gamma\\)) greater cut-value; let’s use 0.9.top keywords topic?\nFigure 8.9: Top keywords topic modeling NASA metadata description field texts\nLet’s take step back remind Figure 8.9 telling us. NASA datasets tagged keywords human beings, built LDA topic model (24 topics) description fields NASA datasets. plot answers question, “datasets description fields high probability belonging given topic, common human-assigned keywords?”’s interesting keywords topics 13, 16, 18 essentially duplicates (“OCEAN COLOR”, “OCEAN OPTICS”, “OCEANS”), top words topics exhibit meaningful differences, shown Figure 8.6. Also note number documents, combination 13, 16, 18 quite large percentage total number datasets represented plot, even include topic 11. number, many datasets NASA deal oceans, ocean color, ocean optics. see “PROJECT COMPLETED” topics 9, 10, 21, along names NASA laboratories research centers. important subject areas stand groups keywords atmospheric science, budget/finance, population/human dimensions. can go back Figure 8.6 terms topics see words description fields driving datasets assigned topics. example, topic 4 associated keywords population human dimensions, top terms topic “population”, “international”, “center”, “university”.","code":"\nlda_gamma <- full_join(lda_gamma, nasa_keyword, by = c(\"document\" = \"id\"))\n\nlda_gamma\n#> # A tibble: 3,037,671 × 4\n#>    document                 topic      gamma keyword                    \n#>    <chr>                    <int>      <dbl> <chr>                      \n#>  1 55942a8ec63a7fe59b4986ef     1 0.00000645 JOHNSON SPACE CENTER       \n#>  2 55942a8ec63a7fe59b4986ef     1 0.00000645 PROJECT                    \n#>  3 55942a8ec63a7fe59b4986ef     1 0.00000645 COMPLETED                  \n#>  4 56cf5b00a759fdadc44e564a     1 0.0000116  DASHLINK                   \n#>  5 56cf5b00a759fdadc44e564a     1 0.0000116  AMES                       \n#>  6 56cf5b00a759fdadc44e564a     1 0.0000116  NASA                       \n#>  7 55942a89c63a7fe59b4982d9     1 0.0492     GODDARD SPACE FLIGHT CENTER\n#>  8 55942a89c63a7fe59b4982d9     1 0.0492     PROJECT                    \n#>  9 55942a89c63a7fe59b4982d9     1 0.0492     COMPLETED                  \n#> 10 56cf5b00a759fdadc44e55cd     1 0.0000225  DASHLINK                   \n#> # … with 3,037,661 more rows\ntop_keywords <- lda_gamma %>% \n  filter(gamma > 0.9) %>% \n  count(topic, keyword, sort = TRUE)\n\ntop_keywords\n#> # A tibble: 1,022 × 3\n#>    topic keyword           n\n#>    <int> <chr>         <int>\n#>  1    13 OCEAN COLOR    4480\n#>  2    13 OCEAN OPTICS   4480\n#>  3    13 OCEANS         4480\n#>  4    11 OCEAN COLOR    1216\n#>  5    11 OCEAN OPTICS   1216\n#>  6    11 OCEANS         1216\n#>  7     9 PROJECT         926\n#>  8    12 EARTH SCIENCE   909\n#>  9     9 COMPLETED       834\n#> 10    16 OCEAN COLOR     768\n#> # … with 1,012 more rows\ntop_keywords %>%\n  group_by(topic) %>%\n  slice_max(n, n = 5, with_ties = FALSE) %>%\n  ungroup %>%\n  mutate(keyword = reorder_within(keyword, n, topic)) %>%\n  ggplot(aes(n, keyword, fill = as.factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Top keywords for each LDA topic\",\n       x = \"Number of documents\", y = NULL) +\n  scale_y_reordered() +\n  facet_wrap(~ topic, ncol = 4, scales = \"free\")"},{"path":"nasa.html","id":"summary-7","chapter":"8 Case study: mining NASA metadata","heading":"8.5 Summary","text":"using combination network analysis, tf-idf, topic modeling, come greater understanding datasets related NASA. Specifically, information now keywords connected datasets likely related. topic model used suggest keywords based words description field, work keywords suggest important combination keywords certain areas study.","code":""},{"path":"usenet.html","id":"usenet","chapter":"9 Case study: analyzing usenet text","heading":"9 Case study: analyzing usenet text","text":"final chapter, ’ll use ’ve learned book perform start--finish analysis set 20,000 messages sent 20 Usenet bulletin boards 1993. Usenet bulletin boards dataset include newsgroups topics like politics, religion, cars, sports, cryptography, offer rich set text written many users. data set publicly available http://qwone.com/~jason/20Newsgroups/ (20news-bydate.tar.gz file) become popular exercises text analysis machine learning.","code":""},{"path":"usenet.html","id":"pre-processing","chapter":"9 Case study: analyzing usenet text","heading":"9.1 Pre-processing","text":"’ll start reading messages 20news-bydate folder, organized sub-folders one file message. can read files like combination read_lines(), map() unnest().\nNote step may take several minutes read \ndocuments.\nNotice newsgroup column, describes 20 newsgroups message comes , id column, identifies unique message within newsgroup. newsgroups included, many messages posted (Figure 9.1)?\nFigure 9.1: Number messages newsgroup\ncan see Usenet newsgroup names named hierarchically, starting main topic “talk”, “sci”, “rec”, followed specifications.","code":"\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(readr)\ntraining_folder <- \"data/20news-bydate/20news-bydate-train/\"\n\n# Define a function to read all files from a folder into a data frame\nread_folder <- function(infolder) {\n  tibble(file = dir(infolder, full.names = TRUE)) %>%\n    mutate(text = map(file, read_lines)) %>%\n    transmute(id = basename(file), text) %>%\n    unnest(text)\n}\n\n# Use unnest() and map() to apply read_folder to each subfolder\nraw_text <- tibble(folder = dir(training_folder, full.names = TRUE)) %>%\n  mutate(folder_out = map(folder, read_folder)) %>%\n  unnest(cols = c(folder_out)) %>%\n  transmute(newsgroup = basename(folder), id, text)\nraw_text\n#> # A tibble: 511,655 × 3\n#>    newsgroup   id    text                                                       \n#>    <chr>       <chr> <chr>                                                      \n#>  1 alt.atheism 49960 \"From: mathew <mathew@mantis.co.uk>\"                       \n#>  2 alt.atheism 49960 \"Subject: Alt.Atheism FAQ: Atheist Resources\"              \n#>  3 alt.atheism 49960 \"Summary: Books, addresses, music -- anything related to a…\n#>  4 alt.atheism 49960 \"Keywords: FAQ, atheism, books, music, fiction, addresses,…\n#>  5 alt.atheism 49960 \"Expires: Thu, 29 Apr 1993 11:57:19 GMT\"                   \n#>  6 alt.atheism 49960 \"Distribution: world\"                                      \n#>  7 alt.atheism 49960 \"Organization: Mantis Consultants, Cambridge. UK.\"         \n#>  8 alt.atheism 49960 \"Supersedes: <19930301143317@mantis.co.uk>\"                \n#>  9 alt.atheism 49960 \"Lines: 290\"                                               \n#> 10 alt.atheism 49960 \"\"                                                         \n#> # … with 511,645 more rows\nlibrary(ggplot2)\n\nraw_text %>%\n  group_by(newsgroup) %>%\n  summarize(messages = n_distinct(id)) %>%\n  ggplot(aes(messages, newsgroup)) +\n  geom_col() +\n  labs(y = NULL)"},{"path":"usenet.html","id":"pre-processing-text","chapter":"9 Case study: analyzing usenet text","heading":"9.1.1 Pre-processing text","text":"datasets ’ve examined book pre-processed, meaning didn’t remove, example, copyright notices Jane Austen novels. , however, message structure extra text don’t want include analysis. example, every message header, containing field “:” “in_reply_to:” describe message. also automated email signatures, occur line like --.kind pre-processing can done within dplyr package, using combination cumsum() (cumulative sum) str_detect() stringr.Many lines also nested text representing quotes users, typically starting line like “--writes…” can removed regular expressions.\nalso choose manually remove two messages, 9704 \n9985 contained large amount non-text content.\npoint, ’re ready use unnest_tokens() split dataset tokens, removing stop-words.Every raw text dataset require different steps data cleaning, often involve trial--error exploration unusual cases dataset. ’s important notice cleaning can achieved using tidy tools dplyr tidyr.","code":"\nlibrary(stringr)\n\n# must occur after the first occurrence of an empty line,\n# and before the first occurrence of a line starting with --\ncleaned_text <- raw_text %>%\n  group_by(newsgroup, id) %>%\n  filter(cumsum(text == \"\") > 0,\n         cumsum(str_detect(text, \"^--\")) == 0) %>%\n  ungroup()\ncleaned_text <- cleaned_text %>%\n  filter(str_detect(text, \"^[^>]+[A-Za-z\\\\d]\") | text == \"\",\n         !str_detect(text, \"writes(:|\\\\.\\\\.\\\\.)$\"),\n         !str_detect(text, \"^In article <\"),\n         !id %in% c(9704, 9985))\nlibrary(tidytext)\n\nusenet_words <- cleaned_text %>%\n  unnest_tokens(word, text) %>%\n  filter(str_detect(word, \"[a-z']$\"),\n         !word %in% stop_words$word)"},{"path":"usenet.html","id":"words-in-newsgroups","chapter":"9 Case study: analyzing usenet text","heading":"9.2 Words in newsgroups","text":"Now ’ve removed headers, signatures, formatting, can start exploring common words. starters, find common words entire dataset, within particular newsgroups.","code":"\nusenet_words %>%\n  count(word, sort = TRUE)\n#> # A tibble: 68,137 × 2\n#>    word            n\n#>    <chr>       <int>\n#>  1 people       3655\n#>  2 time         2705\n#>  3 god          1626\n#>  4 system       1595\n#>  5 program      1103\n#>  6 bit          1097\n#>  7 information  1094\n#>  8 windows      1088\n#>  9 government   1084\n#> 10 space        1072\n#> # … with 68,127 more rows\n\nwords_by_newsgroup <- usenet_words %>%\n  count(newsgroup, word, sort = TRUE) %>%\n  ungroup()\n\nwords_by_newsgroup\n#> # A tibble: 173,913 × 3\n#>    newsgroup               word          n\n#>    <chr>                   <chr>     <int>\n#>  1 soc.religion.christian  god         917\n#>  2 sci.space               space       840\n#>  3 talk.politics.mideast   people      728\n#>  4 sci.crypt               key         704\n#>  5 comp.os.ms-windows.misc windows     625\n#>  6 talk.politics.mideast   armenian    582\n#>  7 sci.crypt               db          549\n#>  8 talk.politics.mideast   turkish     514\n#>  9 rec.autos               car         509\n#> 10 talk.politics.mideast   armenians   509\n#> # … with 173,903 more rows"},{"path":"usenet.html","id":"finding-tf-idf-within-newsgroups","chapter":"9 Case study: analyzing usenet text","heading":"9.2.1 Finding tf-idf within newsgroups","text":"’d expect newsgroups differ terms topic content, therefore frequency words differ . Let’s try quantifying using tf-idf metric (Chapter 3).can examine top tf-idf selected groups extract words specific topics. example, look sci. boards, visualized Figure 9.2.\nFigure 9.2: Terms highest tf-idf within science-related newsgroups\nsee lots characteristic words specific particular newsgroup, “wiring” “circuit” sci.electronics topic “orbit” “lunar” space newsgroup. use code explore newsgroups .newsgroups tended similar text content? discover finding pairwise correlation word frequencies within newsgroup, using pairwise_cor() function widyr package (see Chapter 4.2.2).filter stronger correlations among newsgroups, visualize network (Figure (fig:newsgroupcorsnetwork).\nFigure 9.3: network Usenet groups based correlation word counts , including connections correlation greater .4\nlooks like four main clusters newsgroups: computers/electronics, politics/religion, motor vehicles, sports. certainly makes sense terms words topics ’d expect newsgroups common.","code":"\ntf_idf <- words_by_newsgroup %>%\n  bind_tf_idf(word, newsgroup, n) %>%\n  arrange(desc(tf_idf))\n\ntf_idf\n#> # A tibble: 173,913 × 6\n#>    newsgroup                word               n      tf   idf tf_idf\n#>    <chr>                    <chr>          <int>   <dbl> <dbl>  <dbl>\n#>  1 comp.sys.ibm.pc.hardware scsi             483 0.0176   1.20 0.0212\n#>  2 talk.politics.mideast    armenian         582 0.00805  2.30 0.0185\n#>  3 rec.motorcycles          bike             324 0.0139   1.20 0.0167\n#>  4 talk.politics.mideast    armenians        509 0.00704  2.30 0.0162\n#>  5 sci.crypt                encryption       410 0.00816  1.90 0.0155\n#>  6 rec.sport.hockey         nhl              157 0.00440  3.00 0.0132\n#>  7 talk.politics.misc       stephanopoulos   158 0.00416  3.00 0.0125\n#>  8 rec.motorcycles          bikes             97 0.00416  3.00 0.0125\n#>  9 rec.sport.hockey         hockey           270 0.00756  1.61 0.0122\n#> 10 comp.windows.x           oname            136 0.00354  3.00 0.0106\n#> # … with 173,903 more rows\ntf_idf %>%\n  filter(str_detect(newsgroup, \"^sci\\\\.\")) %>%\n  group_by(newsgroup) %>%\n  slice_max(tf_idf, n = 12) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, tf_idf)) %>%\n  ggplot(aes(tf_idf, word, fill = newsgroup)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ newsgroup, scales = \"free\") +\n  labs(x = \"tf-idf\", y = NULL)\nlibrary(widyr)\n\nnewsgroup_cors <- words_by_newsgroup %>%\n  pairwise_cor(newsgroup, word, n, sort = TRUE)\n\nnewsgroup_cors\n#> # A tibble: 380 × 3\n#>    item1                    item2                    correlation\n#>    <chr>                    <chr>                          <dbl>\n#>  1 talk.religion.misc       soc.religion.christian         0.835\n#>  2 soc.religion.christian   talk.religion.misc             0.835\n#>  3 alt.atheism              talk.religion.misc             0.779\n#>  4 talk.religion.misc       alt.atheism                    0.779\n#>  5 alt.atheism              soc.religion.christian         0.751\n#>  6 soc.religion.christian   alt.atheism                    0.751\n#>  7 comp.sys.mac.hardware    comp.sys.ibm.pc.hardware       0.680\n#>  8 comp.sys.ibm.pc.hardware comp.sys.mac.hardware          0.680\n#>  9 rec.sport.baseball       rec.sport.hockey               0.577\n#> 10 rec.sport.hockey         rec.sport.baseball             0.577\n#> # … with 370 more rows\nlibrary(ggraph)\nlibrary(igraph)\nset.seed(2017)\n\nnewsgroup_cors %>%\n  filter(correlation > .4) %>%\n  graph_from_data_frame() %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(alpha = correlation, width = correlation)) +\n  geom_node_point(size = 6, color = \"lightblue\") +\n  geom_node_text(aes(label = name), repel = TRUE) +\n  theme_void()"},{"path":"usenet.html","id":"topic-modeling-1","chapter":"9 Case study: analyzing usenet text","heading":"9.2.2 Topic modeling","text":"Chapter 6, used latent Dirichlet allocation (LDA) algorithm divide set chapters books originally came . LDA sort Usenet messages came different newsgroups?Let’s try dividing messages four science-related newsgroups. first process document-term matrix cast_dtm() (Chapter 5.2), fit model LDA() function topicmodels package.four topics model extract, match four newsgroups? approach look familiar Chapter 6: visualize topic based frequent terms within (Figure 9.4).\nFigure 9.4: Top words topic fit LDA science-related newsgroups\ntop words, can start suspect topics may capture newsgroups. Topic 1 certainly represents sci.space newsgroup (thus common word “space”), topic 2 likely drawn cryptography, terms “key” “encryption”. Just Chapter 6.2.2, can confirm seeing documents newsgroup higher “gamma” topic (Figure 9.5).\nFigure 9.5: Distribution gamma topic within Usenet newsgroup\nMuch saw literature analysis, topic modeling able discover distinct topics present text without needing consult labels.Notice division Usenet messages wasn’t clean division book chapters, substantial number messages newsgroup getting high values “gamma” topics. isn’t surprising since many messages short overlap terms common words (example, discussions space travel include many words discussions electronics). realistic example LDA might divide documents rough topics still allowing degree overlap.","code":"\n# include only words that occur at least 50 times\nword_sci_newsgroups <- usenet_words %>%\n  filter(str_detect(newsgroup, \"^sci\")) %>%\n  group_by(word) %>%\n  mutate(word_total = n()) %>%\n  ungroup() %>%\n  filter(word_total > 50)\n\n# convert into a document-term matrix\n# with document names such as sci.crypt_14147\nsci_dtm <- word_sci_newsgroups %>%\n  unite(document, newsgroup, id) %>%\n  count(document, word) %>%\n  cast_dtm(document, word, n)\nlibrary(topicmodels)\nsci_lda <- LDA(sci_dtm, k = 4, control = list(seed = 2016))\nsci_lda %>%\n  tidy() %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 8) %>%\n  ungroup() %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\nsci_lda %>%\n  tidy(matrix = \"gamma\") %>%\n  separate(document, c(\"newsgroup\", \"id\"), sep = \"_\") %>%\n  mutate(newsgroup = reorder(newsgroup, gamma * topic)) %>%\n  ggplot(aes(factor(topic), gamma)) +\n  geom_boxplot() +\n  facet_wrap(~ newsgroup) +\n  labs(x = \"Topic\",\n       y = \"# of messages where this was the highest % topic\")"},{"path":"usenet.html","id":"sentiment-analysis","chapter":"9 Case study: analyzing usenet text","heading":"9.3 Sentiment analysis","text":"can use sentiment analysis techniques explored Chapter 2 examine often positive negative words occurred Usenet posts. newsgroups positive negative overall?example ’ll use AFINN sentiment lexicon, provides numeric positivity values word, visualize bar plot (Figure 9.6).\nFigure 9.6: Average AFINN value posts within newsgroup\nAccording analysis, “misc.forsale” newsgroup positive. makes sense, since likely included many positive adjectives products users wanted sell!","code":"\nnewsgroup_sentiments <- words_by_newsgroup %>%\n  inner_join(get_sentiments(\"afinn\"), by = \"word\") %>%\n  group_by(newsgroup) %>%\n  summarize(value = sum(value * n) / sum(n))\n\nnewsgroup_sentiments %>%\n  mutate(newsgroup = reorder(newsgroup, value)) %>%\n  ggplot(aes(value, newsgroup, fill = value > 0)) +\n  geom_col(show.legend = FALSE) +\n  labs(x = \"Average sentiment value\", y = NULL)"},{"path":"usenet.html","id":"sentiment-analysis-by-word","chapter":"9 Case study: analyzing usenet text","heading":"9.3.1 Sentiment analysis by word","text":"’s worth looking deeper understand newsgroups ended positive negative others. , can examine total positive negative contributions word.contributions?words effect sentiment values overall (Figure 9.7)?\nFigure 9.7: Words greatest contributions positive/negative sentiment values Usenet text\nwords look generally reasonable indicators message’s sentiment, can spot possible problems approach. “True” just easily part “true” similar negative expression, words “God” “Jesus” apparently common Usenet easily used many contexts, positive negative.may also care words contributed within newsgroup, can see newsgroups might incorrectly estimated.can calculate word’s contribution newsgroup’s sentiment score, visualize strongest contributors selection groups (Figure 9.8).\nFigure 9.8: Words contributed sentiment scores within six newsgroups\nconfirms hypothesis “misc.forsale” newsgroup: sentiment driven positive adjectives “excellent” “perfect”. can also see much sentiment confounded topic. atheism newsgroup likely discuss “god” detail even negative context, can see makes newsgroup look positive. Similarly, negative contribution word “gun” “talk.politics.guns” group occur even members discussing guns positively.helps remind us sentiment analysis can confounded topic, always examine influential words interpreting deeply.","code":"\ncontributions <- usenet_words %>%\n  inner_join(get_sentiments(\"afinn\"), by = \"word\") %>%\n  group_by(word) %>%\n  summarize(occurences = n(),\n            contribution = sum(value))\ncontributions\n#> # A tibble: 1,909 × 3\n#>    word      occurences contribution\n#>    <chr>          <int>        <dbl>\n#>  1 abandon           13          -26\n#>  2 abandoned         19          -38\n#>  3 abandons           3           -6\n#>  4 abduction          2           -4\n#>  5 abhor              4          -12\n#>  6 abhorred           1           -3\n#>  7 abhorrent          2           -6\n#>  8 abilities         16           32\n#>  9 ability          177          354\n#> 10 aboard             8            8\n#> # … with 1,899 more rows\ncontributions %>%\n  slice_max(abs(contribution), n = 25) %>%\n  mutate(word = reorder(word, contribution)) %>%\n  ggplot(aes(contribution, word, fill = contribution > 0)) +\n  geom_col(show.legend = FALSE) +\n  labs(y = NULL)\ntop_sentiment_words <- words_by_newsgroup %>%\n  inner_join(get_sentiments(\"afinn\"), by = \"word\") %>%\n  mutate(contribution = value * n / sum(n))\ntop_sentiment_words\n#> # A tibble: 13,063 × 5\n#>    newsgroup              word       n value contribution\n#>    <chr>                  <chr>  <int> <dbl>        <dbl>\n#>  1 soc.religion.christian god      917     1      0.0144 \n#>  2 soc.religion.christian jesus    440     1      0.00692\n#>  3 talk.politics.guns     gun      425    -1     -0.00668\n#>  4 talk.religion.misc     god      296     1      0.00465\n#>  5 alt.atheism            god      268     1      0.00421\n#>  6 soc.religion.christian faith    257     1      0.00404\n#>  7 talk.religion.misc     jesus    256     1      0.00403\n#>  8 talk.politics.mideast  killed   202    -3     -0.00953\n#>  9 talk.politics.mideast  war      187    -2     -0.00588\n#> 10 soc.religion.christian true     179     2      0.00563\n#> # … with 13,053 more rows"},{"path":"usenet.html","id":"sentiment-analysis-by-message","chapter":"9 Case study: analyzing usenet text","heading":"9.3.2 Sentiment analysis by message","text":"can also try finding positive negative individual messages, grouping summarizing id rather newsgroup.\nsimple measure reduce role randomness, filtered \nmessages fewer five words contributed \nsentiment.\npositive messages?Let’s check looking positive message whole dataset. assist write short function printing specified message.looks like message chosen uses word “winner” many times. negative message? Turns ’s also hockey site, different attitude.Well, can confidently say sentiment analysis worked!","code":"\nsentiment_messages <- usenet_words %>%\n  inner_join(get_sentiments(\"afinn\"), by = \"word\") %>%\n  group_by(newsgroup, id) %>%\n  summarize(sentiment = mean(value),\n            words = n()) %>%\n  ungroup() %>%\n  filter(words >= 5)\nsentiment_messages %>%\n  arrange(desc(sentiment))\n#> # A tibble: 3,554 × 4\n#>    newsgroup               id     sentiment words\n#>    <chr>                   <chr>      <dbl> <int>\n#>  1 rec.sport.hockey        53560       3.89    18\n#>  2 rec.sport.hockey        53602       3.83    30\n#>  3 rec.sport.hockey        53822       3.83     6\n#>  4 rec.sport.hockey        53645       3.23    13\n#>  5 rec.autos               102768      3.2      5\n#>  6 misc.forsale            75965       3        5\n#>  7 misc.forsale            76037       3        5\n#>  8 rec.sport.baseball      104458      3       11\n#>  9 rec.sport.hockey        53571       3        5\n#> 10 comp.os.ms-windows.misc 9620        2.86     7\n#> # … with 3,544 more rows\nprint_message <- function(group, message_id) {\n  result <- cleaned_text %>%\n    filter(newsgroup == group, id == message_id, text != \"\")\n  \n  cat(result$text, sep = \"\\n\")\n}\n\nprint_message(\"rec.sport.hockey\", 53560)\n#> Everybody.  Please send me your predictions for the Stanley Cup Playoffs!\n#> I want to see who people think will win.!!!!!!!\n#> Please Send them in this format, or something comparable:\n#> 1. Winner of Buffalo-Boston\n#> 2. Winner of Montreal-Quebec\n#> 3. Winner of Pittsburgh-New York\n#> 4. Winner of New Jersey-Washington\n#> 5. Winner of Chicago-(Minnesota/St.Louis)\n#> 6. Winner of Toronto-Detroit\n#> 7. Winner of Vancouver-Winnipeg\n#> 8. Winner of Calgary-Los Angeles\n#> 9. Winner of Adams Division (1-2 above)\n#> 10. Winner of Patrick Division (3-4 above)\n#> 11. Winner of Norris Division (5-6 above)\n#> 12. Winner of Smythe Division (7-8 above)\n#> 13. Winner of Wales Conference (9-10 above)\n#> 14. Winner of Campbell Conference (11-12 above)\n#> 15. Winner of Stanley Cup (13-14 above)\n#> I will summarize the predictions, and see who is the biggest\n#> INTERNET GURU PREDICTING GUY/GAL.\n#> Send entries to Richard Madison\n#> rrmadiso@napier.uwaterloo.ca\n#> PS:  I will send my entries to one of you folks so you know when I say\n#> I won, that I won!!!!!\nsentiment_messages %>%\n  arrange(sentiment)\n#> # A tibble: 3,554 × 4\n#>    newsgroup             id     sentiment words\n#>    <chr>                 <chr>      <dbl> <int>\n#>  1 rec.sport.hockey      53907      -3        6\n#>  2 sci.electronics       53899      -3        5\n#>  3 talk.politics.mideast 75918      -3        7\n#>  4 rec.autos             101627     -2.83     6\n#>  5 comp.graphics         37948      -2.8      5\n#>  6 comp.windows.x        67204      -2.7     10\n#>  7 talk.politics.guns    53362      -2.67     6\n#>  8 alt.atheism           51309      -2.6      5\n#>  9 comp.sys.mac.hardware 51513      -2.6      5\n#> 10 rec.autos             102883     -2.6      5\n#> # … with 3,544 more rows\n\nprint_message(\"rec.sport.hockey\", 53907)\n#> Losers like us? You are the fucking moron who has never heard of the Western\n#> Business School, or the University of Western Ontario for that matter. Why \n#> don't you pull your head out of your asshole and smell something other than\n#> shit for once so you can look on a map to see where UWO is! Back to hockey,\n#> the North Stars should be moved because for the past few years they have\n#> just been SHIT. A real team like Toronto would never be moved!!!\n#> Andrew--"},{"path":"usenet.html","id":"n-gram-analysis","chapter":"9 Case study: analyzing usenet text","heading":"9.3.3 N-gram analysis","text":"Chapter 4, considered effect words “” “” sentiment analysis Jane Austen novels, considering whether phrase like “don’t like” led passages incorrectly labeled positive. Usenet dataset much larger corpus modern text, may interested sentiment analysis may reversed text.’d start finding counting bigrams Usenet posts.define list six words suspect used negation, “”, “”, “without”, visualize sentiment-associated words often followed (Figure 9.9). shows words often contributed “wrong” direction.\nFigure 9.9: Words contributed sentiment followed ‘negating’ word\nlooks like largest sources misidentifying word positive come “don’t want/like/care”, largest source incorrectly classified negative sentiment “problem”.","code":"\nusenet_bigrams <- cleaned_text %>%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\nusenet_bigram_counts <- usenet_bigrams %>%\n  count(newsgroup, bigram, sort = TRUE) %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\nnegate_words <- c(\"not\", \"without\", \"no\", \"can't\", \"don't\", \"won't\")\n\nusenet_bigram_counts %>%\n  filter(word1 %in% negate_words) %>%\n  count(word1, word2, wt = n, sort = TRUE) %>%\n  inner_join(get_sentiments(\"afinn\"), by = c(word2 = \"word\")) %>%\n  mutate(contribution = value * n) %>%\n  group_by(word1) %>%\n  slice_max(abs(contribution), n = 10) %>%\n  ungroup() %>%\n  mutate(word2 = reorder_within(word2, contribution, word1)) %>%\n  ggplot(aes(contribution, word2, fill = contribution > 0)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ word1, scales = \"free\", nrow = 3) +\n  scale_y_reordered() +\n  labs(x = \"Sentiment value * # of occurrences\",\n       y = \"Words preceded by a negation\")"},{"path":"usenet.html","id":"summary-8","chapter":"9 Case study: analyzing usenet text","heading":"9.4 Summary","text":"analysis Usenet messages, ’ve incorporated almost every method tidy text mining described book, ranging tf-idf topic modeling sentiment analysis n-gram tokenization. Throughout chapter, indeed case studies, ’ve able rely small list common tools exploration visualization. hope examples show much tidy text analyses common , indeed tidy data analyses.","code":""},{"path":"references.html","id":"references","chapter":"10 References","heading":"10 References","text":"","code":""}]
