---
title: "Machine learning introductory guide in R. v.1.2"
author: "Arturo Bernal"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: yes
  word_document:
    toc: yes
  pdf_document:
    toc: yes
  theme: united
  toc_depth: 3
  highlight: tango
editor_options: 
  chunk_output_type: inline
---



```{r echo=FALSE}
ver<-"v.1.2"
```

This text and examples have the spirit to generate a basic guide for the Machine Learning (ML) methodology. It was inspired by the students of the AI concentration of Tecnologico de Monterrey in the spring of 2022. For the moment this document is based mainly on Géron (2019), and some text from James, et al. (2017) and Lantz (2015). However, I expect to fill it whit other materials and my own experience. 

Students are welcome to contribute, in which case, they will get credit for this text. 
 
  
To facilitate the explanations, this document follows a practical examples: i) for the continuous variables of the California Housing Prices data set presented in Géron (2019); ii) for the categorical variables, we will follow the <a href="https://www.lendingclub.com" target="_blank">Lending Club fintech</a> data set from kaggle to analyze a credit default. Which is a financial institution  that grant personal loans; iii) and a categorical variable for a Bit-coin prediction.   


The following steps may differ from other books, data scientists, or experts because there are many ways to deal whit machine learning. But that is precisely the richness of this area of expertise. If there would exist only one way to apply machine learning, everybody could apply it!!


# Big picture 

## Frame the Problem

Answer the following: 
what is the business objective? or How does the company (client) expect to use and benefit from this model? 

Knowing the objective is important because it will determine how you frame the problem, which algorithms you will select.


For the housing prices, the objective is to predict a district’s median housing price to detect investment opportunities, applying OLS ordinary least squares OLS. For example, your prediction may show that a house located in latitude x, longitude y, housing median age z, total rooms n, etc. has a median house value of 162,000. However, the house is for sale for $145,000, it may be an opportunity to buy something cheaper than it should cost.

For the Lending club credit default analysis, we will focus on predicting if a new customer would default the loan or not (will repay the loan or not).

what the current solution looks like (if any)? 

Is important to have a comparison, to visualize the solution. For this case, The current situation will often give you a reference for performance, as well as insights on how to solve the problem. 

For the case of the district housing prices, they currently estimated house prices manually by experts: a team gathers up-to-date information about a district, and when they cannot get the median housing price, they estimate it using complex rules.

Regarding the Lending club, we do not have evidence that Lending club applies machine learning methods to analyze the credit applications. But we will assume that as many other credit financial institution, the apply the method manually.  

## Select a Performance Measure


### For continuous variables 

Root Mean Square Error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight for large errors. Shows the mathematical formula to compute the RMSE. The mathematical formula to compute the RMSE is:

$$RMSE =\frac{1}{n}\ \sum_{i=1}^{n} (y_{i}-\hat{f(x_{i}))^{2}} $$


where $ \hat{f(x_{i})} $ is the prediction for the ith observation (the actual), $ y_{i} $ is the observation ith of the independent variable, and n is the number of observations. 

$$\hat{f(x_{i})}=\hat{\beta_{0}}+\hat{\beta_{1}}x_{1}+,..,+\hat{\beta_{n}}x_{n}$$

The RMSE is computed using the training data that was used to fit the model, and so should more accurately be referred to as the training RMSE. 

But in general, we do not really care how well the method works training on the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen
test data.


Mean absolute error (MAE). Even though the RMSE is generally the preferred performance measure for regression tasks, in some contexts you may prefer to use another function. For example, suppose that there are many outlier districts. In that case, you may consider using the mean absolute error (MAE, also called the average absolute deviation;Mean absolute error (MAE).

$$MAE =\frac{1}{n}\ \sum_{i=1}^{n} |y_{i}-\hat{f(x_{i})|}$$


### For categorical variables (classification methods)

Confusion matrix

Is a table that categorizes predictions according to whether they match the actual value. One of the table's dimensions indicates the possible categories of predicted values, while the other dimension indicates the same for actual values. Although we have only seen 2 x 2 confusion matrices so far, a matrix can be created for models that predict any number of class values. The following figure shows a generic confusion matrix. 

<img class="featurette-image img-responsive center-block" src="cm2.jpeg" width="260" height="330"  alt="Generic placeholder image">

Where True Positive (TP): Correctly classified as the class of interest. True Negative (TN) is Correctly classified as not the class of interest. False Positive (FP) is Incorrectly classified as the class of interest. False Negative (FN): Incorrectly classified as not the class of interest. 

In the confusion matrix, one of the mesures of interest is the accuracy, defined as:

$$ accuracy =\frac{TP+TN}{TP+TN+FP+FN}$$

In this formula, the terms TP, TN, FP, and FN refer to the number of times the model's predictions fell into each of these categories. The accuracy is therefore a proportion that represents the number of true positives and true negatives, divided by the total number of predictions.


The error rate or the proportion of the incorrectly classified examples is specified as:

$$ error\ rate =\frac{FP+FN}{TP+TN+FP+FN}=1-acuracy$$
Notice that the error rate can be calculated as one minus the accuracy. Intuitively, this makes sense; a model that is correct 95 percent of the time is incorrect 5 percent of the time.


Sensitivity and specificity

Finding a useful classifier often involves a balance between predictions that are overly conservative and overly aggressive. For example, an e-mail filter could guarantee to eliminate every spam message by aggressively eliminating nearly every ham message at the same time. On the other hand, guaranteeing that no ham message is inadvertently filtered might require us to allow an unacceptable amount of spam to pass through the filter. A pair of performance measures captures this trade off: sensitivity and specificity.

The sensitivity of a model (also called the true positive rate) measures the proportion of positive examples that were correctly classified. Therefore, as shown in the following formula, it is calculated as the number of true positives divided by the total number of positives, both correctly classified (the true positives) as well as incorrectly classified (the false negatives):

$$sensitivity =\frac{TP}{TP+FN}$$


Where True Positive (TP): Correctly classified as the class of interest and False Negative (FN): Incorrectly classified as not the class of interest

The specificity of a model (also called the true negative rate) measures the proportion of negative examples that were correctly classified. As with sensitivity, this is computed as the number of true negatives, divided by the total number of negatives—the true negatives plus the false positives:

$$ specificity =\frac{TN}{TN+FP}$$


True Negative (TN) is Correctly classified as not the class of interest and False Positive (FP) is Incorrectly classified as the class of interest



# Data collection


The data sets we use for this text are stored in Github. The house pricing data set is retrived from:
```{r echo=FALSE}
library(openxlsx)
house<-read.csv("housing.csv")

```


```{r eval=FALSE}
library(openxlsx)
house<-read.csv("https://raw.githubusercontent.com/abernal30/ml_book/main/housing.csv")
```



The credit data set is retrieve from:

```{r eval=FALSE}
library(openxlsx)
credit<-read.csv("https://raw.githubusercontent.com/abernal30/ml_book/main/credit.csv")
```


In the credit data We are interested only in the Fully Paid and Charged Off features, which are equivalent to no-default and default respectively. 

Charge off" means that the credit grantor wrote your account off of their receivables as a loss, and it is closed to future charges. When an account displays a status of "charge off," it means the account is closed to future use, although the debt is still owed.

Then we create a variable Default, which was the variable loan_status, if Fully Paid then 0, and Charged Off, 1. Use the ifelse function to create that variable, call it Default. Also, eliminate the loan_status variable (otherwise it would be duplicated).  
```{r echo=FALSE}
library(dplyr)
credit<-read.csv("credit.csv")
dfn<- credit%>%
  filter(loan_status== "Fully Paid" |loan_status== "Charged Off")
Default<-ifelse(dfn$loan_status=="Fully Paid",0,1)
credit<-cbind(dfn,Default)
credit<-credit[,-13]
```


And for the Bit-coin:

```{r eval=FALSE}
bit<-read.csv("https://raw.githubusercontent.com/abernal30/ml_book/main/bit.csv")
```



```{r echo=FALSE}
bit<-read.csv("bit.csv")
```



## Take a Quick Look at the Data Structure
To review the data class 

```{r}
str(house)
```

To review outliers or Nas. 
```{r}
summary(house)
```



## Create the training and Test Set

It is not a wise machine learning practice to train your model and score its accuracy on the same data set. It is a far superior technique to test your model with varying model parameter values on an unseen test set. 

Is a common practice to divide your data set into two parts training and test. However, other authors sucha as Lantz (2015) suggest to spit into Training Set, Validation Set and a Test Set. in the latter case, the proposal is that to train the model on the training set (60% of the data), then perform model selection (tuning parameters) on validation set (20% of the data) and once you are ready, test your model on the test set (20% of the data).
The set. seed function helps to get the same aleatory partition in the data.


### For cros sectional data or no time series

For the case where time is not relevant, the partition is usually random 80% the training set and 20% the test set. 

For the housing prices data set:

```{r}
set.seed (41)
dim<-dim(house)
train_sample<-sample(dim[1],dim[1]*.8)
house_train <- house[train_sample, ]
house_test  <- house[-train_sample, ]
```


For the credit data set
```{r}
set.seed (43)
dim<-dim(credit)
train_sample<-sample(dim[1],dim[1]*.8)
credit_train <- credit[train_sample, ]
credit_test  <- credit[-train_sample, ]
```


### For time series

For time series, usually we are interested in predicting the future, then is not convenient to split the training-test as we did in the last section. For time series the partition is in periods of time. The training is a period preceding the test. For this example we use the Bit Coin data series, in which case we also add some independent variables. For this case, assume that we want to predict a variable Direction, which is a categorical variable, which takes the value of one when the current price is bigger than the close price ten days ago, and zero otherwise.


$$Direction (t)=\alpha\ +\beta1 x1+\beta2 x2 + ..+e $$

In this case, before make the training-test partition, I suggest to transform the data frame into a xts object, which allow to make date subsets. 


```{r echo=FALSE}
library(xts)
```



```{r warning=FALSE,message=FALSE}
#library(xts)
datedata<-bit[,"Date"]
datedata<-as.Date(datedata,format="%m/%d/%Y")
datax<-xts(bit,order.by = as.Date(datedata),as.POSIXct("%d/%m/%Y"))
datax<-datax[,-1]
head(datax)

```



Finally, we apply the subset function.
```{r}
bit_train<-subset(datax,
  +index(datax)>="2019-11-01" &
  +index(datax)<="2021-10-01") 

bit_test<-subset(datax,
  +index(datax)>="2021-10-02" &
  +index(datax)<="2021-11-01") 
```



# 3 Discover and Visualize the Data to Gain Insights


## Looking for relationships among features and Correlations

Exploring relationships among features – the correlation matrix

For simplicity, we start the analysis removing the Nas
```{r}
colnames(house)
```

```{r}
h_vars<-c("median_house_value","total_rooms","total_bedrooms")

cor(na.omit(house_train[,h_vars]))
```
Visualizing relationships among features – the scatterplot matrix

```{r}
pairs(na.omit(house_train[,h_vars]))
```

Or a combination of correlation and relationships among features

```{r warning=FALSE, message=FALSE}
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y,use="pairwise.complete.obs"))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}
pairs(na.omit(house_train[,h_vars]),upper.panel=panel.cor,na.action = na.omit)
```




## Experimenting with Attribute Combinations
One last thing you may want to do before preparing the data for Machine Learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. Let’s create these new attributes

```{r}
rooms_per_household<-house_train[,"total_rooms"]/house_train[,"households"]
bedrooms_per_room<-house_train[,"total_bedrooms"]/house_train[,"total_rooms"]
population_per_household<-house_train[,"population"]/house_train[,"households"]

house_train<-cbind(house_train,rooms_per_household,bedrooms_per_room,population_per_household)

```


If we look at the correlations again, we see that "total_bedrooms","rooms_per_household","bedrooms_per_room" are more correlated whit median house value than total_rooms","total_bedrooms".

```{r warning=FALSE,message=FALSE}
h_vars<-c("median_house_value","total_rooms","total_bedrooms","rooms_per_household","bedrooms_per_room")

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y,use="pairwise.complete.obs"))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}
pairs(house_train[,h_vars],upper.panel=panel.cor,na.action = na.omit)
```

#  4 Prepare the Data for Machine Learning Algorithms

## Data Cleaning

Common data quality issues:

i) There might be missing or erroneous values in the data set
ii) There might be categorical (Textual, Boolean) values in the data set and not all algorithms work well with textual values.
iii) Some features might have larger values than others and are required to be transformed for equal importance.
 

## Missing values

Most Machine Learning algorithms cannot work with missing values, so analyze the best way to deal white them. We saw earlier that the total_bedrooms attribute has some missing values, so let’s fix this. You have, at least, three options:

i) Get rid of the corresponding districts (rows).
ii) Get rid of the whole attribute (column).
iii) Set the values to some value (zero, the mean, the median, etc.).


## Handling Text and Categorical Attributes

There might be missing or erroneous values in the data set
There might be categorical (Textual, Boolean) values in the data set and not all algorithms work well with textual values.
Some features might have larger values than others and are required to be transformed for equal importance.

If you run a regression including ocean_proximity, you will notice that the regression estimates a coefficient by each category of the variable ocean_proximity. When applying Machine Learning ML for forecasting pourpuses,  is more convenient to transform the categorical . We need only one coefficient associated with the variable ocean_proximity. Then we need to transform the variable into numeric. 

According to Jame et al. (2017), the expected test of the MSE, for a given value of x(0), can be decomposed into the sum of three fundamental quantities: 

$$E (y_{0}-\hat{f(x_{0}))^{2}}= Var(\hat{f(x_{0}))}+[Bias\ \hat{f(x_{0}))}]^2+Var[\epsilon]$$
where $E (y_{0}-\hat{f(x_{0}))^{2}}$ is the expected test MSE. It has the meaning of the expected average test MSE that we would obtain if we repeatedly estimated test MSE *f* using a large number of training sets, and tested at x0. The overall expected test MSE can be computed by averaging. 

Variance refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different $\hat{f}$. 

On the other hand, bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. For example, linear regression assumes that there is a linear relationship between Y and X1,X2, . . . , Xp. It is unlikely that any real-life problem truly has such a simple linear relationship, and so performing linear regression will undoubtedly result in some bias in the estimate of *f*. 

Finally, $\epsilon$ is the error term. 


When the number of observations, n, is much larger than the number of independent variables, *x*, then the least squares estimates tend to also have low variance,  $Var(\hat{f(x_{0}))}$,  and consequently reducing expected test of the MSE, $E (y_{0}-\hat{f(x_{0}))^{2}}$, improving the accuracy. However, the more the number of *x*, relative to the number of observations, the then there can be a lot of variability in the least squares fit, resulting in overfitting and consequently poor predictions on future observations not used in model training. In terms of the equation  

$$Var(\hat{f(x_{i})}) = Var(\hat{\beta_{0}}+\hat{\beta_{1}}x_{1}+,..,+\hat{\beta_{n}}x_{n})$$



Then, the more parameters would be estimated, the biger the variance would be, and 

When do we not apply when we are applying Ordinary Least Squares OLS looking for a causality, or trying  to explain the dependent variable,  

When you looked at the top five rows, you probably noticed that the values in the ocean_proximity column were repetitive, which means that it is probably a categorical attribute. You can find out what categories exist and how many districts belong to each category applying the duplicated function:

```{r}
col="ocean_proximity"
house_train[,col][!duplicated(house_train[,col])]

```


If you run a regression including ocean_proximity, you will notice that the regression estimates a coefficient by each category of the variable ocean_proximity. When applying Machine Learning ML for forecasting pourpuses,  is more convenient to transform the categorical . We need only one coefficient associated with the variable ocean_proximity. Then we need to transform the variable into numeric. 

When do we not apply when we are applying Ordinary Least Squares OLS looking for a causality, or trying  to explain the dependent variable,  

```{r}
dep<-"median_house_value"
model<-lm(median_house_value~.,data=house_train)
summary(model)
```

For this case, the objective is to make a forecast, so is convenient to transform the categorical values into numeric. We apply the function asnum from the library dataclean. 

```{r}
library(datapro)
house_train<-asnum(house_train)
dep<-"median_house_value"
model<-lm(median_house_value~.,data=house_train)
summary(model)
```

Now we see that the ocean_proximity variable onpy have one coefficient. 

## Feature Scaling

One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the target values (y, dependent variable) is generally not required.

There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.

Min-max scaling (many people call this normalization) is the simplest: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min. 

Standardization. first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance. 


# 5 Select and Train and evaluate the Model

## For continuous variables

For continuous variables, the goal of a machine learning model such as linear regression by OLS is to predict a variable y. For example, the "median_house_value" from the house pricing data set.

$$y=\beta_{0}+\beta_{1}x_{1}+,..,+\beta_{n}x_{n} +\epsilon $$
where $x_{1}$, $x_{n}$ are the independent variables and $\epsilon$ is the error term. 

In this case, the predicted value of $\hat{y}$ :

$$\hat{y}=\hat{\beta_{0}}+\hat{\beta_{1}}x_{1}+,..,+\hat{\beta_{n}}x_{n} $$


$$\hat{y}=\hat{\beta_{0}}+\hat{\beta_{1}}Z_{1}+,..,+\hat{\beta_{n}}Z_{m} $$




For the "median_house_value" from the house pricing data set, we use the function lm to make a Ordinary Least squares OLS estimation:


```{r}
dep<-"median_house_value"
house_model<-lm(median_house_value~.,data=house_train)
summary(house_model)
```
### Linear Model Selection



The spirit is the same we mention in a previous section, regarding the expected test of the MSE, for a given value of x(0),

$$E (y_{0}-\hat{f(x_{0}))^{2}}= Var(\hat{f(x_{0}))}+[Bias\ \hat{f(x_{0}))}]^2+Var[\epsilon]$$


In other words, we look for a subset of independent variables or predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables, as as consequence reducing the variance of the $\hat{f}$, the test of the MSE, which improves the prediction accuracy. 

```{r}
stepw<-step(house_model, direction = "both",trace = F)
summary(stepw)
```


## Select the model for categorical variables 

### Clasification models

The linear regression assumes that the response variable Y is quantitative. But in many situations, the response variable is instead qualitative. For example, eye color is qualitative, taking on values blue, brown, or green. Often qualitative variables are referred to as categorical ; we will use these terms interchangeably. In this chapter, we study approaches for predicting qualitative responses, a process that is known as classification. Predicting a qualitative response for an observation can be referred to as classifying that observation, since it involves assigning the observation to a category, or class. On the other hand, often the methods used for classification first predict the probability of each of the categories of a qualitative variable, as the basis for making the classification.

In this sense they also behave like regression methods.

We discuss three of the most widely-used classifiers: logistic regression, linear discriminant analysis, and
logistic regression linear discriminant analysis
K-nearest neighbors.

Examples of classification:
1. A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?

2. An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history, and so forth.

3. On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not

#### Logistic Regression

In a binary response model, interest lies primarily in the response probability:

$$P(y=1/X)=P(y=1|x_{1},x_{2},..,x_{n}) $$
where we use *X* to denote the full set of explanatory variables. For example, when y is the default variable in the house pricing example or direction in the bit coin example. On the other hand, x is the set of independent variables. The previous equation reads: the probability that y=1, given X is equal to the probability that y=1, given $x_{1},x_{2},..,x_{n}$.

To simplify, we usually use a binary response c(0,1). One for the result of interest, the default for example, or zero for other case, not default. To ensures that the result predictions be a binary response c(0,1), also. First we estimate response probabilities that are strictly between zero and one. Specifying a model:

$$P(y=1/X)=G(\beta_{0}+\beta_{1}x_{1}+,..,+\beta_{n}x_{n}) $$
where G is a function taking on values strictly between zero and one: 0 < *G* < 1.  

Various nonlinear functions have been suggested for the function *G* to make sure that the probabilities are between zero and one. The more common is the logit model, where *G* is the logistic function:

$$G(z)=\frac{exp(z)}{1-(exp(z)}$$
which is between zero and one for all real numbers z. This is the cumulative distribution function for a standard logistic random variable.

For the logit model *z* is the predicted model. 

$$ z=\hat{\beta_{0}}+\hat{\beta_{1}}x_{1}+,..,+\hat{\beta_{n}}x_{n}$$

$$G(z)=\frac{exp(z)}{1-(exp(z)}$$
Then,

$$P(y=1/X)=\frac{exp(\hat{\beta_{0}}+\hat{\beta_{1}}x_{1}+,..,+\hat{\beta_{n}}x_{n})}{1-(exp(\hat{\beta_{0}}+\hat{\beta_{1}}x_{1}+,..,+\hat{\beta_{n}}x_{n})}$$


Which is the equation we will use for making a prediction. However, we can not use the OLS to estimate the model, because it is a not linear binary response model. Then we apply the maximum likelihood estimation (MLE). 


For the credit data set we use the function glm to estimate the Logistic regression.

Before that, we trasnform the categorical values into numerival and eliminate Nas.

```{r}

credit_train<-asnum(credit_train)
credit_train<-na.omit(credit_train)

credit_test<-asnum(credit_test)
credit_test<-na.omit(credit_test)
```


```{r echo=FALSE}
credit_model<-glm(Default ~.,data=credit_train,family=binomial())
summary(credit_model)
```

```{r echo=FALSE}
stepw<-step(credit_model, direction = "both",trace = F)
summary(stepw)
```
```{r}
#credit_test<-asnum(credit_test)

```


We apply the stepwise methodology to estimate a model using the argument "both" and trace = F:

```{r echo=FALSE}
predicted_test <- predict(stepw, newdata=credit_test)
head(predicted_test)
```
Take the stepwise result to predict the final model but this time using the test dataset, print the head and tai

The argument type = "response" transform automatically the results into a probability. In other words, applies the formula exp(x)/(1+exp(x)). Then if we use that argument, is no correct to transform it into a probability.  
predicted_test <- predict(model, newdata=)

Transform into a probabilistic model, applying the formula exp(x)/(1+exp(x)). Print the head and tail of the result:
```{r echo=FALSE}
prob<-exp(predicted_test)/(1+exp(predicted_test))
summary(prob)
```
Create a conditional, if the prediction is bigger than 0.5, then 1, otherwise 0. Plot the results:
```{r echo=FALSE}
Default<-ifelse(prob>.5,1,0)
plot(Default)
```


Now measure the Accuracy applying the confusion Matrix:

```{r echo=FALSE}
library("caret")
```



```{r echo=FALSE}
#library("caret")
Defaultf<-factor(Default,levels=c(1,0))
credit_testf<-factor(credit_test$Default,levels=c(1,0))

confusionMatrix(Defaultf,credit_testf)

```

#### Linear Discriminant Analysis LDA

Why do we need another method, when we have logistic regression?
There are several reasons:

• When the classes are well-separated, the parameter estimates for the
logistic regression model are surprisingly unstable. Linear discriminant
analysis does not suffer from this problem.

• If number of observations *n* is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.

We apply the LDA model to the same credit data set. However, we takeout the variable issue_d, otherwise the model is not estimated. 
```{r}
credit_train<-credit_train[,-12]
credit_test<-credit_test[,-12]
```



```{r}
library (MASS)
lda.fit<-lda(Default~.,data=credit_train)
lda.fit
```

```{r}
lda.pred<-predict(lda.fit, credit_test)
#names(lda.pred)
lda.pred$class
```



```{r}
Defaultf_lda<-factor(lda.pred$class,levels=c(1,0))
credit_testf<-factor(credit_test$Default,levels=c(1,0))

confusionMatrix(Defaultf_lda,credit_testf)
```


### Training and Evaluating on the Training Set



Estimating the RMSE

```{r}
#dep="median_house_value"
#rmse<-cbind(test[,dep],predicted_test)
#sqrt(mean((rmse[,2]-rmse[,1])^2,na.rm = T ))

```

## Resampling (cross-validation)

Resampling methods are an indispensable tool in modern statistics. They involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model. For example, in order to estimate the variability of a linear
regression fit, we can repeatedly draw different samples from the training data, fit a linear regression to each new sample, and then examine the extent to which the resulting fits differ. Such an approach may allow us to obtain information that would not be available from fitting the model only once using the original training sample.

One of the most common resampling method is K Fold Cross Validation. Cross-validation can be used to estimate the test
error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility. The process of evaluating a model’s performance is known as model assessment, whereas model
the process of selecting the proper level of flexibility for a model is known as assessment model selection. 

K Fold Cross Validation

This approach involves randomly k-fold CV dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds. The mean squared error, MSE1, is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated
as a validation set.

$$CV_{k} =\frac{1}{k}\ \sum_{i=1}^{k} MSE_{i} $$
In practice, one typically performs k-fold CV using k = 5 or k = 10. 

# 6 Fine-Tune or Tune the ML Model

Tuning a machine learning model is an iterative process. Data scientists typically run numerous experiments to train and evaluate models, trying out different features, different loss functions, different AI/ML models, and adjusting model parameters and hyperparameters. Examples of steps involved in tuning and training a machine learning model include feature engineering, loss function formulation, model testing and selection, regularization, and selection of hyperparameters Krishnan (2022).


Let’s assume that you now have a shortlist of promising models. You now need to fine-tune them. Let’s look at a few ways you can do that.

Once we have retrieved optimum values of individual model parameters then we can use grid search to obtain combination of hyperparameter (The parameters are also known as hyperparameters) values of a model that can give us the highest accuracy.

Grid Search evaluates all possible combinations of the parameter values.

Grid Search is exhaustive and uses brute-force to evaluate the most accurate values. Therefore it is computationally intensive task.


## Analyze the Best Models and Their Errors





## Evaluate Your System on the Test Set





# References

Géron, A. (2019) Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition.  O’Reilly, Sebastopol, CA.

James, G, Witten, D, Hastie, T and Tibshirani, R (2017). An Introduction to Statistical Learning with Applications in R. Springer, NY. 

Krishnan, N. (2022). Enterprise AI and Machine Learning for Managers. Retrieved from c3.ai: https://c3.ai/publications/enterprise-ai-and-machine-learning-for-managers/



Lantz, B (2015). Machine Learning with R Second Edition. Packt Publishing, Birmingham, UK.

Wooldridge (2013). Introductory econometrics. Cengage Learning. Mason, OH
